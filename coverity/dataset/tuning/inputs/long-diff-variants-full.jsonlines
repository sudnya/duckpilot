{"bug_id": "6a999345-2e04-4adf-94b1-63fa6aa230bb", "bug_group_id": "31fbc1f5-ed37-4c7a-9550-90e9aa77e2e9", "bug_report_path": "amdgpu_gem-bug.txt", "bug_report_text": "Type: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n  The function 'amdgpu_gem_fault' takes a pointer parameter 'vmf' of type 'struct vm_fault *'.\n  This pointer and its member 'vma' are dereferenced (struct ttm_buffer_object *bo = vmf->vma->vm_private_data;) without first checking if they are null.\n  If 'vmf' or 'vmf->vma' is null, this will result in a null pointer dereference.\n\n", "diff_path": "amdgpu_gem-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf || !vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n#include <linux/ktime.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_exec.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_display.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = amdgpu_bo_fault_reserve_notify(bo);\n\t\tif (ret) {\n\t\t\tdrm_dev_exit(idx);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nunlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct amdgpu_gem_vm_ops = {\n\t.fault = amdgpu_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void amdgpu_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);\n\n\tif (robj) {\n\t\tamdgpu_hmm_unregister(robj);\n\t\tamdgpu_bo_unref(&robj);\n\t}\n}\n\nint amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n\t\t\t     int alignment, u32 initial_domain,\n\t\t\t     u64 flags, enum ttm_bo_type type,\n\t\t\t     struct dma_resv *resv,\n\t\t\t     struct drm_gem_object **obj, int8_t xcp_id_plus1)\n{\n\tstruct amdgpu_bo *bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo_param bp;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\t*obj = NULL;\n\tflags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbp.size = size;\n\tbp.byte_align = alignment;\n\tbp.type = type;\n\tbp.resv = resv;\n\tbp.preferred_domain = initial_domain;\n\tbp.flags = flags;\n\tbp.domain = initial_domain;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id_plus1;\n\n\tr = amdgpu_bo_create_user(adev, &bp, &ubo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &ubo->bo;\n\t*obj = &bo->tbo.base;\n\t(*obj)->funcs = &amdgpu_gem_object_funcs;\n\n\treturn 0;\n}\n\nvoid amdgpu_gem_force_release(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\n\tmutex_lock(&ddev->filelist_mutex);\n\n\tlist_for_each_entry(file, &ddev->filelist, lhead) {\n\t\tstruct drm_gem_object *gobj;\n\t\tint handle;\n\n\t\tWARN_ONCE(1, \"Still active user space clients!\\n\");\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, handle) {\n\t\t\tWARN_ONCE(1, \"And also active allocations!\\n\");\n\t\t\tdrm_gem_object_put(gobj);\n\t\t}\n\t\tidr_destroy(&file->object_idr);\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&ddev->filelist_mutex);\n}\n\n/*\n * Call from drm_gem_handle_create which appear in both new and open ioctl\n * case.\n */\nstatic int amdgpu_gem_object_open(struct drm_gem_object *obj,\n\t\t\t\t  struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tmm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);\n\tif (mm && mm != current->mm)\n\t\treturn -EPERM;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&\n\t    !amdgpu_vm_is_bo_always_valid(vm, abo))\n\t\treturn -EPERM;\n\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (r)\n\t\treturn r;\n\n\tbo_va = amdgpu_vm_bo_find(vm, abo);\n\tif (!bo_va)\n\t\tbo_va = amdgpu_vm_bo_add(adev, vm, abo);\n\telse\n\t\t++bo_va->ref_count;\n\tamdgpu_bo_unreserve(abo);\n\n\t/* Validate and add eviction fence to DMABuf imports with dynamic\n\t * attachment in compute VMs. Re-validation will be done by\n\t * amdgpu_vm_validate. Fences are on the reservation shared with the\n\t * export, which is currently required to be validated and fenced\n\t * already by amdgpu_amdkfd_gpuvm_restore_process_bos.\n\t *\n\t * Nested locking below for the case that a GEM object is opened in\n\t * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,\n\t * but not for export, this is a different lock class that cannot lead to\n\t * circular lock dependencies.\n\t */\n\tif (!vm->is_compute_context || !vm->process_info)\n\t\treturn 0;\n\tif (!obj->import_attach ||\n\t    !dma_buf_is_dynamic(obj->import_attach->dmabuf))\n\t\treturn 0;\n\tmutex_lock_nested(&vm->process_info->lock, 1);\n\tif (!WARN_ON(!vm->process_info->eviction_fence)) {\n\t\tr = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t&vm->process_info->eviction_fence->base);\n\t\tif (r) {\n\t\t\tstruct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);\n\n\t\t\tdev_warn(adev->dev, \"validate_and_fence failed: %d\\n\", r);\n\t\t\tif (ti) {\n\t\t\t\tdev_warn(adev->dev, \"pid %d\\n\", ti->pid);\n\t\t\t\tamdgpu_vm_put_task_info(ti);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tbo_va = amdgpu_vm_bo_find(vm, bo);\n\tif (!bo_va || --bo_va->ref_count)\n\t\tgoto out_unlock;\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page \"\n\t\t\t\"tables on GEM object close (%ld)\\n\", r);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (r)\n\t\tdev_err(adev->dev, \"leaking bo va (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n}\n\nstatic int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\t/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings\n\t * for debugger access to invisible VRAM. Should have used MAP_SHARED\n\t * instead. Clearing VM_MAYWRITE prevents the mapping from ever\n\t * becoming writable and makes is_cow_mapping(vm_flags) false.\n\t */\n\tif (is_cow_mapping(vma->vm_flags) &&\n\t    !(vma->vm_flags & VM_ACCESS_FLAGS))\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs = {\n\t.free = amdgpu_gem_object_free,\n\t.open = amdgpu_gem_object_open,\n\t.close = amdgpu_gem_object_close,\n\t.export = amdgpu_gem_prime_export,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = amdgpu_gem_object_mmap,\n\t.vm_ops = &amdgpu_gem_vm_ops,\n};\n\n/*\n * GEM ioctls.\n */\nint amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tunion drm_amdgpu_gem_create *args = data;\n\tuint64_t flags = args->in.domain_flags;\n\tuint64_t size = args->in.bo_size;\n\tstruct dma_resv *resv = NULL;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle, initial_domain;\n\tint r;\n\n\t/* reject DOORBELLs until userspace code to use it is available */\n\tif (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem flags */\n\tif (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |\n\t\t      AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t      AMDGPU_GEM_CREATE_VRAM_CLEARED |\n\t\t      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |\n\t\t      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |\n\t\t      AMDGPU_GEM_CREATE_ENCRYPTED |\n\t\t      AMDGPU_GEM_CREATE_GFX12_DCC |\n\t\t      AMDGPU_GEM_CREATE_DISCARDABLE))\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem domains */\n\tif (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)\n\t\treturn -EINVAL;\n\n\tif (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {\n\t\tDRM_NOTE_ONCE(\"Cannot allocate secure buffer since TMZ is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* always clear VRAM */\n\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\t/* create a gem object to contain this object in */\n\tif (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |\n\t    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\t\t/* if gds bo is created from user space, it must be\n\t\t\t * passed to bo list\n\t\t\t */\n\t\t\tDRM_ERROR(\"GDS bo cannot be per-vm-bo\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tflags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t}\n\n\tinitial_domain = (u32)(0xffffffff & args->in.domains);\nretry:\n\tr = amdgpu_gem_object_create(adev, size, args->in.alignment,\n\t\t\t\t     initial_domain,\n\t\t\t\t     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);\n\tif (r && r != -ERESTARTSYS) {\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {\n\t\t\tflags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tinitial_domain |= AMDGPU_GEM_DOMAIN_GTT;\n\t\t\tgoto retry;\n\t\t}\n\t\tDRM_DEBUG(\"Failed to allocate GEM object (%llu, %d, %llu, %d)\\n\",\n\t\t\t\tsize, initial_domain, args->in.alignment, r);\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tif (!r) {\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tabo->parent = amdgpu_bo_ref(vm->root.bo);\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\tif (r)\n\t\treturn r;\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.handle = handle;\n\treturn 0;\n}\n\nint amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_userptr *args = data;\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tstruct hmm_range *range;\n\tstruct amdgpu_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t/* reject unknown flag values */\n\tif (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |\n\t    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |\n\t    AMDGPU_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&\n\t     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {\n\n\t\t/* if we want to write to it we must install a MMU notifier */\n\t\treturn -EACCES;\n\t}\n\n\t/* create a gem object to contain this object in */\n\tr = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,\n\t\t\t\t     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn r;\n\n\tbo = gem_to_amdgpu_bo(gobj);\n\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tr = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tr = amdgpu_hmm_register(bo, args->addr);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t &range);\n\t\tif (r)\n\t\t\tgoto release_object;\n\n\t\tr = amdgpu_bo_reserve(bo, true);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\tif (r)\n\t\tgoto user_pages_done;\n\n\targs->handle = handle;\n\nuser_pages_done:\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\n\treturn r;\n}\n\nint amdgpu_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||\n\t    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = amdgpu_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_mmap *args = data;\n\tuint32_t handle = args->in.handle;\n\n\tmemset(args, 0, sizeof(*args));\n\treturn amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);\n}\n\n/**\n * amdgpu_gem_timeout - calculate jiffies timeout from absolute value\n *\n * @timeout_ns: timeout in ns\n *\n * Calculate the timeout in jiffies from an absolute timeout in ns.\n */\nunsigned long amdgpu_gem_timeout(uint64_t timeout_ns)\n{\n\tunsigned long timeout_jiffies;\n\tktime_t timeout;\n\n\t/* clamp timeout if it's to large */\n\tif (((int64_t)timeout_ns) < 0)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());\n\tif (ktime_to_ns(timeout) < 0)\n\t\treturn 0;\n\n\ttimeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));\n\t/*  clamp timeout to avoid unsigned-> signed overflow */\n\tif (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)\n\t\treturn MAX_SCHEDULE_TIMEOUT - 1;\n\n\treturn timeout_jiffies;\n}\n\nint amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tuint32_t handle = args->in.handle;\n\tunsigned long timeout = amdgpu_gem_timeout(args->in.timeout);\n\tint r = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, timeout);\n\n\t/* ret == 0 means not signaled,\n\t * ret > 0 means signaled\n\t * ret < 0 means interrupted before timeout\n\t */\n\tif (ret >= 0) {\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.status = (ret == 0);\n\t} else\n\t\tr = ret;\n\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_amdgpu_gem_metadata *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tint r = -1;\n\n\tDRM_DEBUG(\"%d\\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\n\tif (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {\n\t\tamdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);\n\t\tr = amdgpu_bo_get_metadata(robj, args->data.data,\n\t\t\t\t\t   sizeof(args->data.data),\n\t\t\t\t\t   &args->data.data_size_bytes,\n\t\t\t\t\t   &args->data.flags);\n\t} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {\n\t\tif (args->data.data_size_bytes > sizeof(args->data.data)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve;\n\t\t}\n\t\tr = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);\n\t\tif (!r)\n\t\t\tr = amdgpu_bo_set_metadata(robj, args->data.data,\n\t\t\t\t\t\t   args->data.data_size_bytes,\n\t\t\t\t\t\t   args->data.flags);\n\t}\n\nunreserve:\n\tamdgpu_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n/**\n * amdgpu_gem_va_update_vm -update the bo_va in its VM\n *\n * @adev: amdgpu_device pointer\n * @vm: vm to update\n * @bo_va: bo_va to update\n * @operation: map, unmap or clear\n *\n * Update the bo_va directly after setting its address. Errors are not\n * vital here, so they are not reported back to userspace.\n */\nstatic void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    uint32_t operation)\n{\n\tint r;\n\n\tif (!amdgpu_vm_ready(vm))\n\t\treturn;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tif (operation == AMDGPU_VA_OP_MAP ||\n\t    operation == AMDGPU_VA_OP_REPLACE) {\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\nerror:\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\n/**\n * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags\n *\n * @adev: amdgpu_device pointer\n * @flags: GEM UAPI flags\n *\n * Returns the GEM UAPI flags mapped into hardware for the ASIC.\n */\nuint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)\n{\n\tuint64_t pte_flag = 0;\n\n\tif (flags & AMDGPU_VM_PAGE_EXECUTABLE)\n\t\tpte_flag |= AMDGPU_PTE_EXECUTABLE;\n\tif (flags & AMDGPU_VM_PAGE_READABLE)\n\t\tpte_flag |= AMDGPU_PTE_READABLE;\n\tif (flags & AMDGPU_VM_PAGE_WRITEABLE)\n\t\tpte_flag |= AMDGPU_PTE_WRITEABLE;\n\tif (flags & AMDGPU_VM_PAGE_PRT)\n\t\tpte_flag |= AMDGPU_PTE_PRT_FLAG(adev);\n\tif (flags & AMDGPU_VM_PAGE_NOALLOC)\n\t\tpte_flag |= AMDGPU_PTE_NOALLOC;\n\n\tif (adev->gmc.gmc_funcs->map_mtype)\n\t\tpte_flag |= amdgpu_gmc_map_mtype(adev,\n\t\t\t\t\t\t flags & AMDGPU_VM_MTYPE_MASK);\n\n\treturn pte_flag;\n}\n\nint amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tconst uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |\n\t\tAMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |\n\t\tAMDGPU_VM_PAGE_NOALLOC;\n\tconst uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_PRT;\n\n\tstruct drm_amdgpu_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_bo *abo;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tuint64_t va_flags;\n\tuint64_t vm_size;\n\tint r = 0;\n\n\tif (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in reserved area 0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_VA_RESERVED_BOTTOM);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->va_address >= AMDGPU_GMC_HOLE_START &&\n\t    args->va_address < AMDGPU_GMC_HOLE_END) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in VA hole 0x%llx-0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_GMC_HOLE_START,\n\t\t\tAMDGPU_GMC_HOLE_END);\n\t\treturn -EINVAL;\n\t}\n\n\targs->va_address &= AMDGPU_GMC_HOLE_MASK;\n\n\tvm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;\n\tvm_size -= AMDGPU_VA_RESERVED_TOP;\n\tif (args->va_address + args->map_size > vm_size) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in top reserved area 0x%llx\\n\",\n\t\t\targs->va_address + args->map_size, vm_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {\n\t\tdev_dbg(dev->dev, \"invalid flags combination 0x%08X\\n\",\n\t\t\targs->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\tcase AMDGPU_VA_OP_UNMAP:\n\tcase AMDGPU_VA_OP_CLEAR:\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->operation != AMDGPU_VA_OP_CLEAR) &&\n\t    !(args->flags & AMDGPU_VM_PAGE_PRT)) {\n\t\tgobj = drm_gem_object_lookup(filp, args->handle);\n\t\tif (gobj == NULL)\n\t\t\treturn -ENOENT;\n\t\tabo = gem_to_amdgpu_bo(gobj);\n\t} else {\n\t\tgobj = NULL;\n\t\tabo = NULL;\n\t}\n\n\tdrm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tif (gobj) {\n\t\t\tr = drm_exec_lock_obj(&exec, gobj);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error;\n\t}\n\n\tif (abo) {\n\t\tbo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);\n\t\tif (!bo_va) {\n\t\t\tr = -ENOENT;\n\t\t\tgoto error;\n\t\t}\n\t} else if (args->operation != AMDGPU_VA_OP_CLEAR) {\n\t\tbo_va = fpriv->prt_va;\n\t} else {\n\t\tbo_va = NULL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_map(adev, bo_va, args->va_address,\n\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t     va_flags);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_UNMAP:\n\t\tr = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);\n\t\tbreak;\n\n\tcase AMDGPU_VA_OP_CLEAR:\n\t\tr = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,\n\t\t\t\t\t\targs->va_address,\n\t\t\t\t\t\targs->map_size);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,\n\t\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t\t     va_flags);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)\n\t\tamdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,\n\t\t\t\t\targs->operation);\n\nerror:\n\tdrm_exec_fini(&exec);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_vm_bo_base *base;\n\tstruct amdgpu_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {\n\t\tstruct drm_amdgpu_gem_create_in info;\n\t\tvoid __user *out = u64_to_user_ptr(args->value);\n\n\t\tinfo.bo_size = robj->tbo.base.size;\n\t\tinfo.alignment = robj->tbo.page_alignment << PAGE_SHIFT;\n\t\tinfo.domains = robj->preferred_domains;\n\t\tinfo.domain_flags = robj->flags;\n\t\tamdgpu_bo_unreserve(robj);\n\t\tif (copy_to_user(out, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase AMDGPU_GEM_OP_SET_PLACEMENT:\n\t\tif (robj->tbo.base.import_attach &&\n\t\t    args->value & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tr = -EINVAL;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {\n\t\t\tr = -EPERM;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tfor (base = robj->vm_bo; base; base = base->next)\n\t\t\tif (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),\n\t\t\t\tamdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\n\t\trobj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\trobj->allowed_domains = robj->preferred_domains;\n\t\tif (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\trobj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\t\tif (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)\n\t\t\tamdgpu_vm_bo_invalidate(adev, robj, true);\n\n\t\tamdgpu_bo_unreserve(robj);\n\t\tbreak;\n\tdefault:\n\t\tamdgpu_bo_unreserve(robj);\n\t\tr = -EINVAL;\n\t}\n\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nstatic int amdgpu_gem_align_pitch(struct amdgpu_device *adev,\n\t\t\t\t  int width,\n\t\t\t\t  int cpp,\n\t\t\t\t  bool tiled)\n{\n\tint aligned = width;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = 255;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = 127;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = 63;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint amdgpu_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tu64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t    AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tu32 domain;\n\tint r;\n\n\t/*\n\t * The buffer returned from this function should be cleared, but\n\t * it can only be done if the ring is enabled or we'll fail to\n\t * create the buffer.\n\t */\n\tif (adev->mman.buffer_funcs_enabled)\n\t\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\targs->pitch = amdgpu_gem_align_pitch(adev, args->width,\n\t\t\t\t\t     DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\tdomain = amdgpu_bo_get_preferred_domain(adev,\n\t\t\t\tamdgpu_display_supported_domains(adev, flags));\n\tr = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,\n\t\t\t\t     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\tint r;\n\n\tr = mutex_lock_interruptible(&dev->filelist_mutex);\n\tif (r)\n\t\treturn r;\n\n\tlist_for_each_entry(file, &dev->filelist, lhead) {\n\t\tstruct task_struct *task;\n\t\tstruct drm_gem_object *gobj;\n\t\tstruct pid *pid;\n\t\tint id;\n\n\t\t/*\n\t\t * Although we have a valid reference on file->pid, that does\n\t\t * not guarantee that the task_struct who called get_pid() is\n\t\t * still alive (e.g. get_pid(current) => fork() => exit()).\n\t\t * Therefore, we need to protect this ->comm access using RCU.\n\t\t */\n\t\trcu_read_lock();\n\t\tpid = rcu_dereference(file->pid);\n\t\ttask = pid_task(pid, PIDTYPE_TGID);\n\t\tseq_printf(m, \"pid %8d command %s:\\n\", pid_nr(pid),\n\t\t\t   task ? task->comm : \"<unknown>\");\n\t\trcu_read_unlock();\n\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, id) {\n\t\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tamdgpu_bo_print_info(id, bo, m);\n\t\t}\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&dev->filelist_mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);\n\n#endif\n\nvoid amdgpu_debugfs_gem_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_gem_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_gem_info_fops);\n#endif\n}\n", "source_code_path": "amdgpu_gem.c", "line_number": 50}
{"bug_id": "d83f7b92-1e57-4cf3-9b45-8e2d49a7c315", "bug_group_id": "31fbc1f5-ed37-4c7a-9550-90e9aa77e2e9", "bug_report_path": "amdgpu_gem-bug.txt", "bug_report_text": "Type: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n The function amdgpu_gem_fault accesses the 'vmf' pointer and its 'vma' member without verifying their validity. This can lead to a null pointer dereference if either 'vmf' or 'vmf->vma' is null, resulting in potential system crashes or instability.\n\n", "diff_path": "amdgpu_gem-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf || !vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n#include <linux/ktime.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_exec.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_display.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = amdgpu_bo_fault_reserve_notify(bo);\n\t\tif (ret) {\n\t\t\tdrm_dev_exit(idx);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nunlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct amdgpu_gem_vm_ops = {\n\t.fault = amdgpu_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void amdgpu_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);\n\n\tif (robj) {\n\t\tamdgpu_hmm_unregister(robj);\n\t\tamdgpu_bo_unref(&robj);\n\t}\n}\n\nint amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n\t\t\t     int alignment, u32 initial_domain,\n\t\t\t     u64 flags, enum ttm_bo_type type,\n\t\t\t     struct dma_resv *resv,\n\t\t\t     struct drm_gem_object **obj, int8_t xcp_id_plus1)\n{\n\tstruct amdgpu_bo *bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo_param bp;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\t*obj = NULL;\n\tflags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbp.size = size;\n\tbp.byte_align = alignment;\n\tbp.type = type;\n\tbp.resv = resv;\n\tbp.preferred_domain = initial_domain;\n\tbp.flags = flags;\n\tbp.domain = initial_domain;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id_plus1;\n\n\tr = amdgpu_bo_create_user(adev, &bp, &ubo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &ubo->bo;\n\t*obj = &bo->tbo.base;\n\t(*obj)->funcs = &amdgpu_gem_object_funcs;\n\n\treturn 0;\n}\n\nvoid amdgpu_gem_force_release(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\n\tmutex_lock(&ddev->filelist_mutex);\n\n\tlist_for_each_entry(file, &ddev->filelist, lhead) {\n\t\tstruct drm_gem_object *gobj;\n\t\tint handle;\n\n\t\tWARN_ONCE(1, \"Still active user space clients!\\n\");\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, handle) {\n\t\t\tWARN_ONCE(1, \"And also active allocations!\\n\");\n\t\t\tdrm_gem_object_put(gobj);\n\t\t}\n\t\tidr_destroy(&file->object_idr);\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&ddev->filelist_mutex);\n}\n\n/*\n * Call from drm_gem_handle_create which appear in both new and open ioctl\n * case.\n */\nstatic int amdgpu_gem_object_open(struct drm_gem_object *obj,\n\t\t\t\t  struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tmm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);\n\tif (mm && mm != current->mm)\n\t\treturn -EPERM;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&\n\t    !amdgpu_vm_is_bo_always_valid(vm, abo))\n\t\treturn -EPERM;\n\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (r)\n\t\treturn r;\n\n\tbo_va = amdgpu_vm_bo_find(vm, abo);\n\tif (!bo_va)\n\t\tbo_va = amdgpu_vm_bo_add(adev, vm, abo);\n\telse\n\t\t++bo_va->ref_count;\n\tamdgpu_bo_unreserve(abo);\n\n\t/* Validate and add eviction fence to DMABuf imports with dynamic\n\t * attachment in compute VMs. Re-validation will be done by\n\t * amdgpu_vm_validate. Fences are on the reservation shared with the\n\t * export, which is currently required to be validated and fenced\n\t * already by amdgpu_amdkfd_gpuvm_restore_process_bos.\n\t *\n\t * Nested locking below for the case that a GEM object is opened in\n\t * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,\n\t * but not for export, this is a different lock class that cannot lead to\n\t * circular lock dependencies.\n\t */\n\tif (!vm->is_compute_context || !vm->process_info)\n\t\treturn 0;\n\tif (!obj->import_attach ||\n\t    !dma_buf_is_dynamic(obj->import_attach->dmabuf))\n\t\treturn 0;\n\tmutex_lock_nested(&vm->process_info->lock, 1);\n\tif (!WARN_ON(!vm->process_info->eviction_fence)) {\n\t\tr = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t&vm->process_info->eviction_fence->base);\n\t\tif (r) {\n\t\t\tstruct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);\n\n\t\t\tdev_warn(adev->dev, \"validate_and_fence failed: %d\\n\", r);\n\t\t\tif (ti) {\n\t\t\t\tdev_warn(adev->dev, \"pid %d\\n\", ti->pid);\n\t\t\t\tamdgpu_vm_put_task_info(ti);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tbo_va = amdgpu_vm_bo_find(vm, bo);\n\tif (!bo_va || --bo_va->ref_count)\n\t\tgoto out_unlock;\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page \"\n\t\t\t\"tables on GEM object close (%ld)\\n\", r);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (r)\n\t\tdev_err(adev->dev, \"leaking bo va (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n}\n\nstatic int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\t/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings\n\t * for debugger access to invisible VRAM. Should have used MAP_SHARED\n\t * instead. Clearing VM_MAYWRITE prevents the mapping from ever\n\t * becoming writable and makes is_cow_mapping(vm_flags) false.\n\t */\n\tif (is_cow_mapping(vma->vm_flags) &&\n\t    !(vma->vm_flags & VM_ACCESS_FLAGS))\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs = {\n\t.free = amdgpu_gem_object_free,\n\t.open = amdgpu_gem_object_open,\n\t.close = amdgpu_gem_object_close,\n\t.export = amdgpu_gem_prime_export,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = amdgpu_gem_object_mmap,\n\t.vm_ops = &amdgpu_gem_vm_ops,\n};\n\n/*\n * GEM ioctls.\n */\nint amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tunion drm_amdgpu_gem_create *args = data;\n\tuint64_t flags = args->in.domain_flags;\n\tuint64_t size = args->in.bo_size;\n\tstruct dma_resv *resv = NULL;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle, initial_domain;\n\tint r;\n\n\t/* reject DOORBELLs until userspace code to use it is available */\n\tif (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem flags */\n\tif (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |\n\t\t      AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t      AMDGPU_GEM_CREATE_VRAM_CLEARED |\n\t\t      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |\n\t\t      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |\n\t\t      AMDGPU_GEM_CREATE_ENCRYPTED |\n\t\t      AMDGPU_GEM_CREATE_GFX12_DCC |\n\t\t      AMDGPU_GEM_CREATE_DISCARDABLE))\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem domains */\n\tif (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)\n\t\treturn -EINVAL;\n\n\tif (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {\n\t\tDRM_NOTE_ONCE(\"Cannot allocate secure buffer since TMZ is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* always clear VRAM */\n\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\t/* create a gem object to contain this object in */\n\tif (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |\n\t    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\t\t/* if gds bo is created from user space, it must be\n\t\t\t * passed to bo list\n\t\t\t */\n\t\t\tDRM_ERROR(\"GDS bo cannot be per-vm-bo\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tflags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t}\n\n\tinitial_domain = (u32)(0xffffffff & args->in.domains);\nretry:\n\tr = amdgpu_gem_object_create(adev, size, args->in.alignment,\n\t\t\t\t     initial_domain,\n\t\t\t\t     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);\n\tif (r && r != -ERESTARTSYS) {\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {\n\t\t\tflags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tinitial_domain |= AMDGPU_GEM_DOMAIN_GTT;\n\t\t\tgoto retry;\n\t\t}\n\t\tDRM_DEBUG(\"Failed to allocate GEM object (%llu, %d, %llu, %d)\\n\",\n\t\t\t\tsize, initial_domain, args->in.alignment, r);\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tif (!r) {\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tabo->parent = amdgpu_bo_ref(vm->root.bo);\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\tif (r)\n\t\treturn r;\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.handle = handle;\n\treturn 0;\n}\n\nint amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_userptr *args = data;\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tstruct hmm_range *range;\n\tstruct amdgpu_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t/* reject unknown flag values */\n\tif (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |\n\t    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |\n\t    AMDGPU_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&\n\t     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {\n\n\t\t/* if we want to write to it we must install a MMU notifier */\n\t\treturn -EACCES;\n\t}\n\n\t/* create a gem object to contain this object in */\n\tr = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,\n\t\t\t\t     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn r;\n\n\tbo = gem_to_amdgpu_bo(gobj);\n\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tr = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tr = amdgpu_hmm_register(bo, args->addr);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t &range);\n\t\tif (r)\n\t\t\tgoto release_object;\n\n\t\tr = amdgpu_bo_reserve(bo, true);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\tif (r)\n\t\tgoto user_pages_done;\n\n\targs->handle = handle;\n\nuser_pages_done:\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\n\treturn r;\n}\n\nint amdgpu_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||\n\t    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = amdgpu_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_mmap *args = data;\n\tuint32_t handle = args->in.handle;\n\n\tmemset(args, 0, sizeof(*args));\n\treturn amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);\n}\n\n/**\n * amdgpu_gem_timeout - calculate jiffies timeout from absolute value\n *\n * @timeout_ns: timeout in ns\n *\n * Calculate the timeout in jiffies from an absolute timeout in ns.\n */\nunsigned long amdgpu_gem_timeout(uint64_t timeout_ns)\n{\n\tunsigned long timeout_jiffies;\n\tktime_t timeout;\n\n\t/* clamp timeout if it's to large */\n\tif (((int64_t)timeout_ns) < 0)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());\n\tif (ktime_to_ns(timeout) < 0)\n\t\treturn 0;\n\n\ttimeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));\n\t/*  clamp timeout to avoid unsigned-> signed overflow */\n\tif (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)\n\t\treturn MAX_SCHEDULE_TIMEOUT - 1;\n\n\treturn timeout_jiffies;\n}\n\nint amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tuint32_t handle = args->in.handle;\n\tunsigned long timeout = amdgpu_gem_timeout(args->in.timeout);\n\tint r = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, timeout);\n\n\t/* ret == 0 means not signaled,\n\t * ret > 0 means signaled\n\t * ret < 0 means interrupted before timeout\n\t */\n\tif (ret >= 0) {\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.status = (ret == 0);\n\t} else\n\t\tr = ret;\n\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_amdgpu_gem_metadata *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tint r = -1;\n\n\tDRM_DEBUG(\"%d\\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\n\tif (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {\n\t\tamdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);\n\t\tr = amdgpu_bo_get_metadata(robj, args->data.data,\n\t\t\t\t\t   sizeof(args->data.data),\n\t\t\t\t\t   &args->data.data_size_bytes,\n\t\t\t\t\t   &args->data.flags);\n\t} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {\n\t\tif (args->data.data_size_bytes > sizeof(args->data.data)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve;\n\t\t}\n\t\tr = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);\n\t\tif (!r)\n\t\t\tr = amdgpu_bo_set_metadata(robj, args->data.data,\n\t\t\t\t\t\t   args->data.data_size_bytes,\n\t\t\t\t\t\t   args->data.flags);\n\t}\n\nunreserve:\n\tamdgpu_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n/**\n * amdgpu_gem_va_update_vm -update the bo_va in its VM\n *\n * @adev: amdgpu_device pointer\n * @vm: vm to update\n * @bo_va: bo_va to update\n * @operation: map, unmap or clear\n *\n * Update the bo_va directly after setting its address. Errors are not\n * vital here, so they are not reported back to userspace.\n */\nstatic void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    uint32_t operation)\n{\n\tint r;\n\n\tif (!amdgpu_vm_ready(vm))\n\t\treturn;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tif (operation == AMDGPU_VA_OP_MAP ||\n\t    operation == AMDGPU_VA_OP_REPLACE) {\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\nerror:\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\n/**\n * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags\n *\n * @adev: amdgpu_device pointer\n * @flags: GEM UAPI flags\n *\n * Returns the GEM UAPI flags mapped into hardware for the ASIC.\n */\nuint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)\n{\n\tuint64_t pte_flag = 0;\n\n\tif (flags & AMDGPU_VM_PAGE_EXECUTABLE)\n\t\tpte_flag |= AMDGPU_PTE_EXECUTABLE;\n\tif (flags & AMDGPU_VM_PAGE_READABLE)\n\t\tpte_flag |= AMDGPU_PTE_READABLE;\n\tif (flags & AMDGPU_VM_PAGE_WRITEABLE)\n\t\tpte_flag |= AMDGPU_PTE_WRITEABLE;\n\tif (flags & AMDGPU_VM_PAGE_PRT)\n\t\tpte_flag |= AMDGPU_PTE_PRT_FLAG(adev);\n\tif (flags & AMDGPU_VM_PAGE_NOALLOC)\n\t\tpte_flag |= AMDGPU_PTE_NOALLOC;\n\n\tif (adev->gmc.gmc_funcs->map_mtype)\n\t\tpte_flag |= amdgpu_gmc_map_mtype(adev,\n\t\t\t\t\t\t flags & AMDGPU_VM_MTYPE_MASK);\n\n\treturn pte_flag;\n}\n\nint amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tconst uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |\n\t\tAMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |\n\t\tAMDGPU_VM_PAGE_NOALLOC;\n\tconst uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_PRT;\n\n\tstruct drm_amdgpu_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_bo *abo;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tuint64_t va_flags;\n\tuint64_t vm_size;\n\tint r = 0;\n\n\tif (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in reserved area 0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_VA_RESERVED_BOTTOM);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->va_address >= AMDGPU_GMC_HOLE_START &&\n\t    args->va_address < AMDGPU_GMC_HOLE_END) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in VA hole 0x%llx-0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_GMC_HOLE_START,\n\t\t\tAMDGPU_GMC_HOLE_END);\n\t\treturn -EINVAL;\n\t}\n\n\targs->va_address &= AMDGPU_GMC_HOLE_MASK;\n\n\tvm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;\n\tvm_size -= AMDGPU_VA_RESERVED_TOP;\n\tif (args->va_address + args->map_size > vm_size) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in top reserved area 0x%llx\\n\",\n\t\t\targs->va_address + args->map_size, vm_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {\n\t\tdev_dbg(dev->dev, \"invalid flags combination 0x%08X\\n\",\n\t\t\targs->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\tcase AMDGPU_VA_OP_UNMAP:\n\tcase AMDGPU_VA_OP_CLEAR:\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->operation != AMDGPU_VA_OP_CLEAR) &&\n\t    !(args->flags & AMDGPU_VM_PAGE_PRT)) {\n\t\tgobj = drm_gem_object_lookup(filp, args->handle);\n\t\tif (gobj == NULL)\n\t\t\treturn -ENOENT;\n\t\tabo = gem_to_amdgpu_bo(gobj);\n\t} else {\n\t\tgobj = NULL;\n\t\tabo = NULL;\n\t}\n\n\tdrm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tif (gobj) {\n\t\t\tr = drm_exec_lock_obj(&exec, gobj);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error;\n\t}\n\n\tif (abo) {\n\t\tbo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);\n\t\tif (!bo_va) {\n\t\t\tr = -ENOENT;\n\t\t\tgoto error;\n\t\t}\n\t} else if (args->operation != AMDGPU_VA_OP_CLEAR) {\n\t\tbo_va = fpriv->prt_va;\n\t} else {\n\t\tbo_va = NULL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_map(adev, bo_va, args->va_address,\n\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t     va_flags);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_UNMAP:\n\t\tr = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);\n\t\tbreak;\n\n\tcase AMDGPU_VA_OP_CLEAR:\n\t\tr = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,\n\t\t\t\t\t\targs->va_address,\n\t\t\t\t\t\targs->map_size);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,\n\t\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t\t     va_flags);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)\n\t\tamdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,\n\t\t\t\t\targs->operation);\n\nerror:\n\tdrm_exec_fini(&exec);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_vm_bo_base *base;\n\tstruct amdgpu_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {\n\t\tstruct drm_amdgpu_gem_create_in info;\n\t\tvoid __user *out = u64_to_user_ptr(args->value);\n\n\t\tinfo.bo_size = robj->tbo.base.size;\n\t\tinfo.alignment = robj->tbo.page_alignment << PAGE_SHIFT;\n\t\tinfo.domains = robj->preferred_domains;\n\t\tinfo.domain_flags = robj->flags;\n\t\tamdgpu_bo_unreserve(robj);\n\t\tif (copy_to_user(out, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase AMDGPU_GEM_OP_SET_PLACEMENT:\n\t\tif (robj->tbo.base.import_attach &&\n\t\t    args->value & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tr = -EINVAL;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {\n\t\t\tr = -EPERM;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tfor (base = robj->vm_bo; base; base = base->next)\n\t\t\tif (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),\n\t\t\t\tamdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\n\t\trobj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\trobj->allowed_domains = robj->preferred_domains;\n\t\tif (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\trobj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\t\tif (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)\n\t\t\tamdgpu_vm_bo_invalidate(adev, robj, true);\n\n\t\tamdgpu_bo_unreserve(robj);\n\t\tbreak;\n\tdefault:\n\t\tamdgpu_bo_unreserve(robj);\n\t\tr = -EINVAL;\n\t}\n\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nstatic int amdgpu_gem_align_pitch(struct amdgpu_device *adev,\n\t\t\t\t  int width,\n\t\t\t\t  int cpp,\n\t\t\t\t  bool tiled)\n{\n\tint aligned = width;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = 255;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = 127;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = 63;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint amdgpu_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tu64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t    AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tu32 domain;\n\tint r;\n\n\t/*\n\t * The buffer returned from this function should be cleared, but\n\t * it can only be done if the ring is enabled or we'll fail to\n\t * create the buffer.\n\t */\n\tif (adev->mman.buffer_funcs_enabled)\n\t\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\targs->pitch = amdgpu_gem_align_pitch(adev, args->width,\n\t\t\t\t\t     DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\tdomain = amdgpu_bo_get_preferred_domain(adev,\n\t\t\t\tamdgpu_display_supported_domains(adev, flags));\n\tr = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,\n\t\t\t\t     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\tint r;\n\n\tr = mutex_lock_interruptible(&dev->filelist_mutex);\n\tif (r)\n\t\treturn r;\n\n\tlist_for_each_entry(file, &dev->filelist, lhead) {\n\t\tstruct task_struct *task;\n\t\tstruct drm_gem_object *gobj;\n\t\tstruct pid *pid;\n\t\tint id;\n\n\t\t/*\n\t\t * Although we have a valid reference on file->pid, that does\n\t\t * not guarantee that the task_struct who called get_pid() is\n\t\t * still alive (e.g. get_pid(current) => fork() => exit()).\n\t\t * Therefore, we need to protect this ->comm access using RCU.\n\t\t */\n\t\trcu_read_lock();\n\t\tpid = rcu_dereference(file->pid);\n\t\ttask = pid_task(pid, PIDTYPE_TGID);\n\t\tseq_printf(m, \"pid %8d command %s:\\n\", pid_nr(pid),\n\t\t\t   task ? task->comm : \"<unknown>\");\n\t\trcu_read_unlock();\n\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, id) {\n\t\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tamdgpu_bo_print_info(id, bo, m);\n\t\t}\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&dev->filelist_mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);\n\n#endif\n\nvoid amdgpu_debugfs_gem_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_gem_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_gem_info_fops);\n#endif\n}\n", "source_code_path": "amdgpu_gem.c", "line_number": 50}
{"bug_id": "f4e8c673-90ab-42d1-ae56-b12f8d4c9029", "bug_group_id": "31fbc1f5-ed37-4c7a-9550-90e9aa77e2e9", "bug_report_path": "amdgpu_gem-bug.txt", "bug_report_text": "Type: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\nA null pointer dereference vulnerability exists in the function amdgpu_gem_fault, which fails to check the validity of the 'vmf' pointer and its 'vma' member before accessing them. If either 'vmf' or 'vmf->vma' is null, this can result in a null pointer dereference and potential system crashes.\n\n", "diff_path": "amdgpu_gem-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf || !vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n#include <linux/ktime.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_exec.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_display.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = amdgpu_bo_fault_reserve_notify(bo);\n\t\tif (ret) {\n\t\t\tdrm_dev_exit(idx);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nunlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct amdgpu_gem_vm_ops = {\n\t.fault = amdgpu_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void amdgpu_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);\n\n\tif (robj) {\n\t\tamdgpu_hmm_unregister(robj);\n\t\tamdgpu_bo_unref(&robj);\n\t}\n}\n\nint amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n\t\t\t     int alignment, u32 initial_domain,\n\t\t\t     u64 flags, enum ttm_bo_type type,\n\t\t\t     struct dma_resv *resv,\n\t\t\t     struct drm_gem_object **obj, int8_t xcp_id_plus1)\n{\n\tstruct amdgpu_bo *bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo_param bp;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\t*obj = NULL;\n\tflags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbp.size = size;\n\tbp.byte_align = alignment;\n\tbp.type = type;\n\tbp.resv = resv;\n\tbp.preferred_domain = initial_domain;\n\tbp.flags = flags;\n\tbp.domain = initial_domain;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id_plus1;\n\n\tr = amdgpu_bo_create_user(adev, &bp, &ubo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &ubo->bo;\n\t*obj = &bo->tbo.base;\n\t(*obj)->funcs = &amdgpu_gem_object_funcs;\n\n\treturn 0;\n}\n\nvoid amdgpu_gem_force_release(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\n\tmutex_lock(&ddev->filelist_mutex);\n\n\tlist_for_each_entry(file, &ddev->filelist, lhead) {\n\t\tstruct drm_gem_object *gobj;\n\t\tint handle;\n\n\t\tWARN_ONCE(1, \"Still active user space clients!\\n\");\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, handle) {\n\t\t\tWARN_ONCE(1, \"And also active allocations!\\n\");\n\t\t\tdrm_gem_object_put(gobj);\n\t\t}\n\t\tidr_destroy(&file->object_idr);\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&ddev->filelist_mutex);\n}\n\n/*\n * Call from drm_gem_handle_create which appear in both new and open ioctl\n * case.\n */\nstatic int amdgpu_gem_object_open(struct drm_gem_object *obj,\n\t\t\t\t  struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tmm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);\n\tif (mm && mm != current->mm)\n\t\treturn -EPERM;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&\n\t    !amdgpu_vm_is_bo_always_valid(vm, abo))\n\t\treturn -EPERM;\n\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (r)\n\t\treturn r;\n\n\tbo_va = amdgpu_vm_bo_find(vm, abo);\n\tif (!bo_va)\n\t\tbo_va = amdgpu_vm_bo_add(adev, vm, abo);\n\telse\n\t\t++bo_va->ref_count;\n\tamdgpu_bo_unreserve(abo);\n\n\t/* Validate and add eviction fence to DMABuf imports with dynamic\n\t * attachment in compute VMs. Re-validation will be done by\n\t * amdgpu_vm_validate. Fences are on the reservation shared with the\n\t * export, which is currently required to be validated and fenced\n\t * already by amdgpu_amdkfd_gpuvm_restore_process_bos.\n\t *\n\t * Nested locking below for the case that a GEM object is opened in\n\t * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,\n\t * but not for export, this is a different lock class that cannot lead to\n\t * circular lock dependencies.\n\t */\n\tif (!vm->is_compute_context || !vm->process_info)\n\t\treturn 0;\n\tif (!obj->import_attach ||\n\t    !dma_buf_is_dynamic(obj->import_attach->dmabuf))\n\t\treturn 0;\n\tmutex_lock_nested(&vm->process_info->lock, 1);\n\tif (!WARN_ON(!vm->process_info->eviction_fence)) {\n\t\tr = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t&vm->process_info->eviction_fence->base);\n\t\tif (r) {\n\t\t\tstruct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);\n\n\t\t\tdev_warn(adev->dev, \"validate_and_fence failed: %d\\n\", r);\n\t\t\tif (ti) {\n\t\t\t\tdev_warn(adev->dev, \"pid %d\\n\", ti->pid);\n\t\t\t\tamdgpu_vm_put_task_info(ti);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tbo_va = amdgpu_vm_bo_find(vm, bo);\n\tif (!bo_va || --bo_va->ref_count)\n\t\tgoto out_unlock;\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page \"\n\t\t\t\"tables on GEM object close (%ld)\\n\", r);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (r)\n\t\tdev_err(adev->dev, \"leaking bo va (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n}\n\nstatic int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\t/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings\n\t * for debugger access to invisible VRAM. Should have used MAP_SHARED\n\t * instead. Clearing VM_MAYWRITE prevents the mapping from ever\n\t * becoming writable and makes is_cow_mapping(vm_flags) false.\n\t */\n\tif (is_cow_mapping(vma->vm_flags) &&\n\t    !(vma->vm_flags & VM_ACCESS_FLAGS))\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs = {\n\t.free = amdgpu_gem_object_free,\n\t.open = amdgpu_gem_object_open,\n\t.close = amdgpu_gem_object_close,\n\t.export = amdgpu_gem_prime_export,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = amdgpu_gem_object_mmap,\n\t.vm_ops = &amdgpu_gem_vm_ops,\n};\n\n/*\n * GEM ioctls.\n */\nint amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tunion drm_amdgpu_gem_create *args = data;\n\tuint64_t flags = args->in.domain_flags;\n\tuint64_t size = args->in.bo_size;\n\tstruct dma_resv *resv = NULL;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle, initial_domain;\n\tint r;\n\n\t/* reject DOORBELLs until userspace code to use it is available */\n\tif (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem flags */\n\tif (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |\n\t\t      AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t      AMDGPU_GEM_CREATE_VRAM_CLEARED |\n\t\t      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |\n\t\t      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |\n\t\t      AMDGPU_GEM_CREATE_ENCRYPTED |\n\t\t      AMDGPU_GEM_CREATE_GFX12_DCC |\n\t\t      AMDGPU_GEM_CREATE_DISCARDABLE))\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem domains */\n\tif (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)\n\t\treturn -EINVAL;\n\n\tif (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {\n\t\tDRM_NOTE_ONCE(\"Cannot allocate secure buffer since TMZ is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* always clear VRAM */\n\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\t/* create a gem object to contain this object in */\n\tif (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |\n\t    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\t\t/* if gds bo is created from user space, it must be\n\t\t\t * passed to bo list\n\t\t\t */\n\t\t\tDRM_ERROR(\"GDS bo cannot be per-vm-bo\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tflags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t}\n\n\tinitial_domain = (u32)(0xffffffff & args->in.domains);\nretry:\n\tr = amdgpu_gem_object_create(adev, size, args->in.alignment,\n\t\t\t\t     initial_domain,\n\t\t\t\t     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);\n\tif (r && r != -ERESTARTSYS) {\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {\n\t\t\tflags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tinitial_domain |= AMDGPU_GEM_DOMAIN_GTT;\n\t\t\tgoto retry;\n\t\t}\n\t\tDRM_DEBUG(\"Failed to allocate GEM object (%llu, %d, %llu, %d)\\n\",\n\t\t\t\tsize, initial_domain, args->in.alignment, r);\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tif (!r) {\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tabo->parent = amdgpu_bo_ref(vm->root.bo);\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\tif (r)\n\t\treturn r;\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.handle = handle;\n\treturn 0;\n}\n\nint amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_userptr *args = data;\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tstruct hmm_range *range;\n\tstruct amdgpu_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t/* reject unknown flag values */\n\tif (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |\n\t    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |\n\t    AMDGPU_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&\n\t     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {\n\n\t\t/* if we want to write to it we must install a MMU notifier */\n\t\treturn -EACCES;\n\t}\n\n\t/* create a gem object to contain this object in */\n\tr = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,\n\t\t\t\t     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn r;\n\n\tbo = gem_to_amdgpu_bo(gobj);\n\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tr = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tr = amdgpu_hmm_register(bo, args->addr);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t &range);\n\t\tif (r)\n\t\t\tgoto release_object;\n\n\t\tr = amdgpu_bo_reserve(bo, true);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\tif (r)\n\t\tgoto user_pages_done;\n\n\targs->handle = handle;\n\nuser_pages_done:\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\n\treturn r;\n}\n\nint amdgpu_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||\n\t    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = amdgpu_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_mmap *args = data;\n\tuint32_t handle = args->in.handle;\n\n\tmemset(args, 0, sizeof(*args));\n\treturn amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);\n}\n\n/**\n * amdgpu_gem_timeout - calculate jiffies timeout from absolute value\n *\n * @timeout_ns: timeout in ns\n *\n * Calculate the timeout in jiffies from an absolute timeout in ns.\n */\nunsigned long amdgpu_gem_timeout(uint64_t timeout_ns)\n{\n\tunsigned long timeout_jiffies;\n\tktime_t timeout;\n\n\t/* clamp timeout if it's to large */\n\tif (((int64_t)timeout_ns) < 0)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());\n\tif (ktime_to_ns(timeout) < 0)\n\t\treturn 0;\n\n\ttimeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));\n\t/*  clamp timeout to avoid unsigned-> signed overflow */\n\tif (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)\n\t\treturn MAX_SCHEDULE_TIMEOUT - 1;\n\n\treturn timeout_jiffies;\n}\n\nint amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tuint32_t handle = args->in.handle;\n\tunsigned long timeout = amdgpu_gem_timeout(args->in.timeout);\n\tint r = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, timeout);\n\n\t/* ret == 0 means not signaled,\n\t * ret > 0 means signaled\n\t * ret < 0 means interrupted before timeout\n\t */\n\tif (ret >= 0) {\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.status = (ret == 0);\n\t} else\n\t\tr = ret;\n\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_amdgpu_gem_metadata *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tint r = -1;\n\n\tDRM_DEBUG(\"%d\\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\n\tif (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {\n\t\tamdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);\n\t\tr = amdgpu_bo_get_metadata(robj, args->data.data,\n\t\t\t\t\t   sizeof(args->data.data),\n\t\t\t\t\t   &args->data.data_size_bytes,\n\t\t\t\t\t   &args->data.flags);\n\t} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {\n\t\tif (args->data.data_size_bytes > sizeof(args->data.data)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve;\n\t\t}\n\t\tr = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);\n\t\tif (!r)\n\t\t\tr = amdgpu_bo_set_metadata(robj, args->data.data,\n\t\t\t\t\t\t   args->data.data_size_bytes,\n\t\t\t\t\t\t   args->data.flags);\n\t}\n\nunreserve:\n\tamdgpu_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n/**\n * amdgpu_gem_va_update_vm -update the bo_va in its VM\n *\n * @adev: amdgpu_device pointer\n * @vm: vm to update\n * @bo_va: bo_va to update\n * @operation: map, unmap or clear\n *\n * Update the bo_va directly after setting its address. Errors are not\n * vital here, so they are not reported back to userspace.\n */\nstatic void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    uint32_t operation)\n{\n\tint r;\n\n\tif (!amdgpu_vm_ready(vm))\n\t\treturn;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tif (operation == AMDGPU_VA_OP_MAP ||\n\t    operation == AMDGPU_VA_OP_REPLACE) {\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\nerror:\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\n/**\n * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags\n *\n * @adev: amdgpu_device pointer\n * @flags: GEM UAPI flags\n *\n * Returns the GEM UAPI flags mapped into hardware for the ASIC.\n */\nuint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)\n{\n\tuint64_t pte_flag = 0;\n\n\tif (flags & AMDGPU_VM_PAGE_EXECUTABLE)\n\t\tpte_flag |= AMDGPU_PTE_EXECUTABLE;\n\tif (flags & AMDGPU_VM_PAGE_READABLE)\n\t\tpte_flag |= AMDGPU_PTE_READABLE;\n\tif (flags & AMDGPU_VM_PAGE_WRITEABLE)\n\t\tpte_flag |= AMDGPU_PTE_WRITEABLE;\n\tif (flags & AMDGPU_VM_PAGE_PRT)\n\t\tpte_flag |= AMDGPU_PTE_PRT_FLAG(adev);\n\tif (flags & AMDGPU_VM_PAGE_NOALLOC)\n\t\tpte_flag |= AMDGPU_PTE_NOALLOC;\n\n\tif (adev->gmc.gmc_funcs->map_mtype)\n\t\tpte_flag |= amdgpu_gmc_map_mtype(adev,\n\t\t\t\t\t\t flags & AMDGPU_VM_MTYPE_MASK);\n\n\treturn pte_flag;\n}\n\nint amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tconst uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |\n\t\tAMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |\n\t\tAMDGPU_VM_PAGE_NOALLOC;\n\tconst uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_PRT;\n\n\tstruct drm_amdgpu_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_bo *abo;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tuint64_t va_flags;\n\tuint64_t vm_size;\n\tint r = 0;\n\n\tif (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in reserved area 0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_VA_RESERVED_BOTTOM);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->va_address >= AMDGPU_GMC_HOLE_START &&\n\t    args->va_address < AMDGPU_GMC_HOLE_END) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in VA hole 0x%llx-0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_GMC_HOLE_START,\n\t\t\tAMDGPU_GMC_HOLE_END);\n\t\treturn -EINVAL;\n\t}\n\n\targs->va_address &= AMDGPU_GMC_HOLE_MASK;\n\n\tvm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;\n\tvm_size -= AMDGPU_VA_RESERVED_TOP;\n\tif (args->va_address + args->map_size > vm_size) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in top reserved area 0x%llx\\n\",\n\t\t\targs->va_address + args->map_size, vm_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {\n\t\tdev_dbg(dev->dev, \"invalid flags combination 0x%08X\\n\",\n\t\t\targs->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\tcase AMDGPU_VA_OP_UNMAP:\n\tcase AMDGPU_VA_OP_CLEAR:\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->operation != AMDGPU_VA_OP_CLEAR) &&\n\t    !(args->flags & AMDGPU_VM_PAGE_PRT)) {\n\t\tgobj = drm_gem_object_lookup(filp, args->handle);\n\t\tif (gobj == NULL)\n\t\t\treturn -ENOENT;\n\t\tabo = gem_to_amdgpu_bo(gobj);\n\t} else {\n\t\tgobj = NULL;\n\t\tabo = NULL;\n\t}\n\n\tdrm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tif (gobj) {\n\t\t\tr = drm_exec_lock_obj(&exec, gobj);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error;\n\t}\n\n\tif (abo) {\n\t\tbo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);\n\t\tif (!bo_va) {\n\t\t\tr = -ENOENT;\n\t\t\tgoto error;\n\t\t}\n\t} else if (args->operation != AMDGPU_VA_OP_CLEAR) {\n\t\tbo_va = fpriv->prt_va;\n\t} else {\n\t\tbo_va = NULL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_map(adev, bo_va, args->va_address,\n\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t     va_flags);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_UNMAP:\n\t\tr = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);\n\t\tbreak;\n\n\tcase AMDGPU_VA_OP_CLEAR:\n\t\tr = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,\n\t\t\t\t\t\targs->va_address,\n\t\t\t\t\t\targs->map_size);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,\n\t\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t\t     va_flags);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)\n\t\tamdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,\n\t\t\t\t\targs->operation);\n\nerror:\n\tdrm_exec_fini(&exec);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_vm_bo_base *base;\n\tstruct amdgpu_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {\n\t\tstruct drm_amdgpu_gem_create_in info;\n\t\tvoid __user *out = u64_to_user_ptr(args->value);\n\n\t\tinfo.bo_size = robj->tbo.base.size;\n\t\tinfo.alignment = robj->tbo.page_alignment << PAGE_SHIFT;\n\t\tinfo.domains = robj->preferred_domains;\n\t\tinfo.domain_flags = robj->flags;\n\t\tamdgpu_bo_unreserve(robj);\n\t\tif (copy_to_user(out, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase AMDGPU_GEM_OP_SET_PLACEMENT:\n\t\tif (robj->tbo.base.import_attach &&\n\t\t    args->value & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tr = -EINVAL;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {\n\t\t\tr = -EPERM;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tfor (base = robj->vm_bo; base; base = base->next)\n\t\t\tif (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),\n\t\t\t\tamdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\n\t\trobj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\trobj->allowed_domains = robj->preferred_domains;\n\t\tif (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\trobj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\t\tif (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)\n\t\t\tamdgpu_vm_bo_invalidate(adev, robj, true);\n\n\t\tamdgpu_bo_unreserve(robj);\n\t\tbreak;\n\tdefault:\n\t\tamdgpu_bo_unreserve(robj);\n\t\tr = -EINVAL;\n\t}\n\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nstatic int amdgpu_gem_align_pitch(struct amdgpu_device *adev,\n\t\t\t\t  int width,\n\t\t\t\t  int cpp,\n\t\t\t\t  bool tiled)\n{\n\tint aligned = width;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = 255;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = 127;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = 63;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint amdgpu_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tu64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t    AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tu32 domain;\n\tint r;\n\n\t/*\n\t * The buffer returned from this function should be cleared, but\n\t * it can only be done if the ring is enabled or we'll fail to\n\t * create the buffer.\n\t */\n\tif (adev->mman.buffer_funcs_enabled)\n\t\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\targs->pitch = amdgpu_gem_align_pitch(adev, args->width,\n\t\t\t\t\t     DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\tdomain = amdgpu_bo_get_preferred_domain(adev,\n\t\t\t\tamdgpu_display_supported_domains(adev, flags));\n\tr = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,\n\t\t\t\t     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\tint r;\n\n\tr = mutex_lock_interruptible(&dev->filelist_mutex);\n\tif (r)\n\t\treturn r;\n\n\tlist_for_each_entry(file, &dev->filelist, lhead) {\n\t\tstruct task_struct *task;\n\t\tstruct drm_gem_object *gobj;\n\t\tstruct pid *pid;\n\t\tint id;\n\n\t\t/*\n\t\t * Although we have a valid reference on file->pid, that does\n\t\t * not guarantee that the task_struct who called get_pid() is\n\t\t * still alive (e.g. get_pid(current) => fork() => exit()).\n\t\t * Therefore, we need to protect this ->comm access using RCU.\n\t\t */\n\t\trcu_read_lock();\n\t\tpid = rcu_dereference(file->pid);\n\t\ttask = pid_task(pid, PIDTYPE_TGID);\n\t\tseq_printf(m, \"pid %8d command %s:\\n\", pid_nr(pid),\n\t\t\t   task ? task->comm : \"<unknown>\");\n\t\trcu_read_unlock();\n\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, id) {\n\t\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tamdgpu_bo_print_info(id, bo, m);\n\t\t}\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&dev->filelist_mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);\n\n#endif\n\nvoid amdgpu_debugfs_gem_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_gem_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_gem_info_fops);\n#endif\n}\n", "source_code_path": "amdgpu_gem.c", "line_number": 50}
{"bug_id": "7bc52194-6d83-4fa8-95c2-3e716b48dfac", "bug_group_id": "31fbc1f5-ed37-4c7a-9550-90e9aa77e2e9", "bug_report_path": "amdgpu_gem-bug.txt", "bug_report_text": "Type: Null pointer dereferences\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nFunction: amdgpu_gem_fault\nLine: 50\n\nDescription:\n The function amdgpu_gem_fault accesses the 'vmf' pointer and its 'vma' member without performing a null check. This can lead to a null pointer dereference if 'vmf' or 'vmf->vma' is null, resulting in potential system instability or crashes due to invalid memory access.\n\n", "diff_path": "amdgpu_gem-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\nindex 0e617dff8765..47521241ed06 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c\n@@ -47,7 +47,12 @@ static const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\n static vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n {\n+       if (!vmf || !vmf->vma)\n+          return VM_FAULT_SIGSEGV;\n+\n        struct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n+       if (!bo)\n+          return VM_FAULT_SIGSEGV;\n        struct drm_device *ddev = bo->base.dev;\n        vm_fault_t ret;\n        int idx;\n", "code": "/*\n * Copyright 2008 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n * Copyright 2009 Jerome Glisse.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n *          Jerome Glisse\n */\n#include <linux/ktime.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_exec.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_display.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_xgmi.h\"\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs;\n\nstatic vm_fault_t amdgpu_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = amdgpu_bo_fault_reserve_notify(bo);\n\t\tif (ret) {\n\t\t\tdrm_dev_exit(idx);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nunlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct amdgpu_gem_vm_ops = {\n\t.fault = amdgpu_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void amdgpu_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct amdgpu_bo *robj = gem_to_amdgpu_bo(gobj);\n\n\tif (robj) {\n\t\tamdgpu_hmm_unregister(robj);\n\t\tamdgpu_bo_unref(&robj);\n\t}\n}\n\nint amdgpu_gem_object_create(struct amdgpu_device *adev, unsigned long size,\n\t\t\t     int alignment, u32 initial_domain,\n\t\t\t     u64 flags, enum ttm_bo_type type,\n\t\t\t     struct dma_resv *resv,\n\t\t\t     struct drm_gem_object **obj, int8_t xcp_id_plus1)\n{\n\tstruct amdgpu_bo *bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo_param bp;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\t*obj = NULL;\n\tflags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbp.size = size;\n\tbp.byte_align = alignment;\n\tbp.type = type;\n\tbp.resv = resv;\n\tbp.preferred_domain = initial_domain;\n\tbp.flags = flags;\n\tbp.domain = initial_domain;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id_plus1;\n\n\tr = amdgpu_bo_create_user(adev, &bp, &ubo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &ubo->bo;\n\t*obj = &bo->tbo.base;\n\t(*obj)->funcs = &amdgpu_gem_object_funcs;\n\n\treturn 0;\n}\n\nvoid amdgpu_gem_force_release(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\n\tmutex_lock(&ddev->filelist_mutex);\n\n\tlist_for_each_entry(file, &ddev->filelist, lhead) {\n\t\tstruct drm_gem_object *gobj;\n\t\tint handle;\n\n\t\tWARN_ONCE(1, \"Still active user space clients!\\n\");\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, handle) {\n\t\t\tWARN_ONCE(1, \"And also active allocations!\\n\");\n\t\t\tdrm_gem_object_put(gobj);\n\t\t}\n\t\tidr_destroy(&file->object_idr);\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&ddev->filelist_mutex);\n}\n\n/*\n * Call from drm_gem_handle_create which appear in both new and open ioctl\n * case.\n */\nstatic int amdgpu_gem_object_open(struct drm_gem_object *obj,\n\t\t\t\t  struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tmm = amdgpu_ttm_tt_get_usermm(abo->tbo.ttm);\n\tif (mm && mm != current->mm)\n\t\treturn -EPERM;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID &&\n\t    !amdgpu_vm_is_bo_always_valid(vm, abo))\n\t\treturn -EPERM;\n\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (r)\n\t\treturn r;\n\n\tbo_va = amdgpu_vm_bo_find(vm, abo);\n\tif (!bo_va)\n\t\tbo_va = amdgpu_vm_bo_add(adev, vm, abo);\n\telse\n\t\t++bo_va->ref_count;\n\tamdgpu_bo_unreserve(abo);\n\n\t/* Validate and add eviction fence to DMABuf imports with dynamic\n\t * attachment in compute VMs. Re-validation will be done by\n\t * amdgpu_vm_validate. Fences are on the reservation shared with the\n\t * export, which is currently required to be validated and fenced\n\t * already by amdgpu_amdkfd_gpuvm_restore_process_bos.\n\t *\n\t * Nested locking below for the case that a GEM object is opened in\n\t * kfd_mem_export_dmabuf. Since the lock below is only taken for imports,\n\t * but not for export, this is a different lock class that cannot lead to\n\t * circular lock dependencies.\n\t */\n\tif (!vm->is_compute_context || !vm->process_info)\n\t\treturn 0;\n\tif (!obj->import_attach ||\n\t    !dma_buf_is_dynamic(obj->import_attach->dmabuf))\n\t\treturn 0;\n\tmutex_lock_nested(&vm->process_info->lock, 1);\n\tif (!WARN_ON(!vm->process_info->eviction_fence)) {\n\t\tr = amdgpu_amdkfd_bo_validate_and_fence(abo, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t&vm->process_info->eviction_fence->base);\n\t\tif (r) {\n\t\t\tstruct amdgpu_task_info *ti = amdgpu_vm_get_task_info_vm(vm);\n\n\t\t\tdev_warn(adev->dev, \"validate_and_fence failed: %d\\n\", r);\n\t\t\tif (ti) {\n\t\t\t\tdev_warn(adev->dev, \"pid %d\\n\", ti->pid);\n\t\t\t\tamdgpu_vm_put_task_info(ti);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_prepare_obj(&exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tbo_va = amdgpu_vm_bo_find(vm, bo);\n\tif (!bo_va || --bo_va->ref_count)\n\t\tgoto out_unlock;\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page \"\n\t\t\t\"tables on GEM object close (%ld)\\n\", r);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (r)\n\t\tdev_err(adev->dev, \"leaking bo va (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n}\n\nstatic int amdgpu_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(obj);\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\t/* Workaround for Thunk bug creating PROT_NONE,MAP_PRIVATE mappings\n\t * for debugger access to invisible VRAM. Should have used MAP_SHARED\n\t * instead. Clearing VM_MAYWRITE prevents the mapping from ever\n\t * becoming writable and makes is_cow_mapping(vm_flags) false.\n\t */\n\tif (is_cow_mapping(vma->vm_flags) &&\n\t    !(vma->vm_flags & VM_ACCESS_FLAGS))\n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nstatic const struct drm_gem_object_funcs amdgpu_gem_object_funcs = {\n\t.free = amdgpu_gem_object_free,\n\t.open = amdgpu_gem_object_open,\n\t.close = amdgpu_gem_object_close,\n\t.export = amdgpu_gem_prime_export,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = amdgpu_gem_object_mmap,\n\t.vm_ops = &amdgpu_gem_vm_ops,\n};\n\n/*\n * GEM ioctls.\n */\nint amdgpu_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_vm *vm = &fpriv->vm;\n\tunion drm_amdgpu_gem_create *args = data;\n\tuint64_t flags = args->in.domain_flags;\n\tuint64_t size = args->in.bo_size;\n\tstruct dma_resv *resv = NULL;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle, initial_domain;\n\tint r;\n\n\t/* reject DOORBELLs until userspace code to use it is available */\n\tif (args->in.domains & AMDGPU_GEM_DOMAIN_DOORBELL)\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem flags */\n\tif (flags & ~(AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t      AMDGPU_GEM_CREATE_NO_CPU_ACCESS |\n\t\t      AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t      AMDGPU_GEM_CREATE_VRAM_CLEARED |\n\t\t      AMDGPU_GEM_CREATE_VM_ALWAYS_VALID |\n\t\t      AMDGPU_GEM_CREATE_EXPLICIT_SYNC |\n\t\t      AMDGPU_GEM_CREATE_ENCRYPTED |\n\t\t      AMDGPU_GEM_CREATE_GFX12_DCC |\n\t\t      AMDGPU_GEM_CREATE_DISCARDABLE))\n\t\treturn -EINVAL;\n\n\t/* reject invalid gem domains */\n\tif (args->in.domains & ~AMDGPU_GEM_DOMAIN_MASK)\n\t\treturn -EINVAL;\n\n\tif (!amdgpu_is_tmz(adev) && (flags & AMDGPU_GEM_CREATE_ENCRYPTED)) {\n\t\tDRM_NOTE_ONCE(\"Cannot allocate secure buffer since TMZ is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* always clear VRAM */\n\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\t/* create a gem object to contain this object in */\n\tif (args->in.domains & (AMDGPU_GEM_DOMAIN_GDS |\n\t    AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\t\t/* if gds bo is created from user space, it must be\n\t\t\t * passed to bo list\n\t\t\t */\n\t\t\tDRM_ERROR(\"GDS bo cannot be per-vm-bo\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tflags |= AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t}\n\n\tinitial_domain = (u32)(0xffffffff & args->in.domains);\nretry:\n\tr = amdgpu_gem_object_create(adev, size, args->in.alignment,\n\t\t\t\t     initial_domain,\n\t\t\t\t     flags, ttm_bo_type_device, resv, &gobj, fpriv->xcp_id + 1);\n\tif (r && r != -ERESTARTSYS) {\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {\n\t\t\tflags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (initial_domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tinitial_domain |= AMDGPU_GEM_DOMAIN_GTT;\n\t\t\tgoto retry;\n\t\t}\n\t\tDRM_DEBUG(\"Failed to allocate GEM object (%llu, %d, %llu, %d)\\n\",\n\t\t\t\tsize, initial_domain, args->in.alignment, r);\n\t}\n\n\tif (flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID) {\n\t\tif (!r) {\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tabo->parent = amdgpu_bo_ref(vm->root.bo);\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\tif (r)\n\t\treturn r;\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.handle = handle;\n\treturn 0;\n}\n\nint amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_userptr *args = data;\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tstruct hmm_range *range;\n\tstruct amdgpu_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t/* reject unknown flag values */\n\tif (args->flags & ~(AMDGPU_GEM_USERPTR_READONLY |\n\t    AMDGPU_GEM_USERPTR_ANONONLY | AMDGPU_GEM_USERPTR_VALIDATE |\n\t    AMDGPU_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (!(args->flags & AMDGPU_GEM_USERPTR_READONLY) &&\n\t     !(args->flags & AMDGPU_GEM_USERPTR_REGISTER)) {\n\n\t\t/* if we want to write to it we must install a MMU notifier */\n\t\treturn -EACCES;\n\t}\n\n\t/* create a gem object to contain this object in */\n\tr = amdgpu_gem_object_create(adev, args->size, 0, AMDGPU_GEM_DOMAIN_CPU,\n\t\t\t\t     0, ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn r;\n\n\tbo = gem_to_amdgpu_bo(gobj);\n\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\tr = amdgpu_ttm_tt_set_userptr(&bo->tbo, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tr = amdgpu_hmm_register(bo, args->addr);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE) {\n\t\tr = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t &range);\n\t\tif (r)\n\t\t\tgoto release_object;\n\n\t\tr = amdgpu_bo_reserve(bo, true);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tif (r)\n\t\t\tgoto user_pages_done;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\tif (r)\n\t\tgoto user_pages_done;\n\n\targs->handle = handle;\n\nuser_pages_done:\n\tif (args->flags & AMDGPU_GEM_USERPTR_VALIDATE)\n\t\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\n\treturn r;\n}\n\nint amdgpu_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm) ||\n\t    (robj->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = amdgpu_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint amdgpu_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_mmap *args = data;\n\tuint32_t handle = args->in.handle;\n\n\tmemset(args, 0, sizeof(*args));\n\treturn amdgpu_mode_dumb_mmap(filp, dev, handle, &args->out.addr_ptr);\n}\n\n/**\n * amdgpu_gem_timeout - calculate jiffies timeout from absolute value\n *\n * @timeout_ns: timeout in ns\n *\n * Calculate the timeout in jiffies from an absolute timeout in ns.\n */\nunsigned long amdgpu_gem_timeout(uint64_t timeout_ns)\n{\n\tunsigned long timeout_jiffies;\n\tktime_t timeout;\n\n\t/* clamp timeout if it's to large */\n\tif (((int64_t)timeout_ns) < 0)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = ktime_sub(ns_to_ktime(timeout_ns), ktime_get());\n\tif (ktime_to_ns(timeout) < 0)\n\t\treturn 0;\n\n\ttimeout_jiffies = nsecs_to_jiffies(ktime_to_ns(timeout));\n\t/*  clamp timeout to avoid unsigned-> signed overflow */\n\tif (timeout_jiffies > MAX_SCHEDULE_TIMEOUT)\n\t\treturn MAX_SCHEDULE_TIMEOUT - 1;\n\n\treturn timeout_jiffies;\n}\n\nint amdgpu_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tunion drm_amdgpu_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tuint32_t handle = args->in.handle;\n\tunsigned long timeout = amdgpu_gem_timeout(args->in.timeout);\n\tint r = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, timeout);\n\n\t/* ret == 0 means not signaled,\n\t * ret > 0 means signaled\n\t * ret < 0 means interrupted before timeout\n\t */\n\tif (ret >= 0) {\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.status = (ret == 0);\n\t} else\n\t\tr = ret;\n\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_metadata_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_amdgpu_gem_metadata *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_bo *robj;\n\tint r = -1;\n\n\tDRM_DEBUG(\"%d\\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\n\tif (args->op == AMDGPU_GEM_METADATA_OP_GET_METADATA) {\n\t\tamdgpu_bo_get_tiling_flags(robj, &args->data.tiling_info);\n\t\tr = amdgpu_bo_get_metadata(robj, args->data.data,\n\t\t\t\t\t   sizeof(args->data.data),\n\t\t\t\t\t   &args->data.data_size_bytes,\n\t\t\t\t\t   &args->data.flags);\n\t} else if (args->op == AMDGPU_GEM_METADATA_OP_SET_METADATA) {\n\t\tif (args->data.data_size_bytes > sizeof(args->data.data)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve;\n\t\t}\n\t\tr = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);\n\t\tif (!r)\n\t\t\tr = amdgpu_bo_set_metadata(robj, args->data.data,\n\t\t\t\t\t\t   args->data.data_size_bytes,\n\t\t\t\t\t\t   args->data.flags);\n\t}\n\nunreserve:\n\tamdgpu_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n/**\n * amdgpu_gem_va_update_vm -update the bo_va in its VM\n *\n * @adev: amdgpu_device pointer\n * @vm: vm to update\n * @bo_va: bo_va to update\n * @operation: map, unmap or clear\n *\n * Update the bo_va directly after setting its address. Errors are not\n * vital here, so they are not reported back to userspace.\n */\nstatic void amdgpu_gem_va_update_vm(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    uint32_t operation)\n{\n\tint r;\n\n\tif (!amdgpu_vm_ready(vm))\n\t\treturn;\n\n\tr = amdgpu_vm_clear_freed(adev, vm, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tif (operation == AMDGPU_VA_OP_MAP ||\n\t    operation == AMDGPU_VA_OP_REPLACE) {\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\nerror:\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\n/**\n * amdgpu_gem_va_map_flags - map GEM UAPI flags into hardware flags\n *\n * @adev: amdgpu_device pointer\n * @flags: GEM UAPI flags\n *\n * Returns the GEM UAPI flags mapped into hardware for the ASIC.\n */\nuint64_t amdgpu_gem_va_map_flags(struct amdgpu_device *adev, uint32_t flags)\n{\n\tuint64_t pte_flag = 0;\n\n\tif (flags & AMDGPU_VM_PAGE_EXECUTABLE)\n\t\tpte_flag |= AMDGPU_PTE_EXECUTABLE;\n\tif (flags & AMDGPU_VM_PAGE_READABLE)\n\t\tpte_flag |= AMDGPU_PTE_READABLE;\n\tif (flags & AMDGPU_VM_PAGE_WRITEABLE)\n\t\tpte_flag |= AMDGPU_PTE_WRITEABLE;\n\tif (flags & AMDGPU_VM_PAGE_PRT)\n\t\tpte_flag |= AMDGPU_PTE_PRT_FLAG(adev);\n\tif (flags & AMDGPU_VM_PAGE_NOALLOC)\n\t\tpte_flag |= AMDGPU_PTE_NOALLOC;\n\n\tif (adev->gmc.gmc_funcs->map_mtype)\n\t\tpte_flag |= amdgpu_gmc_map_mtype(adev,\n\t\t\t\t\t\t flags & AMDGPU_VM_MTYPE_MASK);\n\n\treturn pte_flag;\n}\n\nint amdgpu_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tconst uint32_t valid_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE |\n\t\tAMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_MTYPE_MASK |\n\t\tAMDGPU_VM_PAGE_NOALLOC;\n\tconst uint32_t prt_flags = AMDGPU_VM_DELAY_UPDATE |\n\t\tAMDGPU_VM_PAGE_PRT;\n\n\tstruct drm_amdgpu_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tstruct amdgpu_bo *abo;\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct drm_exec exec;\n\tuint64_t va_flags;\n\tuint64_t vm_size;\n\tint r = 0;\n\n\tif (args->va_address < AMDGPU_VA_RESERVED_BOTTOM) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in reserved area 0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_VA_RESERVED_BOTTOM);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->va_address >= AMDGPU_GMC_HOLE_START &&\n\t    args->va_address < AMDGPU_GMC_HOLE_END) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in VA hole 0x%llx-0x%llx\\n\",\n\t\t\targs->va_address, AMDGPU_GMC_HOLE_START,\n\t\t\tAMDGPU_GMC_HOLE_END);\n\t\treturn -EINVAL;\n\t}\n\n\targs->va_address &= AMDGPU_GMC_HOLE_MASK;\n\n\tvm_size = adev->vm_manager.max_pfn * AMDGPU_GPU_PAGE_SIZE;\n\tvm_size -= AMDGPU_VA_RESERVED_TOP;\n\tif (args->va_address + args->map_size > vm_size) {\n\t\tdev_dbg(dev->dev,\n\t\t\t\"va_address 0x%llx is in top reserved area 0x%llx\\n\",\n\t\t\targs->va_address + args->map_size, vm_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->flags & ~valid_flags) && (args->flags & ~prt_flags)) {\n\t\tdev_dbg(dev->dev, \"invalid flags combination 0x%08X\\n\",\n\t\t\targs->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\tcase AMDGPU_VA_OP_UNMAP:\n\tcase AMDGPU_VA_OP_CLEAR:\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->operation != AMDGPU_VA_OP_CLEAR) &&\n\t    !(args->flags & AMDGPU_VM_PAGE_PRT)) {\n\t\tgobj = drm_gem_object_lookup(filp, args->handle);\n\t\tif (gobj == NULL)\n\t\t\treturn -ENOENT;\n\t\tabo = gem_to_amdgpu_bo(gobj);\n\t} else {\n\t\tgobj = NULL;\n\t\tabo = NULL;\n\t}\n\n\tdrm_exec_init(&exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t      DRM_EXEC_IGNORE_DUPLICATES, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tif (gobj) {\n\t\t\tr = drm_exec_lock_obj(&exec, gobj);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tr = amdgpu_vm_lock_pd(&fpriv->vm, &exec, 2);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error;\n\t}\n\n\tif (abo) {\n\t\tbo_va = amdgpu_vm_bo_find(&fpriv->vm, abo);\n\t\tif (!bo_va) {\n\t\t\tr = -ENOENT;\n\t\t\tgoto error;\n\t\t}\n\t} else if (args->operation != AMDGPU_VA_OP_CLEAR) {\n\t\tbo_va = fpriv->prt_va;\n\t} else {\n\t\tbo_va = NULL;\n\t}\n\n\tswitch (args->operation) {\n\tcase AMDGPU_VA_OP_MAP:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_map(adev, bo_va, args->va_address,\n\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t     va_flags);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_UNMAP:\n\t\tr = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);\n\t\tbreak;\n\n\tcase AMDGPU_VA_OP_CLEAR:\n\t\tr = amdgpu_vm_bo_clear_mappings(adev, &fpriv->vm,\n\t\t\t\t\t\targs->va_address,\n\t\t\t\t\t\targs->map_size);\n\t\tbreak;\n\tcase AMDGPU_VA_OP_REPLACE:\n\t\tva_flags = amdgpu_gem_va_map_flags(adev, args->flags);\n\t\tr = amdgpu_vm_bo_replace_map(adev, bo_va, args->va_address,\n\t\t\t\t\t     args->offset_in_bo, args->map_size,\n\t\t\t\t\t     va_flags);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r && !(args->flags & AMDGPU_VM_DELAY_UPDATE) && !adev->debug_vm)\n\t\tamdgpu_gem_va_update_vm(adev, &fpriv->vm, bo_va,\n\t\t\t\t\targs->operation);\n\nerror:\n\tdrm_exec_fini(&exec);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint amdgpu_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_amdgpu_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct amdgpu_vm_bo_base *base;\n\tstruct amdgpu_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (!gobj)\n\t\treturn -ENOENT;\n\n\trobj = gem_to_amdgpu_bo(gobj);\n\n\tr = amdgpu_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase AMDGPU_GEM_OP_GET_GEM_CREATE_INFO: {\n\t\tstruct drm_amdgpu_gem_create_in info;\n\t\tvoid __user *out = u64_to_user_ptr(args->value);\n\n\t\tinfo.bo_size = robj->tbo.base.size;\n\t\tinfo.alignment = robj->tbo.page_alignment << PAGE_SHIFT;\n\t\tinfo.domains = robj->preferred_domains;\n\t\tinfo.domain_flags = robj->flags;\n\t\tamdgpu_bo_unreserve(robj);\n\t\tif (copy_to_user(out, &info, sizeof(info)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\t}\n\tcase AMDGPU_GEM_OP_SET_PLACEMENT:\n\t\tif (robj->tbo.base.import_attach &&\n\t\t    args->value & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tr = -EINVAL;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tif (amdgpu_ttm_tt_get_usermm(robj->tbo.ttm)) {\n\t\t\tr = -EPERM;\n\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\tbreak;\n\t\t}\n\t\tfor (base = robj->vm_bo; base; base = base->next)\n\t\t\tif (amdgpu_xgmi_same_hive(amdgpu_ttm_adev(robj->tbo.bdev),\n\t\t\t\tamdgpu_ttm_adev(base->vm->root.bo->tbo.bdev))) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tamdgpu_bo_unreserve(robj);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\n\t\trobj->preferred_domains = args->value & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\trobj->allowed_domains = robj->preferred_domains;\n\t\tif (robj->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\trobj->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\t\tif (robj->flags & AMDGPU_GEM_CREATE_VM_ALWAYS_VALID)\n\t\t\tamdgpu_vm_bo_invalidate(adev, robj, true);\n\n\t\tamdgpu_bo_unreserve(robj);\n\t\tbreak;\n\tdefault:\n\t\tamdgpu_bo_unreserve(robj);\n\t\tr = -EINVAL;\n\t}\n\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nstatic int amdgpu_gem_align_pitch(struct amdgpu_device *adev,\n\t\t\t\t  int width,\n\t\t\t\t  int cpp,\n\t\t\t\t  bool tiled)\n{\n\tint aligned = width;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = 255;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = 127;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = 63;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint amdgpu_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = file_priv->driver_priv;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tu64 flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\t    AMDGPU_GEM_CREATE_CPU_GTT_USWC |\n\t\t    AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tu32 domain;\n\tint r;\n\n\t/*\n\t * The buffer returned from this function should be cleared, but\n\t * it can only be done if the ring is enabled or we'll fail to\n\t * create the buffer.\n\t */\n\tif (adev->mman.buffer_funcs_enabled)\n\t\tflags |= AMDGPU_GEM_CREATE_VRAM_CLEARED;\n\n\targs->pitch = amdgpu_gem_align_pitch(adev, args->width,\n\t\t\t\t\t     DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\tdomain = amdgpu_bo_get_preferred_domain(adev,\n\t\t\t\tamdgpu_display_supported_domains(adev, flags));\n\tr = amdgpu_gem_object_create(adev, args->size, 0, domain, flags,\n\t\t\t\t     ttm_bo_type_device, NULL, &gobj, fpriv->xcp_id + 1);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t/* drop reference from allocate - handle holds it now */\n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\treturn r;\n\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_file *file;\n\tint r;\n\n\tr = mutex_lock_interruptible(&dev->filelist_mutex);\n\tif (r)\n\t\treturn r;\n\n\tlist_for_each_entry(file, &dev->filelist, lhead) {\n\t\tstruct task_struct *task;\n\t\tstruct drm_gem_object *gobj;\n\t\tstruct pid *pid;\n\t\tint id;\n\n\t\t/*\n\t\t * Although we have a valid reference on file->pid, that does\n\t\t * not guarantee that the task_struct who called get_pid() is\n\t\t * still alive (e.g. get_pid(current) => fork() => exit()).\n\t\t * Therefore, we need to protect this ->comm access using RCU.\n\t\t */\n\t\trcu_read_lock();\n\t\tpid = rcu_dereference(file->pid);\n\t\ttask = pid_task(pid, PIDTYPE_TGID);\n\t\tseq_printf(m, \"pid %8d command %s:\\n\", pid_nr(pid),\n\t\t\t   task ? task->comm : \"<unknown>\");\n\t\trcu_read_unlock();\n\n\t\tspin_lock(&file->table_lock);\n\t\tidr_for_each_entry(&file->object_idr, gobj, id) {\n\t\t\tstruct amdgpu_bo *bo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tamdgpu_bo_print_info(id, bo, m);\n\t\t}\n\t\tspin_unlock(&file->table_lock);\n\t}\n\n\tmutex_unlock(&dev->filelist_mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_gem_info);\n\n#endif\n\nvoid amdgpu_debugfs_gem_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_gem_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_gem_info_fops);\n#endif\n}\n", "source_code_path": "amdgpu_gem.c", "line_number": 50}
