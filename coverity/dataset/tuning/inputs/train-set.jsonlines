{"bug_report_path": "dataset/raw_data/bugs/dev-set/sprintf2-bug.txt", "bug_report_text": "CID: 492835\nType: Buffer overflow\nSeverity: High\nChecker: BUFFER_SIZE\nCategory: SECURE_CODING\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c\nFunction: amdgpu_fru_get_product_info\nLine: 131\n\nProblem:\nPotential buffer overflow when formatting device serial number using sprintf\n\nAbstract:\nThe code uses sprintf to write a hexadecimal string representation of a 64-bit \nunique ID into the fru_info->serial buffer without size checking. This could \nlead to a buffer overflow if the destination buffer is smaller than the \nformatted string length (including null terminator).\n\nPath:\n  drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c:131\n    sprintf(fru_info->serial, \"%llx\", adev->unique_id)\n\nDetails:\nConverting a 64-bit value to hexadecimal format using sprintf can produce up to \n16 characters plus a null terminator. Without bounds checking, this operation \nrisks writing beyond the end of the serial buffer if it's smaller than 17 bytes. \nThe code handles critical device identification information in kernel space, \nmaking buffer overflows particularly dangerous.\n\nFix:\nReplace sprintf with snprintf to enforce buffer size limits:\n  snprintf(fru_info->serial, sizeof(fru_info->serial), \"%llx\", adev->unique_id)\n", "diff_path": "dataset/raw_data/bugs/dev-set/sprintf2-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c\nindex ceb5163480f4..a4696976c809 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c\n@@ -131,7 +131,7 @@ int amdgpu_fru_get_product_info(struct amdgpu_device *adev)\n         * so convert it to a 16-digit HEX string for convenience and\n         * backwards-compatibility.\n         */\n-       sprintf(fru_info->serial, \"%llx\", adev->unique_id);\n+       snprintf(fru_info->serial, sizeof(fru_info->serial), \"%llx\", adev->unique_id);\n\n        /* If algo exists, it means that the i2c_adapter's initialized */\n        if (!adev->pm.fru_eeprom_i2c_bus || !adev->pm.fru_eeprom_i2c_bus->algo) {", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_fru_eeprom.c", "line_number": 131, "code": "/*\n * Copyright 2019 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n#include <linux/pci.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_i2c.h\"\n#include \"smu_v11_0_i2c.h\"\n#include \"atom.h\"\n#include \"amdgpu_fru_eeprom.h\"\n#include \"amdgpu_eeprom.h\"\n\n#define FRU_EEPROM_MADDR_6      0x60000\n#define FRU_EEPROM_MADDR_8      0x80000\n\nstatic bool is_fru_eeprom_supported(struct amdgpu_device *adev, u32 *fru_addr)\n{\n\t/* Only server cards have the FRU EEPROM\n\t * TODO: See if we can figure this out dynamically instead of\n\t * having to parse VBIOS versions.\n\t */\n\tstruct atom_context *atom_ctx = adev->mode_info.atom_context;\n\n\t/* The i2c access is blocked on VF\n\t * TODO: Need other way to get the info\n\t * Also, FRU not valid for APU devices.\n\t */\n\tif (amdgpu_sriov_vf(adev) || (adev->flags & AMD_IS_APU))\n\t\treturn false;\n\n\t/* The default I2C EEPROM address of the FRU.\n\t */\n\tif (fru_addr)\n\t\t*fru_addr = FRU_EEPROM_MADDR_8;\n\n\t/* VBIOS is of the format ###-DXXXYYYY-##. For SKU identification,\n\t * we can use just the \"DXXX\" portion. If there were more models, we\n\t * could convert the 3 characters to a hex integer and use a switch\n\t * for ease/speed/readability. For now, 2 string comparisons are\n\t * reasonable and not too expensive\n\t */\n\tswitch (amdgpu_ip_version(adev, MP1_HWIP, 0)) {\n\tcase IP_VERSION(11, 0, 2):\n\t\tswitch (adev->asic_type) {\n\t\tcase CHIP_VEGA20:\n\t\t\t/* D161 and D163 are the VG20 server SKUs */\n\t\t\tif (strnstr(atom_ctx->vbios_pn, \"D161\",\n\t\t\t\t    sizeof(atom_ctx->vbios_pn)) ||\n\t\t\t    strnstr(atom_ctx->vbios_pn, \"D163\",\n\t\t\t\t    sizeof(atom_ctx->vbios_pn))) {\n\t\t\t\tif (fru_addr)\n\t\t\t\t\t*fru_addr = FRU_EEPROM_MADDR_6;\n\t\t\t\treturn true;\n\t\t\t} else {\n\t\t\t\treturn false;\n\t\t\t}\n\t\tcase CHIP_ARCTURUS:\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase IP_VERSION(11, 0, 7):\n\t\tif (strnstr(atom_ctx->vbios_pn, \"D603\",\n\t\t\t    sizeof(atom_ctx->vbios_pn))) {\n\t\t\tif (strnstr(atom_ctx->vbios_pn, \"D603GLXE\",\n\t\t\t\t    sizeof(atom_ctx->vbios_pn))) {\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tif (fru_addr)\n\t\t\t\t*fru_addr = FRU_EEPROM_MADDR_6;\n\t\t\treturn true;\n\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\tcase IP_VERSION(13, 0, 2):\n\t\t/* All Aldebaran SKUs have an FRU */\n\t\tif (!strnstr(atom_ctx->vbios_pn, \"D673\",\n\t\t\t     sizeof(atom_ctx->vbios_pn)))\n\t\t\tif (fru_addr)\n\t\t\t\t*fru_addr = FRU_EEPROM_MADDR_6;\n\t\treturn true;\n\tcase IP_VERSION(13, 0, 6):\n\tcase IP_VERSION(13, 0, 14):\n\t\t\tif (fru_addr)\n\t\t\t\t*fru_addr = FRU_EEPROM_MADDR_8;\n\t\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nint amdgpu_fru_get_product_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_fru_info *fru_info;\n\tunsigned char buf[8], *pia;\n\tu32 addr, fru_addr;\n\tint size, len;\n\tu8 csum;\n\n\tif (!is_fru_eeprom_supported(adev, &fru_addr))\n\t\treturn 0;\n\n\tif (!adev->fru_info) {\n\t\tadev->fru_info = kzalloc(sizeof(*adev->fru_info), GFP_KERNEL);\n\t\tif (!adev->fru_info)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tfru_info = adev->fru_info;\n\t/* For Arcturus-and-later, default value of serial_number is unique_id\n\t * so convert it to a 16-digit HEX string for convenience and\n\t * backwards-compatibility.\n\t */\n\tsprintf(fru_info->serial, \"%llx\", adev->unique_id);\n\n\t/* If algo exists, it means that the i2c_adapter's initialized */\n\tif (!adev->pm.fru_eeprom_i2c_bus || !adev->pm.fru_eeprom_i2c_bus->algo) {\n\t\tDRM_WARN(\"Cannot access FRU, EEPROM accessor not initialized\");\n\t\treturn -ENODEV;\n\t}\n\n\t/* Read the IPMI Common header */\n\tlen = amdgpu_eeprom_read(adev->pm.fru_eeprom_i2c_bus, fru_addr, buf,\n\t\t\t\t sizeof(buf));\n\tif (len != 8) {\n\t\tDRM_ERROR(\"Couldn't read the IPMI Common Header: %d\", len);\n\t\treturn len < 0 ? len : -EIO;\n\t}\n\n\tif (buf[0] != 1) {\n\t\tDRM_ERROR(\"Bad IPMI Common Header version: 0x%02x\", buf[0]);\n\t\treturn -EIO;\n\t}\n\n\tfor (csum = 0; len > 0; len--)\n\t\tcsum += buf[len - 1];\n\tif (csum) {\n\t\tDRM_ERROR(\"Bad IPMI Common Header checksum: 0x%02x\", csum);\n\t\treturn -EIO;\n\t}\n\n\t/* Get the offset to the Product Info Area (PIA). */\n\taddr = buf[4] * 8;\n\tif (!addr)\n\t\treturn 0;\n\n\t/* Get the absolute address to the PIA. */\n\taddr += fru_addr;\n\n\t/* Read the header of the PIA. */\n\tlen = amdgpu_eeprom_read(adev->pm.fru_eeprom_i2c_bus, addr, buf, 3);\n\tif (len != 3) {\n\t\tDRM_ERROR(\"Couldn't read the Product Info Area header: %d\", len);\n\t\treturn len < 0 ? len : -EIO;\n\t}\n\n\tif (buf[0] != 1) {\n\t\tDRM_ERROR(\"Bad IPMI Product Info Area version: 0x%02x\", buf[0]);\n\t\treturn -EIO;\n\t}\n\n\tsize = buf[1] * 8;\n\tpia = kzalloc(size, GFP_KERNEL);\n\tif (!pia)\n\t\treturn -ENOMEM;\n\n\t/* Read the whole PIA. */\n\tlen = amdgpu_eeprom_read(adev->pm.fru_eeprom_i2c_bus, addr, pia, size);\n\tif (len != size) {\n\t\tkfree(pia);\n\t\tDRM_ERROR(\"Couldn't read the Product Info Area: %d\", len);\n\t\treturn len < 0 ? len : -EIO;\n\t}\n\n\tfor (csum = 0; size > 0; size--)\n\t\tcsum += pia[size - 1];\n\tif (csum) {\n\t\tDRM_ERROR(\"Bad Product Info Area checksum: 0x%02x\", csum);\n\t\tkfree(pia);\n\t\treturn -EIO;\n\t}\n\n\t/* Now extract useful information from the PIA.\n\t *\n\t * Read Manufacturer Name field whose length is [3].\n\t */\n\taddr = 3;\n\tif (addr + 1 >= len)\n\t\tgoto Out;\n\tmemcpy(fru_info->manufacturer_name, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->manufacturer_name),\n\t\t     pia[addr] & 0x3F));\n\tfru_info->manufacturer_name[sizeof(fru_info->manufacturer_name) - 1] =\n\t\t'\\0';\n\n\t/* Read Product Name field. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\tif (addr + 1 >= len)\n\t\tgoto Out;\n\tmemcpy(fru_info->product_name, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->product_name), pia[addr] & 0x3F));\n\tfru_info->product_name[sizeof(fru_info->product_name) - 1] = '\\0';\n\n\t/* Go to the Product Part/Model Number field. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\tif (addr + 1 >= len)\n\t\tgoto Out;\n\tmemcpy(fru_info->product_number, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->product_number),\n\t\t     pia[addr] & 0x3F));\n\tfru_info->product_number[sizeof(fru_info->product_number) - 1] = '\\0';\n\n\t/* Go to the Product Version field. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\n\t/* Go to the Product Serial Number field. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\tif (addr + 1 >= len)\n\t\tgoto Out;\n\tmemcpy(fru_info->serial, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->serial), pia[addr] & 0x3F));\n\tfru_info->serial[sizeof(fru_info->serial) - 1] = '\\0';\n\n\t/* Asset Tag field */\n\taddr += 1 + (pia[addr] & 0x3F);\n\n\t/* FRU File Id field. This could be 'null'. */\n\taddr += 1 + (pia[addr] & 0x3F);\n\tif ((addr + 1 >= len) || !(pia[addr] & 0x3F))\n\t\tgoto Out;\n\tmemcpy(fru_info->fru_id, pia + addr + 1,\n\t       min_t(size_t, sizeof(fru_info->fru_id), pia[addr] & 0x3F));\n\tfru_info->fru_id[sizeof(fru_info->fru_id) - 1] = '\\0';\n\nOut:\n\tkfree(pia);\n\treturn 0;\n}\n\n/**\n * DOC: product_name\n *\n * The amdgpu driver provides a sysfs API for reporting the product name\n * for the device\n * The file product_name is used for this and returns the product name\n * as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_product_name_show(struct device *dev,\n\t\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t\t    char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->product_name);\n}\n\nstatic DEVICE_ATTR(product_name, 0444, amdgpu_fru_product_name_show, NULL);\n\n/**\n * DOC: product_number\n *\n * The amdgpu driver provides a sysfs API for reporting the part number\n * for the device\n * The file product_number is used for this and returns the part number\n * as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_product_number_show(struct device *dev,\n\t\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t\t      char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->product_number);\n}\n\nstatic DEVICE_ATTR(product_number, 0444, amdgpu_fru_product_number_show, NULL);\n\n/**\n * DOC: serial_number\n *\n * The amdgpu driver provides a sysfs API for reporting the serial number\n * for the device\n * The file serial_number is used for this and returns the serial number\n * as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_serial_number_show(struct device *dev,\n\t\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t\t     char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->serial);\n}\n\nstatic DEVICE_ATTR(serial_number, 0444, amdgpu_fru_serial_number_show, NULL);\n\n/**\n * DOC: fru_id\n *\n * The amdgpu driver provides a sysfs API for reporting FRU File Id\n * for the device.\n * The file fru_id is used for this and returns the File Id value\n * as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_id_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->fru_id);\n}\n\nstatic DEVICE_ATTR(fru_id, 0444, amdgpu_fru_id_show, NULL);\n\n/**\n * DOC: manufacturer\n *\n * The amdgpu driver provides a sysfs API for reporting manufacturer name from\n * FRU information.\n * The file manufacturer returns the value as returned from the FRU.\n * NOTE: This is only available for certain server cards\n */\n\nstatic ssize_t amdgpu_fru_manufacturer_name_show(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", adev->fru_info->manufacturer_name);\n}\n\nstatic DEVICE_ATTR(manufacturer, 0444, amdgpu_fru_manufacturer_name_show, NULL);\n\nstatic const struct attribute *amdgpu_fru_attributes[] = {\n\t&dev_attr_product_name.attr,\n\t&dev_attr_product_number.attr,\n\t&dev_attr_serial_number.attr,\n\t&dev_attr_fru_id.attr,\n\t&dev_attr_manufacturer.attr,\n\tNULL\n};\n\nint amdgpu_fru_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (!is_fru_eeprom_supported(adev, NULL) || !adev->fru_info)\n\t\treturn 0;\n\n\treturn sysfs_create_files(&adev->dev->kobj, amdgpu_fru_attributes);\n}\n\nvoid amdgpu_fru_sysfs_fini(struct amdgpu_device *adev)\n{\n\tif (!is_fru_eeprom_supported(adev, NULL) || !adev->fru_info)\n\t\treturn;\n\n\tsysfs_remove_files(&adev->dev->kobj, amdgpu_fru_attributes);\n}\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/sprintf3-bug.txt", "bug_report_text": "CID: 501947\nType: Buffer overflow\nSeverity: High\nChecker: BUFFER_SIZE\nCategory: SECURE_CODING\nFile: drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c\nFunction: vcn_v2_0_sw_init\nLine: 169\n\nProblem: \nUnbounded string copy when initializing VCN decoder ring name using sprintf\n\nAbstract:\nThe code uses sprintf to write a fixed string into ring->name without size checking. \nAdditionally, there appears to be a syntax error in the fix attempt, as the sizeof \nparameter is malformed. This could potentially overflow the destination buffer and \ncorrupt adjacent memory.\n\nPath:\n drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c:169\n   sprintf(ring->name, \"vcn_dec\")\n\nDetails:\nThe VCN decoder ring initialization code copies a hardcoded string \"vcn_dec\" into \nthe ring name buffer using sprintf without verifying the buffer size. While the \nsource string is constant in this case, using unbounded string operations in \nkernel code is risky. Additionally, the attempted fix contains a syntax error in \nthe sizeof usage.\n\nFix:\nReplace with properly formatted snprintf call:\n snprintf(ring->name, sizeof(ring->name), \"vcn_dec\")", "diff_path": "dataset/raw_data/bugs/dev-set/sprintf3-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c b/drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c\nindex bfd067e2d2f1..6419ab97d922 100644\n--- a/drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c\n+++ b/drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c\n@@ -169,7 +169,7 @@ static int vcn_v2_0_sw_init(void *handle)\n        ring->doorbell_index = adev->doorbell_index.vcn.vcn_ring0_1 << 1;\n        ring->vm_hub = AMDGPU_MMHUB0(0);\n\n-       sprintf(ring->name, \"vcn_dec\");\n+       snprintf(ring->name, sizeof(ring->name), \"vcn_dec\");\n        r = amdgpu_ring_init(adev, ring, 512, &adev->vcn.inst->irq, 0,\n                             AMDGPU_RING_PRIO_DEFAULT, NULL);\n        if (r)", "source_code_path": "drivers/gpu/drm/amd/amdgpu/vcn_v2_0.c", "line_number": 169, "code": "/*\n * Copyright 2018 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/firmware.h>\n#include <drm/drm_drv.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_vcn.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_psp.h\"\n#include \"mmsch_v2_0.h\"\n#include \"vcn_v2_0.h\"\n\n#include \"vcn/vcn_2_0_0_offset.h\"\n#include \"vcn/vcn_2_0_0_sh_mask.h\"\n#include \"ivsrcid/vcn/irqsrcs_vcn_2_0.h\"\n\n#define VCN_VID_SOC_ADDRESS_2_0\t\t\t\t\t0x1fa00\n#define VCN1_VID_SOC_ADDRESS_3_0\t\t\t\t0x48200\n\n#define mmUVD_CONTEXT_ID_INTERNAL_OFFSET\t\t\t0x1fd\n#define mmUVD_GPCOM_VCPU_CMD_INTERNAL_OFFSET\t\t\t0x503\n#define mmUVD_GPCOM_VCPU_DATA0_INTERNAL_OFFSET\t\t\t0x504\n#define mmUVD_GPCOM_VCPU_DATA1_INTERNAL_OFFSET\t\t\t0x505\n#define mmUVD_NO_OP_INTERNAL_OFFSET\t\t\t\t0x53f\n#define mmUVD_GP_SCRATCH8_INTERNAL_OFFSET\t\t\t0x54a\n#define mmUVD_SCRATCH9_INTERNAL_OFFSET\t\t\t\t0xc01d\n\n#define mmUVD_LMI_RBC_IB_VMID_INTERNAL_OFFSET\t\t\t0x1e1\n#define mmUVD_LMI_RBC_IB_64BIT_BAR_HIGH_INTERNAL_OFFSET\t\t0x5a6\n#define mmUVD_LMI_RBC_IB_64BIT_BAR_LOW_INTERNAL_OFFSET\t\t0x5a7\n#define mmUVD_RBC_IB_SIZE_INTERNAL_OFFSET\t\t\t0x1e2\n\nstatic const struct amdgpu_hwip_reg_entry vcn_reg_list_2_0[] = {\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_POWER_STATUS),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_STATUS),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_CONTEXT_ID),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_CONTEXT_ID2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_GPCOM_VCPU_DATA0),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_GPCOM_VCPU_DATA1),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_GPCOM_VCPU_CMD),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_HI),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_LO),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_HI2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_LO2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_HI3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_LO3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_HI4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_BASE_LO4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_RPTR),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_WPTR),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_RPTR2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_WPTR2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_RPTR3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_WPTR3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_RPTR4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_WPTR4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_SIZE),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_SIZE2),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_SIZE3),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_RB_SIZE4),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_PGFSM_CONFIG),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_PGFSM_STATUS),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_DPG_LMA_CTL),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_DPG_LMA_DATA),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_DPG_LMA_MASK),\n\tSOC15_REG_ENTRY_STR(VCN, 0, mmUVD_DPG_PAUSE)\n};\n\nstatic void vcn_v2_0_set_dec_ring_funcs(struct amdgpu_device *adev);\nstatic void vcn_v2_0_set_enc_ring_funcs(struct amdgpu_device *adev);\nstatic void vcn_v2_0_set_irq_funcs(struct amdgpu_device *adev);\nstatic int vcn_v2_0_set_powergating_state(void *handle,\n\t\t\t\tenum amd_powergating_state state);\nstatic int vcn_v2_0_pause_dpg_mode(struct amdgpu_device *adev,\n\t\t\t\tint inst_idx, struct dpg_pause_state *new_state);\nstatic int vcn_v2_0_start_sriov(struct amdgpu_device *adev);\n/**\n * vcn_v2_0_early_init - set function pointers and load microcode\n *\n * @handle: amdgpu_device pointer\n *\n * Set ring and irq function pointers\n * Load microcode from filesystem\n */\nstatic int vcn_v2_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tadev->vcn.num_enc_rings = 1;\n\telse\n\t\tadev->vcn.num_enc_rings = 2;\n\n\tvcn_v2_0_set_dec_ring_funcs(adev);\n\tvcn_v2_0_set_enc_ring_funcs(adev);\n\tvcn_v2_0_set_irq_funcs(adev);\n\n\treturn amdgpu_vcn_early_init(adev);\n}\n\n/**\n * vcn_v2_0_sw_init - sw init for VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * Load firmware and sw initialization\n */\nstatic int vcn_v2_0_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tint i, r;\n\tuint32_t reg_count = ARRAY_SIZE(vcn_reg_list_2_0);\n\tuint32_t *ptr;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tvolatile struct amdgpu_fw_shared *fw_shared;\n\n\t/* VCN DEC TRAP */\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VCN,\n\t\t\t      VCN_2_0__SRCID__UVD_SYSTEM_MESSAGE_INTERRUPT,\n\t\t\t      &adev->vcn.inst->irq);\n\tif (r)\n\t\treturn r;\n\n\t/* VCN ENC TRAP */\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i) {\n\t\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VCN,\n\t\t\t\t      i + VCN_2_0__SRCID__UVD_ENC_GENERAL_PURPOSE,\n\t\t\t\t      &adev->vcn.inst->irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vcn_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_vcn_setup_ucode(adev);\n\n\tr = amdgpu_vcn_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tring = &adev->vcn.inst->ring_dec;\n\n\tring->use_doorbell = true;\n\tring->doorbell_index = adev->doorbell_index.vcn.vcn_ring0_1 << 1;\n\tring->vm_hub = AMDGPU_MMHUB0(0);\n\n\tsprintf(ring->name, \"vcn_dec\");\n\tr = amdgpu_ring_init(adev, ring, 512, &adev->vcn.inst->irq, 0,\n\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\tif (r)\n\t\treturn r;\n\n\tadev->vcn.internal.context_id = mmUVD_CONTEXT_ID_INTERNAL_OFFSET;\n\tadev->vcn.internal.ib_vmid = mmUVD_LMI_RBC_IB_VMID_INTERNAL_OFFSET;\n\tadev->vcn.internal.ib_bar_low = mmUVD_LMI_RBC_IB_64BIT_BAR_LOW_INTERNAL_OFFSET;\n\tadev->vcn.internal.ib_bar_high = mmUVD_LMI_RBC_IB_64BIT_BAR_HIGH_INTERNAL_OFFSET;\n\tadev->vcn.internal.ib_size = mmUVD_RBC_IB_SIZE_INTERNAL_OFFSET;\n\tadev->vcn.internal.gp_scratch8 = mmUVD_GP_SCRATCH8_INTERNAL_OFFSET;\n\n\tadev->vcn.internal.scratch9 = mmUVD_SCRATCH9_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.scratch9 = SOC15_REG_OFFSET(UVD, 0, mmUVD_SCRATCH9);\n\tadev->vcn.internal.data0 = mmUVD_GPCOM_VCPU_DATA0_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.data0 = SOC15_REG_OFFSET(UVD, 0, mmUVD_GPCOM_VCPU_DATA0);\n\tadev->vcn.internal.data1 = mmUVD_GPCOM_VCPU_DATA1_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.data1 = SOC15_REG_OFFSET(UVD, 0, mmUVD_GPCOM_VCPU_DATA1);\n\tadev->vcn.internal.cmd = mmUVD_GPCOM_VCPU_CMD_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.cmd = SOC15_REG_OFFSET(UVD, 0, mmUVD_GPCOM_VCPU_CMD);\n\tadev->vcn.internal.nop = mmUVD_NO_OP_INTERNAL_OFFSET;\n\tadev->vcn.inst->external.nop = SOC15_REG_OFFSET(UVD, 0, mmUVD_NO_OP);\n\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i) {\n\t\tenum amdgpu_ring_priority_level hw_prio = amdgpu_vcn_get_enc_ring_prio(i);\n\n\t\tring = &adev->vcn.inst->ring_enc[i];\n\t\tring->use_doorbell = true;\n\t\tring->vm_hub = AMDGPU_MMHUB0(0);\n\t\tif (!amdgpu_sriov_vf(adev))\n\t\t\tring->doorbell_index = (adev->doorbell_index.vcn.vcn_ring0_1 << 1) + 2 + i;\n\t\telse\n\t\t\tring->doorbell_index = (adev->doorbell_index.vcn.vcn_ring0_1 << 1) + 1 + i;\n\t\tsprintf(ring->name, \"vcn_enc%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 512, &adev->vcn.inst->irq, 0,\n\t\t\t\t     hw_prio, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tadev->vcn.pause_dpg_mode = vcn_v2_0_pause_dpg_mode;\n\n\tr = amdgpu_virt_alloc_mm_table(adev);\n\tif (r)\n\t\treturn r;\n\n\tfw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\tfw_shared->present_flag_0 = cpu_to_le32(AMDGPU_VCN_MULTI_QUEUE_FLAG);\n\n\tif (amdgpu_vcnfw_log)\n\t\tamdgpu_vcn_fwlog_init(adev->vcn.inst);\n\n\t/* Allocate memory for VCN IP Dump buffer */\n\tptr = kcalloc(adev->vcn.num_vcn_inst * reg_count, sizeof(uint32_t), GFP_KERNEL);\n\tif (!ptr) {\n\t\tDRM_ERROR(\"Failed to allocate memory for VCN IP Dump\\n\");\n\t\tadev->vcn.ip_dump = NULL;\n\t} else {\n\t\tadev->vcn.ip_dump = ptr;\n\t}\n\n\treturn 0;\n}\n\n/**\n * vcn_v2_0_sw_fini - sw fini for VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * VCN suspend and free up sw allocation\n */\nstatic int vcn_v2_0_sw_fini(void *handle)\n{\n\tint r, idx;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tvolatile struct amdgpu_fw_shared *fw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\n\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\tfw_shared->present_flag_0 = 0;\n\t\tdrm_dev_exit(idx);\n\t}\n\n\tamdgpu_virt_free_mm_table(adev);\n\n\tr = amdgpu_vcn_suspend(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vcn_sw_fini(adev);\n\n\tkfree(adev->vcn.ip_dump);\n\n\treturn r;\n}\n\n/**\n * vcn_v2_0_hw_init - start and test VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * Initialize the hardware, boot up the VCPU and do some testing\n */\nstatic int vcn_v2_0_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tstruct amdgpu_ring *ring = &adev->vcn.inst->ring_dec;\n\tint i, r;\n\n\tadev->nbio.funcs->vcn_doorbell_range(adev, ring->use_doorbell,\n\t\t\t\t\t     ring->doorbell_index, 0);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tvcn_v2_0_start_sriov(adev);\n\n\tr = amdgpu_ring_test_helper(ring);\n\tif (r)\n\t\treturn r;\n\n\t//Disable vcn decode for sriov\n\tif (amdgpu_sriov_vf(adev))\n\t\tring->sched.ready = false;\n\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i) {\n\t\tring = &adev->vcn.inst->ring_enc[i];\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\n/**\n * vcn_v2_0_hw_fini - stop the hardware block\n *\n * @handle: amdgpu_device pointer\n *\n * Stop the VCN block, mark ring as not ready any more\n */\nstatic int vcn_v2_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tcancel_delayed_work_sync(&adev->vcn.idle_work);\n\n\tif ((adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG) ||\n\t    (adev->vcn.cur_state != AMD_PG_STATE_GATE &&\n\t      RREG32_SOC15(VCN, 0, mmUVD_STATUS)))\n\t\tvcn_v2_0_set_powergating_state(adev, AMD_PG_STATE_GATE);\n\n\treturn 0;\n}\n\n/**\n * vcn_v2_0_suspend - suspend VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * HW fini and suspend VCN block\n */\nstatic int vcn_v2_0_suspend(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = vcn_v2_0_hw_fini(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vcn_suspend(adev);\n\n\treturn r;\n}\n\n/**\n * vcn_v2_0_resume - resume VCN block\n *\n * @handle: amdgpu_device pointer\n *\n * Resume firmware and hw init VCN block\n */\nstatic int vcn_v2_0_resume(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = amdgpu_vcn_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = vcn_v2_0_hw_init(adev);\n\n\treturn r;\n}\n\n/**\n * vcn_v2_0_mc_resume - memory controller programming\n *\n * @adev: amdgpu_device pointer\n *\n * Let the VCN memory controller know it's offsets\n */\nstatic void vcn_v2_0_mc_resume(struct amdgpu_device *adev)\n{\n\tuint32_t size = AMDGPU_GPU_PAGE_ALIGN(adev->vcn.fw[0]->size + 4);\n\tuint32_t offset;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\t/* cache window 0: fw */\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW,\n\t\t\t(adev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_lo));\n\t\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH,\n\t\t\t(adev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_hi));\n\t\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_OFFSET0, 0);\n\t\toffset = 0;\n\t} else {\n\t\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW,\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr));\n\t\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH,\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr));\n\t\toffset = size;\n\t\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_OFFSET0,\n\t\t\tAMDGPU_UVD_FIRMWARE_OFFSET >> 3);\n\t}\n\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_SIZE0, size);\n\n\t/* cache window 1: stack */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_LOW,\n\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_HIGH,\n\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset));\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_OFFSET1, 0);\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_SIZE1, AMDGPU_VCN_STACK_SIZE);\n\n\t/* cache window 2: context */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE2_64BIT_BAR_LOW,\n\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset + AMDGPU_VCN_STACK_SIZE));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_CACHE2_64BIT_BAR_HIGH,\n\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset + AMDGPU_VCN_STACK_SIZE));\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_OFFSET2, 0);\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_CACHE_SIZE2, AMDGPU_VCN_CONTEXT_SIZE);\n\n\t/* non-cache window */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_NC0_64BIT_BAR_LOW,\n\t\tlower_32_bits(adev->vcn.inst->fw_shared.gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_VCPU_NC0_64BIT_BAR_HIGH,\n\t\tupper_32_bits(adev->vcn.inst->fw_shared.gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_NONCACHE_OFFSET0, 0);\n\tWREG32_SOC15(UVD, 0, mmUVD_VCPU_NONCACHE_SIZE0,\n\t\tAMDGPU_GPU_PAGE_ALIGN(sizeof(struct amdgpu_fw_shared)));\n\n\tWREG32_SOC15(UVD, 0, mmUVD_GFX10_ADDR_CONFIG, adev->gfx.config.gb_addr_config);\n}\n\nstatic void vcn_v2_0_mc_resume_dpg_mode(struct amdgpu_device *adev, bool indirect)\n{\n\tuint32_t size = AMDGPU_GPU_PAGE_ALIGN(adev->vcn.fw[0]->size + 4);\n\tuint32_t offset;\n\n\t/* cache window 0: fw */\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\tif (!indirect) {\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW),\n\t\t\t\t(adev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_lo), 0, indirect);\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH),\n\t\t\t\t(adev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_hi), 0, indirect);\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET0), 0, 0, indirect);\n\t\t} else {\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW), 0, 0, indirect);\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH), 0, 0, indirect);\n\t\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET0), 0, 0, indirect);\n\t\t}\n\t\toffset = 0;\n\t} else {\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr), 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr), 0, indirect);\n\t\toffset = size;\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET0),\n\t\t\tAMDGPU_UVD_FIRMWARE_OFFSET >> 3, 0, indirect);\n\t}\n\n\tif (!indirect)\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_SIZE0), size, 0, indirect);\n\telse\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_SIZE0), 0, 0, indirect);\n\n\t/* cache window 1: stack */\n\tif (!indirect) {\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset), 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset), 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET1), 0, 0, indirect);\n\t} else {\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_LOW), 0, 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE1_64BIT_BAR_HIGH), 0, 0, indirect);\n\t\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET1), 0, 0, indirect);\n\t}\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_CACHE_SIZE1), AMDGPU_VCN_STACK_SIZE, 0, indirect);\n\n\t/* cache window 2: context */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE2_64BIT_BAR_LOW),\n\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset + AMDGPU_VCN_STACK_SIZE), 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_VCPU_CACHE2_64BIT_BAR_HIGH),\n\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset + AMDGPU_VCN_STACK_SIZE), 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_CACHE_OFFSET2), 0, 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_CACHE_SIZE2), AMDGPU_VCN_CONTEXT_SIZE, 0, indirect);\n\n\t/* non-cache window */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_VCPU_NC0_64BIT_BAR_LOW),\n\t\tlower_32_bits(adev->vcn.inst->fw_shared.gpu_addr), 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_VCPU_NC0_64BIT_BAR_HIGH),\n\t\tupper_32_bits(adev->vcn.inst->fw_shared.gpu_addr), 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_NONCACHE_OFFSET0), 0, 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_NONCACHE_SIZE0),\n\t\tAMDGPU_GPU_PAGE_ALIGN(sizeof(struct amdgpu_fw_shared)), 0, indirect);\n\n\t/* VCN global tiling registers */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_GFX10_ADDR_CONFIG), adev->gfx.config.gb_addr_config, 0, indirect);\n}\n\n/**\n * vcn_v2_0_disable_clock_gating - disable VCN clock gating\n *\n * @adev: amdgpu_device pointer\n *\n * Disable clock gating for VCN block\n */\nstatic void vcn_v2_0_disable_clock_gating(struct amdgpu_device *adev)\n{\n\tuint32_t data;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\t/* UVD disable CGC */\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL);\n\tif (adev->cg_flags & AMD_CG_SUPPORT_VCN_MGCG)\n\t\tdata |= 1 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\telse\n\t\tdata &= ~UVD_CGC_CTRL__DYN_CLOCK_MODE_MASK;\n\tdata |= 1 << UVD_CGC_CTRL__CLK_GATE_DLY_TIMER__SHIFT;\n\tdata |= 4 << UVD_CGC_CTRL__CLK_OFF_DELAY__SHIFT;\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_GATE);\n\tdata &= ~(UVD_CGC_GATE__SYS_MASK\n\t\t| UVD_CGC_GATE__UDEC_MASK\n\t\t| UVD_CGC_GATE__MPEG2_MASK\n\t\t| UVD_CGC_GATE__REGS_MASK\n\t\t| UVD_CGC_GATE__RBC_MASK\n\t\t| UVD_CGC_GATE__LMI_MC_MASK\n\t\t| UVD_CGC_GATE__LMI_UMC_MASK\n\t\t| UVD_CGC_GATE__IDCT_MASK\n\t\t| UVD_CGC_GATE__MPRD_MASK\n\t\t| UVD_CGC_GATE__MPC_MASK\n\t\t| UVD_CGC_GATE__LBSI_MASK\n\t\t| UVD_CGC_GATE__LRBBM_MASK\n\t\t| UVD_CGC_GATE__UDEC_RE_MASK\n\t\t| UVD_CGC_GATE__UDEC_CM_MASK\n\t\t| UVD_CGC_GATE__UDEC_IT_MASK\n\t\t| UVD_CGC_GATE__UDEC_DB_MASK\n\t\t| UVD_CGC_GATE__UDEC_MP_MASK\n\t\t| UVD_CGC_GATE__WCB_MASK\n\t\t| UVD_CGC_GATE__VCPU_MASK\n\t\t| UVD_CGC_GATE__SCPU_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_GATE, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL);\n\tdata &= ~(UVD_CGC_CTRL__UDEC_RE_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_CM_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_IT_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_DB_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_MP_MODE_MASK\n\t\t| UVD_CGC_CTRL__SYS_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPEG2_MODE_MASK\n\t\t| UVD_CGC_CTRL__REGS_MODE_MASK\n\t\t| UVD_CGC_CTRL__RBC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LMI_MC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LMI_UMC_MODE_MASK\n\t\t| UVD_CGC_CTRL__IDCT_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPRD_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LBSI_MODE_MASK\n\t\t| UVD_CGC_CTRL__LRBBM_MODE_MASK\n\t\t| UVD_CGC_CTRL__WCB_MODE_MASK\n\t\t| UVD_CGC_CTRL__VCPU_MODE_MASK\n\t\t| UVD_CGC_CTRL__SCPU_MODE_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL, data);\n\n\t/* turn on */\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_GATE);\n\tdata |= (UVD_SUVD_CGC_GATE__SRE_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_MASK\n\t\t| UVD_SUVD_CGC_GATE__SMP_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCM_MASK\n\t\t| UVD_SUVD_CGC_GATE__SDB_MASK\n\t\t| UVD_SUVD_CGC_GATE__SRE_H264_MASK\n\t\t| UVD_SUVD_CGC_GATE__SRE_HEVC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_H264_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_HEVC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCM_H264_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCM_HEVC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SDB_H264_MASK\n\t\t| UVD_SUVD_CGC_GATE__SDB_HEVC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCLR_MASK\n\t\t| UVD_SUVD_CGC_GATE__UVD_SC_MASK\n\t\t| UVD_SUVD_CGC_GATE__ENT_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_HEVC_DEC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_HEVC_ENC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SITE_MASK\n\t\t| UVD_SUVD_CGC_GATE__SRE_VP9_MASK\n\t\t| UVD_SUVD_CGC_GATE__SCM_VP9_MASK\n\t\t| UVD_SUVD_CGC_GATE__SIT_VP9_DEC_MASK\n\t\t| UVD_SUVD_CGC_GATE__SDB_VP9_MASK\n\t\t| UVD_SUVD_CGC_GATE__IME_HEVC_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_GATE, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_CTRL);\n\tdata &= ~(UVD_SUVD_CGC_CTRL__SRE_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SIT_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SMP_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SCM_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SDB_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SCLR_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__UVD_SC_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__ENT_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__IME_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SITE_MODE_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_CTRL, data);\n}\n\nstatic void vcn_v2_0_clock_gating_dpg_mode(struct amdgpu_device *adev,\n\t\tuint8_t sram_sel, uint8_t indirect)\n{\n\tuint32_t reg_data = 0;\n\n\t/* enable sw clock gating control */\n\tif (adev->cg_flags & AMD_CG_SUPPORT_VCN_MGCG)\n\t\treg_data = 1 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\telse\n\t\treg_data = 0 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\treg_data |= 1 << UVD_CGC_CTRL__CLK_GATE_DLY_TIMER__SHIFT;\n\treg_data |= 4 << UVD_CGC_CTRL__CLK_OFF_DELAY__SHIFT;\n\treg_data &= ~(UVD_CGC_CTRL__UDEC_RE_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_CM_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_IT_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_DB_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_MP_MODE_MASK |\n\t\t UVD_CGC_CTRL__SYS_MODE_MASK |\n\t\t UVD_CGC_CTRL__UDEC_MODE_MASK |\n\t\t UVD_CGC_CTRL__MPEG2_MODE_MASK |\n\t\t UVD_CGC_CTRL__REGS_MODE_MASK |\n\t\t UVD_CGC_CTRL__RBC_MODE_MASK |\n\t\t UVD_CGC_CTRL__LMI_MC_MODE_MASK |\n\t\t UVD_CGC_CTRL__LMI_UMC_MODE_MASK |\n\t\t UVD_CGC_CTRL__IDCT_MODE_MASK |\n\t\t UVD_CGC_CTRL__MPRD_MODE_MASK |\n\t\t UVD_CGC_CTRL__MPC_MODE_MASK |\n\t\t UVD_CGC_CTRL__LBSI_MODE_MASK |\n\t\t UVD_CGC_CTRL__LRBBM_MODE_MASK |\n\t\t UVD_CGC_CTRL__WCB_MODE_MASK |\n\t\t UVD_CGC_CTRL__VCPU_MODE_MASK |\n\t\t UVD_CGC_CTRL__SCPU_MODE_MASK);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_CGC_CTRL), reg_data, sram_sel, indirect);\n\n\t/* turn off clock gating */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_CGC_GATE), 0, sram_sel, indirect);\n\n\t/* turn on SUVD clock gating */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_SUVD_CGC_GATE), 1, sram_sel, indirect);\n\n\t/* turn on sw mode in UVD_SUVD_CGC_CTRL */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_SUVD_CGC_CTRL), 0, sram_sel, indirect);\n}\n\n/**\n * vcn_v2_0_enable_clock_gating - enable VCN clock gating\n *\n * @adev: amdgpu_device pointer\n *\n * Enable clock gating for VCN block\n */\nstatic void vcn_v2_0_enable_clock_gating(struct amdgpu_device *adev)\n{\n\tuint32_t data = 0;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\t/* enable UVD CGC */\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL);\n\tif (adev->cg_flags & AMD_CG_SUPPORT_VCN_MGCG)\n\t\tdata |= 1 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\telse\n\t\tdata |= 0 << UVD_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\tdata |= 1 << UVD_CGC_CTRL__CLK_GATE_DLY_TIMER__SHIFT;\n\tdata |= 4 << UVD_CGC_CTRL__CLK_OFF_DELAY__SHIFT;\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL);\n\tdata |= (UVD_CGC_CTRL__UDEC_RE_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_CM_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_IT_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_DB_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_MP_MODE_MASK\n\t\t| UVD_CGC_CTRL__SYS_MODE_MASK\n\t\t| UVD_CGC_CTRL__UDEC_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPEG2_MODE_MASK\n\t\t| UVD_CGC_CTRL__REGS_MODE_MASK\n\t\t| UVD_CGC_CTRL__RBC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LMI_MC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LMI_UMC_MODE_MASK\n\t\t| UVD_CGC_CTRL__IDCT_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPRD_MODE_MASK\n\t\t| UVD_CGC_CTRL__MPC_MODE_MASK\n\t\t| UVD_CGC_CTRL__LBSI_MODE_MASK\n\t\t| UVD_CGC_CTRL__LRBBM_MODE_MASK\n\t\t| UVD_CGC_CTRL__WCB_MODE_MASK\n\t\t| UVD_CGC_CTRL__VCPU_MODE_MASK\n\t\t| UVD_CGC_CTRL__SCPU_MODE_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_CGC_CTRL, data);\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_CTRL);\n\tdata |= (UVD_SUVD_CGC_CTRL__SRE_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SIT_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SMP_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SCM_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SDB_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SCLR_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__UVD_SC_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__ENT_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__IME_MODE_MASK\n\t\t| UVD_SUVD_CGC_CTRL__SITE_MODE_MASK);\n\tWREG32_SOC15(VCN, 0, mmUVD_SUVD_CGC_CTRL, data);\n}\n\nstatic void vcn_v2_0_disable_static_power_gating(struct amdgpu_device *adev)\n{\n\tuint32_t data = 0;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN) {\n\t\tdata = (1 << UVD_PGFSM_CONFIG__UVDM_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDU_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDF_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDC_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDB_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDIL_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDIR_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDTD_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDTE_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDE_PWR_CONFIG__SHIFT);\n\n\t\tWREG32_SOC15(VCN, 0, mmUVD_PGFSM_CONFIG, data);\n\t\tSOC15_WAIT_ON_RREG(VCN, 0, mmUVD_PGFSM_STATUS,\n\t\t\tUVD_PGFSM_STATUS__UVDM_UVDU_PWR_ON_2_0, 0xFFFFF);\n\t} else {\n\t\tdata = (1 << UVD_PGFSM_CONFIG__UVDM_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDU_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDF_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDC_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDB_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDIL_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDIR_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDTD_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDTE_PWR_CONFIG__SHIFT\n\t\t\t| 1 << UVD_PGFSM_CONFIG__UVDE_PWR_CONFIG__SHIFT);\n\t\tWREG32_SOC15(VCN, 0, mmUVD_PGFSM_CONFIG, data);\n\t\tSOC15_WAIT_ON_RREG(VCN, 0, mmUVD_PGFSM_STATUS, 0,  0xFFFFF);\n\t}\n\n\t/* polling UVD_PGFSM_STATUS to confirm UVDM_PWR_STATUS,\n\t * UVDU_PWR_STATUS are 0 (power on) */\n\n\tdata = RREG32_SOC15(VCN, 0, mmUVD_POWER_STATUS);\n\tdata &= ~0x103;\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN)\n\t\tdata |= UVD_PGFSM_CONFIG__UVDM_UVDU_PWR_ON |\n\t\t\tUVD_POWER_STATUS__UVD_PG_EN_MASK;\n\n\tWREG32_SOC15(VCN, 0, mmUVD_POWER_STATUS, data);\n}\n\nstatic void vcn_v2_0_enable_static_power_gating(struct amdgpu_device *adev)\n{\n\tuint32_t data = 0;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN) {\n\t\t/* Before power off, this indicator has to be turned on */\n\t\tdata = RREG32_SOC15(VCN, 0, mmUVD_POWER_STATUS);\n\t\tdata &= ~UVD_POWER_STATUS__UVD_POWER_STATUS_MASK;\n\t\tdata |= UVD_POWER_STATUS__UVD_POWER_STATUS_TILES_OFF;\n\t\tWREG32_SOC15(VCN, 0, mmUVD_POWER_STATUS, data);\n\n\n\t\tdata = (2 << UVD_PGFSM_CONFIG__UVDM_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDU_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDF_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDC_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDB_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDIL_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDIR_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDTD_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDTE_PWR_CONFIG__SHIFT\n\t\t\t| 2 << UVD_PGFSM_CONFIG__UVDE_PWR_CONFIG__SHIFT);\n\n\t\tWREG32_SOC15(VCN, 0, mmUVD_PGFSM_CONFIG, data);\n\n\t\tdata = (2 << UVD_PGFSM_STATUS__UVDM_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDU_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDF_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDC_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDB_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDIL_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDIR_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDTD_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDTE_PWR_STATUS__SHIFT\n\t\t\t| 2 << UVD_PGFSM_STATUS__UVDE_PWR_STATUS__SHIFT);\n\t\tSOC15_WAIT_ON_RREG(VCN, 0, mmUVD_PGFSM_STATUS, data, 0xFFFFF);\n\t}\n}\n\nstatic int vcn_v2_0_start_dpg_mode(struct amdgpu_device *adev, bool indirect)\n{\n\tvolatile struct amdgpu_fw_shared *fw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\tstruct amdgpu_ring *ring = &adev->vcn.inst->ring_dec;\n\tuint32_t rb_bufsz, tmp;\n\n\tvcn_v2_0_enable_static_power_gating(adev);\n\n\t/* enable dynamic power gating mode */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_POWER_STATUS);\n\ttmp |= UVD_POWER_STATUS__UVD_PG_MODE_MASK;\n\ttmp |= UVD_POWER_STATUS__UVD_PG_EN_MASK;\n\tWREG32_SOC15(UVD, 0, mmUVD_POWER_STATUS, tmp);\n\n\tif (indirect)\n\t\tadev->vcn.inst->dpg_sram_curr_addr = (uint32_t *)adev->vcn.inst->dpg_sram_cpu_addr;\n\n\t/* enable clock gating */\n\tvcn_v2_0_clock_gating_dpg_mode(adev, 0, indirect);\n\n\t/* enable VCPU clock */\n\ttmp = (0xFF << UVD_VCPU_CNTL__PRB_TIMEOUT_VAL__SHIFT);\n\ttmp |= UVD_VCPU_CNTL__CLK_EN_MASK;\n\ttmp |= UVD_VCPU_CNTL__MIF_WR_LOW_THRESHOLD_BP_MASK;\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_VCPU_CNTL), tmp, 0, indirect);\n\n\t/* disable master interupt */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MASTINT_EN), 0, 0, indirect);\n\n\t/* setup mmUVD_LMI_CTRL */\n\ttmp = (UVD_LMI_CTRL__WRITE_CLEAN_TIMER_EN_MASK |\n\t\tUVD_LMI_CTRL__REQ_MODE_MASK |\n\t\tUVD_LMI_CTRL__CRC_RESET_MASK |\n\t\tUVD_LMI_CTRL__MASK_MC_URGENT_MASK |\n\t\tUVD_LMI_CTRL__DATA_COHERENCY_EN_MASK |\n\t\tUVD_LMI_CTRL__VCPU_DATA_COHERENCY_EN_MASK |\n\t\t(8 << UVD_LMI_CTRL__WRITE_CLEAN_TIMER__SHIFT) |\n\t\t0x00100000L);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_CTRL), tmp, 0, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MPC_CNTL),\n\t\t0x2 << UVD_MPC_CNTL__REPLACEMENT_MODE__SHIFT, 0, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MPC_SET_MUXA0),\n\t\t((0x1 << UVD_MPC_SET_MUXA0__VARA_1__SHIFT) |\n\t\t (0x2 << UVD_MPC_SET_MUXA0__VARA_2__SHIFT) |\n\t\t (0x3 << UVD_MPC_SET_MUXA0__VARA_3__SHIFT) |\n\t\t (0x4 << UVD_MPC_SET_MUXA0__VARA_4__SHIFT)), 0, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MPC_SET_MUXB0),\n\t\t((0x1 << UVD_MPC_SET_MUXB0__VARB_1__SHIFT) |\n\t\t (0x2 << UVD_MPC_SET_MUXB0__VARB_2__SHIFT) |\n\t\t (0x3 << UVD_MPC_SET_MUXB0__VARB_3__SHIFT) |\n\t\t (0x4 << UVD_MPC_SET_MUXB0__VARB_4__SHIFT)), 0, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MPC_SET_MUX),\n\t\t((0x0 << UVD_MPC_SET_MUX__SET_0__SHIFT) |\n\t\t (0x1 << UVD_MPC_SET_MUX__SET_1__SHIFT) |\n\t\t (0x2 << UVD_MPC_SET_MUX__SET_2__SHIFT)), 0, indirect);\n\n\tvcn_v2_0_mc_resume_dpg_mode(adev, indirect);\n\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_REG_XX_MASK), 0x10, 0, indirect);\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_RBC_XX_IB_REG_CHECK), 0x3, 0, indirect);\n\n\t/* release VCPU reset to boot */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_SOFT_RESET), 0, 0, indirect);\n\n\t/* enable LMI MC and UMC channels */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_LMI_CTRL2),\n\t\t0x1F << UVD_LMI_CTRL2__RE_OFLD_MIF_WR_REQ_NUM__SHIFT, 0, indirect);\n\n\t/* enable master interrupt */\n\tWREG32_SOC15_DPG_MODE(0, SOC15_DPG_MODE_OFFSET(\n\t\tUVD, 0, mmUVD_MASTINT_EN),\n\t\tUVD_MASTINT_EN__VCPU_EN_MASK, 0, indirect);\n\n\tif (indirect)\n\t\tamdgpu_vcn_psp_update_sram(adev, 0, 0);\n\n\t/* force RBC into idle state */\n\trb_bufsz = order_base_2(ring->ring_size);\n\ttmp = REG_SET_FIELD(0, UVD_RBC_RB_CNTL, RB_BUFSZ, rb_bufsz);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_BLKSZ, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_FETCH, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_UPDATE, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_RPTR_WR_EN, 1);\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_CNTL, tmp);\n\n\t/* Stall DPG before WPTR/RPTR reset */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS),\n\t\tUVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK,\n\t\t~UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK);\n\tfw_shared->multi_queue.decode_queue_mode |= FW_QUEUE_RING_RESET;\n\n\t/* set the write pointer delay */\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR_CNTL, 0);\n\n\t/* set the wb address */\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR_ADDR,\n\t\t(upper_32_bits(ring->gpu_addr) >> 2));\n\n\t/* program the RB_BASE for ring buffer */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_64BIT_BAR_LOW,\n\t\tlower_32_bits(ring->gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_64BIT_BAR_HIGH,\n\t\tupper_32_bits(ring->gpu_addr));\n\n\t/* Initialize the ring buffer's read and write pointers */\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR, 0);\n\n\tWREG32_SOC15(UVD, 0, mmUVD_SCRATCH2, 0);\n\n\tring->wptr = RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR);\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR,\n\t\tlower_32_bits(ring->wptr));\n\n\tfw_shared->multi_queue.decode_queue_mode &= ~FW_QUEUE_RING_RESET;\n\t/* Unstall DPG */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS),\n\t\t0, ~UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK);\n\treturn 0;\n}\n\nstatic int vcn_v2_0_start(struct amdgpu_device *adev)\n{\n\tvolatile struct amdgpu_fw_shared *fw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\tstruct amdgpu_ring *ring = &adev->vcn.inst->ring_dec;\n\tuint32_t rb_bufsz, tmp;\n\tuint32_t lmi_swap_cntl;\n\tint i, j, r;\n\n\tif (adev->pm.dpm_enabled)\n\t\tamdgpu_dpm_enable_uvd(adev, true);\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG)\n\t\treturn vcn_v2_0_start_dpg_mode(adev, adev->vcn.indirect_sram);\n\n\tvcn_v2_0_disable_static_power_gating(adev);\n\n\t/* set uvd status busy */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_STATUS) | UVD_STATUS__UVD_BUSY;\n\tWREG32_SOC15(UVD, 0, mmUVD_STATUS, tmp);\n\n\t/*SW clock gating */\n\tvcn_v2_0_disable_clock_gating(adev);\n\n\t/* enable VCPU clock */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_VCPU_CNTL),\n\t\tUVD_VCPU_CNTL__CLK_EN_MASK, ~UVD_VCPU_CNTL__CLK_EN_MASK);\n\n\t/* disable master interrupt */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_MASTINT_EN), 0,\n\t\t~UVD_MASTINT_EN__VCPU_EN_MASK);\n\n\t/* setup mmUVD_LMI_CTRL */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_LMI_CTRL);\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_CTRL, tmp |\n\t\tUVD_LMI_CTRL__WRITE_CLEAN_TIMER_EN_MASK\t|\n\t\tUVD_LMI_CTRL__MASK_MC_URGENT_MASK |\n\t\tUVD_LMI_CTRL__DATA_COHERENCY_EN_MASK |\n\t\tUVD_LMI_CTRL__VCPU_DATA_COHERENCY_EN_MASK);\n\n\t/* setup mmUVD_MPC_CNTL */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_MPC_CNTL);\n\ttmp &= ~UVD_MPC_CNTL__REPLACEMENT_MODE_MASK;\n\ttmp |= 0x2 << UVD_MPC_CNTL__REPLACEMENT_MODE__SHIFT;\n\tWREG32_SOC15(VCN, 0, mmUVD_MPC_CNTL, tmp);\n\n\t/* setup UVD_MPC_SET_MUXA0 */\n\tWREG32_SOC15(UVD, 0, mmUVD_MPC_SET_MUXA0,\n\t\t((0x1 << UVD_MPC_SET_MUXA0__VARA_1__SHIFT) |\n\t\t(0x2 << UVD_MPC_SET_MUXA0__VARA_2__SHIFT) |\n\t\t(0x3 << UVD_MPC_SET_MUXA0__VARA_3__SHIFT) |\n\t\t(0x4 << UVD_MPC_SET_MUXA0__VARA_4__SHIFT)));\n\n\t/* setup UVD_MPC_SET_MUXB0 */\n\tWREG32_SOC15(UVD, 0, mmUVD_MPC_SET_MUXB0,\n\t\t((0x1 << UVD_MPC_SET_MUXB0__VARB_1__SHIFT) |\n\t\t(0x2 << UVD_MPC_SET_MUXB0__VARB_2__SHIFT) |\n\t\t(0x3 << UVD_MPC_SET_MUXB0__VARB_3__SHIFT) |\n\t\t(0x4 << UVD_MPC_SET_MUXB0__VARB_4__SHIFT)));\n\n\t/* setup mmUVD_MPC_SET_MUX */\n\tWREG32_SOC15(UVD, 0, mmUVD_MPC_SET_MUX,\n\t\t((0x0 << UVD_MPC_SET_MUX__SET_0__SHIFT) |\n\t\t(0x1 << UVD_MPC_SET_MUX__SET_1__SHIFT) |\n\t\t(0x2 << UVD_MPC_SET_MUX__SET_2__SHIFT)));\n\n\tvcn_v2_0_mc_resume(adev);\n\n\t/* release VCPU reset to boot */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET), 0,\n\t\t~UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK);\n\n\t/* enable LMI MC and UMC channels */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_LMI_CTRL2), 0,\n\t\t~UVD_LMI_CTRL2__STALL_ARB_UMC_MASK);\n\n\ttmp = RREG32_SOC15(VCN, 0, mmUVD_SOFT_RESET);\n\ttmp &= ~UVD_SOFT_RESET__LMI_SOFT_RESET_MASK;\n\ttmp &= ~UVD_SOFT_RESET__LMI_UMC_SOFT_RESET_MASK;\n\tWREG32_SOC15(VCN, 0, mmUVD_SOFT_RESET, tmp);\n\n\t/* disable byte swapping */\n\tlmi_swap_cntl = 0;\n#ifdef __BIG_ENDIAN\n\t/* swap (8 in 32) RB and IB */\n\tlmi_swap_cntl = 0xa;\n#endif\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_SWAP_CNTL, lmi_swap_cntl);\n\n\tfor (i = 0; i < 10; ++i) {\n\t\tuint32_t status;\n\n\t\tfor (j = 0; j < 100; ++j) {\n\t\t\tstatus = RREG32_SOC15(UVD, 0, mmUVD_STATUS);\n\t\t\tif (status & 2)\n\t\t\t\tbreak;\n\t\t\tmdelay(10);\n\t\t}\n\t\tr = 0;\n\t\tif (status & 2)\n\t\t\tbreak;\n\n\t\tDRM_ERROR(\"VCN decode not responding, trying to reset the VCPU!!!\\n\");\n\t\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET),\n\t\t\tUVD_SOFT_RESET__VCPU_SOFT_RESET_MASK,\n\t\t\t~UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK);\n\t\tmdelay(10);\n\t\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET), 0,\n\t\t\t~UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK);\n\t\tmdelay(10);\n\t\tr = -1;\n\t}\n\n\tif (r) {\n\t\tDRM_ERROR(\"VCN decode not responding, giving up!!!\\n\");\n\t\treturn r;\n\t}\n\n\t/* enable master interrupt */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_MASTINT_EN),\n\t\tUVD_MASTINT_EN__VCPU_EN_MASK,\n\t\t~UVD_MASTINT_EN__VCPU_EN_MASK);\n\n\t/* clear the busy bit of VCN_STATUS */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_STATUS), 0,\n\t\t~(2 << UVD_STATUS__VCPU_REPORT__SHIFT));\n\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_VMID, 0);\n\n\t/* force RBC into idle state */\n\trb_bufsz = order_base_2(ring->ring_size);\n\ttmp = REG_SET_FIELD(0, UVD_RBC_RB_CNTL, RB_BUFSZ, rb_bufsz);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_BLKSZ, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_FETCH, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_UPDATE, 1);\n\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_RPTR_WR_EN, 1);\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_CNTL, tmp);\n\n\tfw_shared->multi_queue.decode_queue_mode |= FW_QUEUE_RING_RESET;\n\t/* program the RB_BASE for ring buffer */\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_64BIT_BAR_LOW,\n\t\tlower_32_bits(ring->gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_LMI_RBC_RB_64BIT_BAR_HIGH,\n\t\tupper_32_bits(ring->gpu_addr));\n\n\t/* Initialize the ring buffer's read and write pointers */\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR, 0);\n\n\tring->wptr = RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR);\n\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR,\n\t\t\tlower_32_bits(ring->wptr));\n\tfw_shared->multi_queue.decode_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\tfw_shared->multi_queue.encode_generalpurpose_queue_mode |= FW_QUEUE_RING_RESET;\n\tring = &adev->vcn.inst->ring_enc[0];\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_RPTR, lower_32_bits(ring->wptr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR, lower_32_bits(ring->wptr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_LO, ring->gpu_addr);\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_HI, upper_32_bits(ring->gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_SIZE, ring->ring_size / 4);\n\tfw_shared->multi_queue.encode_generalpurpose_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\tfw_shared->multi_queue.encode_lowlatency_queue_mode |= FW_QUEUE_RING_RESET;\n\tring = &adev->vcn.inst->ring_enc[1];\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_RPTR2, lower_32_bits(ring->wptr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2, lower_32_bits(ring->wptr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_LO2, ring->gpu_addr);\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_HI2, upper_32_bits(ring->gpu_addr));\n\tWREG32_SOC15(UVD, 0, mmUVD_RB_SIZE2, ring->ring_size / 4);\n\tfw_shared->multi_queue.encode_lowlatency_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\treturn 0;\n}\n\nstatic int vcn_v2_0_stop_dpg_mode(struct amdgpu_device *adev)\n{\n\tstruct dpg_pause_state state = {.fw_based = VCN_DPG_STATE__UNPAUSE};\n\tuint32_t tmp;\n\n\tvcn_v2_0_pause_dpg_mode(adev, 0, &state);\n\t/* Wait for power status to be 1 */\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_POWER_STATUS, 1,\n\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK);\n\n\t/* wait for read ptr to be equal to write ptr */\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_RB_WPTR);\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_RB_RPTR, tmp, 0xFFFFFFFF);\n\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2);\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_RB_RPTR2, tmp, 0xFFFFFFFF);\n\n\ttmp = RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR) & 0x7FFFFFFF;\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_RBC_RB_RPTR, tmp, 0xFFFFFFFF);\n\n\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_POWER_STATUS, 1,\n\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK);\n\n\t/* disable dynamic power gating mode */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS), 0,\n\t\t\t~UVD_POWER_STATUS__UVD_PG_MODE_MASK);\n\n\treturn 0;\n}\n\nstatic int vcn_v2_0_stop(struct amdgpu_device *adev)\n{\n\tuint32_t tmp;\n\tint r;\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG) {\n\t\tr = vcn_v2_0_stop_dpg_mode(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgoto power_off;\n\t}\n\n\t/* wait for uvd idle */\n\tr = SOC15_WAIT_ON_RREG(VCN, 0, mmUVD_STATUS, UVD_STATUS__IDLE, 0x7);\n\tif (r)\n\t\treturn r;\n\n\ttmp = UVD_LMI_STATUS__VCPU_LMI_WRITE_CLEAN_MASK |\n\t\tUVD_LMI_STATUS__READ_CLEAN_MASK |\n\t\tUVD_LMI_STATUS__WRITE_CLEAN_MASK |\n\t\tUVD_LMI_STATUS__WRITE_CLEAN_RAW_MASK;\n\tr = SOC15_WAIT_ON_RREG(VCN, 0, mmUVD_LMI_STATUS, tmp, tmp);\n\tif (r)\n\t\treturn r;\n\n\t/* stall UMC channel */\n\ttmp = RREG32_SOC15(VCN, 0, mmUVD_LMI_CTRL2);\n\ttmp |= UVD_LMI_CTRL2__STALL_ARB_UMC_MASK;\n\tWREG32_SOC15(VCN, 0, mmUVD_LMI_CTRL2, tmp);\n\n\ttmp = UVD_LMI_STATUS__UMC_READ_CLEAN_RAW_MASK|\n\t\tUVD_LMI_STATUS__UMC_WRITE_CLEAN_RAW_MASK;\n\tr = SOC15_WAIT_ON_RREG(VCN, 0, mmUVD_LMI_STATUS, tmp, tmp);\n\tif (r)\n\t\treturn r;\n\n\t/* disable VCPU clock */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_VCPU_CNTL), 0,\n\t\t~(UVD_VCPU_CNTL__CLK_EN_MASK));\n\n\t/* reset LMI UMC */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET),\n\t\tUVD_SOFT_RESET__LMI_UMC_SOFT_RESET_MASK,\n\t\t~UVD_SOFT_RESET__LMI_UMC_SOFT_RESET_MASK);\n\n\t/* reset LMI */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET),\n\t\tUVD_SOFT_RESET__LMI_SOFT_RESET_MASK,\n\t\t~UVD_SOFT_RESET__LMI_SOFT_RESET_MASK);\n\n\t/* reset VCPU */\n\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_SOFT_RESET),\n\t\tUVD_SOFT_RESET__VCPU_SOFT_RESET_MASK,\n\t\t~UVD_SOFT_RESET__VCPU_SOFT_RESET_MASK);\n\n\t/* clear status */\n\tWREG32_SOC15(VCN, 0, mmUVD_STATUS, 0);\n\n\tvcn_v2_0_enable_clock_gating(adev);\n\tvcn_v2_0_enable_static_power_gating(adev);\n\npower_off:\n\tif (adev->pm.dpm_enabled)\n\t\tamdgpu_dpm_enable_uvd(adev, false);\n\n\treturn 0;\n}\n\nstatic int vcn_v2_0_pause_dpg_mode(struct amdgpu_device *adev,\n\t\t\t\tint inst_idx, struct dpg_pause_state *new_state)\n{\n\tstruct amdgpu_ring *ring;\n\tuint32_t reg_data = 0;\n\tint ret_code;\n\n\t/* pause/unpause if state is changed */\n\tif (adev->vcn.inst[inst_idx].pause_state.fw_based != new_state->fw_based) {\n\t\tDRM_DEBUG(\"dpg pause state changed %d -> %d\",\n\t\t\tadev->vcn.inst[inst_idx].pause_state.fw_based,\tnew_state->fw_based);\n\t\treg_data = RREG32_SOC15(UVD, 0, mmUVD_DPG_PAUSE) &\n\t\t\t(~UVD_DPG_PAUSE__NJ_PAUSE_DPG_ACK_MASK);\n\n\t\tif (new_state->fw_based == VCN_DPG_STATE__PAUSE) {\n\t\t\tret_code = SOC15_WAIT_ON_RREG(UVD, 0, mmUVD_POWER_STATUS, 0x1,\n\t\t\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK);\n\n\t\t\tif (!ret_code) {\n\t\t\t\tvolatile struct amdgpu_fw_shared *fw_shared = adev->vcn.inst->fw_shared.cpu_addr;\n\t\t\t\t/* pause DPG */\n\t\t\t\treg_data |= UVD_DPG_PAUSE__NJ_PAUSE_DPG_REQ_MASK;\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_DPG_PAUSE, reg_data);\n\n\t\t\t\t/* wait for ACK */\n\t\t\t\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_DPG_PAUSE,\n\t\t\t\t\t   UVD_DPG_PAUSE__NJ_PAUSE_DPG_ACK_MASK,\n\t\t\t\t\t   UVD_DPG_PAUSE__NJ_PAUSE_DPG_ACK_MASK);\n\n\t\t\t\t/* Stall DPG before WPTR/RPTR reset */\n\t\t\t\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS),\n\t\t\t\t\t   UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK,\n\t\t\t\t\t   ~UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK);\n\t\t\t\t/* Restore */\n\t\t\t\tfw_shared->multi_queue.encode_generalpurpose_queue_mode |= FW_QUEUE_RING_RESET;\n\t\t\t\tring = &adev->vcn.inst->ring_enc[0];\n\t\t\t\tring->wptr = 0;\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_LO, ring->gpu_addr);\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_HI, upper_32_bits(ring->gpu_addr));\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_SIZE, ring->ring_size / 4);\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_RPTR, lower_32_bits(ring->wptr));\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR, lower_32_bits(ring->wptr));\n\t\t\t\tfw_shared->multi_queue.encode_generalpurpose_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\t\t\t\tfw_shared->multi_queue.encode_lowlatency_queue_mode |= FW_QUEUE_RING_RESET;\n\t\t\t\tring = &adev->vcn.inst->ring_enc[1];\n\t\t\t\tring->wptr = 0;\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_LO2, ring->gpu_addr);\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_BASE_HI2, upper_32_bits(ring->gpu_addr));\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_SIZE2, ring->ring_size / 4);\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_RPTR2, lower_32_bits(ring->wptr));\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2, lower_32_bits(ring->wptr));\n\t\t\t\tfw_shared->multi_queue.encode_lowlatency_queue_mode &= ~FW_QUEUE_RING_RESET;\n\n\t\t\t\tfw_shared->multi_queue.decode_queue_mode |= FW_QUEUE_RING_RESET;\n\t\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR,\n\t\t\t\t\t   RREG32_SOC15(UVD, 0, mmUVD_SCRATCH2) & 0x7FFFFFFF);\n\t\t\t\tfw_shared->multi_queue.decode_queue_mode &= ~FW_QUEUE_RING_RESET;\n\t\t\t\t/* Unstall DPG */\n\t\t\t\tWREG32_P(SOC15_REG_OFFSET(UVD, 0, mmUVD_POWER_STATUS),\n\t\t\t\t\t   0, ~UVD_POWER_STATUS__STALL_DPG_POWER_UP_MASK);\n\n\t\t\t\tSOC15_WAIT_ON_RREG(UVD, 0, mmUVD_POWER_STATUS,\n\t\t\t\t\t   UVD_PGFSM_CONFIG__UVDM_UVDU_PWR_ON,\n\t\t\t\t\t   UVD_POWER_STATUS__UVD_POWER_STATUS_MASK);\n\t\t\t}\n\t\t} else {\n\t\t\t/* unpause dpg, no need to wait */\n\t\t\treg_data &= ~UVD_DPG_PAUSE__NJ_PAUSE_DPG_REQ_MASK;\n\t\t\tWREG32_SOC15(UVD, 0, mmUVD_DPG_PAUSE, reg_data);\n\t\t}\n\t\tadev->vcn.inst[inst_idx].pause_state.fw_based = new_state->fw_based;\n\t}\n\n\treturn 0;\n}\n\nstatic bool vcn_v2_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn (RREG32_SOC15(VCN, 0, mmUVD_STATUS) == UVD_STATUS__IDLE);\n}\n\nstatic int vcn_v2_0_wait_for_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint ret;\n\n\tret = SOC15_WAIT_ON_RREG(VCN, 0, mmUVD_STATUS, UVD_STATUS__IDLE,\n\t\tUVD_STATUS__IDLE);\n\n\treturn ret;\n}\n\nstatic int vcn_v2_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tbool enable = (state == AMD_CG_STATE_GATE);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tif (enable) {\n\t\t/* wait for STATUS to clear */\n\t\tif (!vcn_v2_0_is_idle(handle))\n\t\t\treturn -EBUSY;\n\t\tvcn_v2_0_enable_clock_gating(adev);\n\t} else {\n\t\t/* disable HW gating and enable Sw gating */\n\t\tvcn_v2_0_disable_clock_gating(adev);\n\t}\n\treturn 0;\n}\n\n/**\n * vcn_v2_0_dec_ring_get_rptr - get read pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Returns the current hardware read pointer\n */\nstatic uint64_t vcn_v2_0_dec_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\treturn RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_RPTR);\n}\n\n/**\n * vcn_v2_0_dec_ring_get_wptr - get write pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Returns the current hardware write pointer\n */\nstatic uint64_t vcn_v2_0_dec_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->use_doorbell)\n\t\treturn *ring->wptr_cpu_addr;\n\telse\n\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR);\n}\n\n/**\n * vcn_v2_0_dec_ring_set_wptr - set write pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Commits the write pointer to the hardware\n */\nstatic void vcn_v2_0_dec_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG)\n\t\tWREG32_SOC15(UVD, 0, mmUVD_SCRATCH2,\n\t\t\tlower_32_bits(ring->wptr) | 0x80000000);\n\n\tif (ring->use_doorbell) {\n\t\t*ring->wptr_cpu_addr = lower_32_bits(ring->wptr);\n\t\tWDOORBELL32(ring->doorbell_index, lower_32_bits(ring->wptr));\n\t} else {\n\t\tWREG32_SOC15(UVD, 0, mmUVD_RBC_RB_WPTR, lower_32_bits(ring->wptr));\n\t}\n}\n\n/**\n * vcn_v2_0_dec_ring_insert_start - insert a start command\n *\n * @ring: amdgpu_ring pointer\n *\n * Write a start command to the ring.\n */\nvoid vcn_v2_0_dec_ring_insert_start(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_PACKET_START << 1));\n}\n\n/**\n * vcn_v2_0_dec_ring_insert_end - insert a end command\n *\n * @ring: amdgpu_ring pointer\n *\n * Write a end command to the ring.\n */\nvoid vcn_v2_0_dec_ring_insert_end(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_PACKET_END << 1));\n}\n\n/**\n * vcn_v2_0_dec_ring_insert_nop - insert a nop command\n *\n * @ring: amdgpu_ring pointer\n * @count: the number of NOP packets to insert\n *\n * Write a nop command to the ring.\n */\nvoid vcn_v2_0_dec_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tint i;\n\n\tWARN_ON(ring->wptr % 2 || count % 2);\n\n\tfor (i = 0; i < count / 2; i++) {\n\t\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.nop, 0));\n\t\tamdgpu_ring_write(ring, 0);\n\t}\n}\n\n/**\n * vcn_v2_0_dec_ring_emit_fence - emit an fence & trap command\n *\n * @ring: amdgpu_ring pointer\n * @addr: address\n * @seq: sequence number\n * @flags: fence related flags\n *\n * Write a fence and a trap command to the ring.\n */\nvoid vcn_v2_0_dec_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\tunsigned flags)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tWARN_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.context_id, 0));\n\tamdgpu_ring_write(ring, seq);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, addr & 0xffffffff);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data1, 0));\n\tamdgpu_ring_write(ring, upper_32_bits(addr) & 0xff);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_FENCE << 1));\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, 0);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data1, 0));\n\tamdgpu_ring_write(ring, 0);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_TRAP << 1));\n}\n\n/**\n * vcn_v2_0_dec_ring_emit_ib - execute indirect buffer\n *\n * @ring: amdgpu_ring pointer\n * @job: job to retrieve vmid from\n * @ib: indirect buffer to execute\n * @flags: unused\n *\n * Write ring commands to execute the indirect buffer\n */\nvoid vcn_v2_0_dec_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t       struct amdgpu_job *job,\n\t\t\t       struct amdgpu_ib *ib,\n\t\t\t       uint32_t flags)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.ib_vmid, 0));\n\tamdgpu_ring_write(ring, vmid);\n\n\tamdgpu_ring_write(ring,\tPACKET0(adev->vcn.internal.ib_bar_low, 0));\n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring,\tPACKET0(adev->vcn.internal.ib_bar_high, 0));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring,\tPACKET0(adev->vcn.internal.ib_size, 0));\n\tamdgpu_ring_write(ring, ib->length_dw);\n}\n\nvoid vcn_v2_0_dec_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\tuint32_t val, uint32_t mask)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, reg << 2);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data1, 0));\n\tamdgpu_ring_write(ring, val);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.gp_scratch8, 0));\n\tamdgpu_ring_write(ring, mask);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_REG_READ_COND_WAIT << 1));\n}\n\nvoid vcn_v2_0_dec_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\tunsigned vmid, uint64_t pd_addr)\n{\n\tstruct amdgpu_vmhub *hub = &ring->adev->vmhub[ring->vm_hub];\n\tuint32_t data0, data1, mask;\n\n\tpd_addr = amdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\t/* wait for register write */\n\tdata0 = hub->ctx0_ptb_addr_lo32 + vmid * hub->ctx_addr_distance;\n\tdata1 = lower_32_bits(pd_addr);\n\tmask = 0xffffffff;\n\tvcn_v2_0_dec_ring_emit_reg_wait(ring, data0, data1, mask);\n}\n\nvoid vcn_v2_0_dec_ring_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t\tuint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data0, 0));\n\tamdgpu_ring_write(ring, reg << 2);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.data1, 0));\n\tamdgpu_ring_write(ring, val);\n\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_WRITE_REG << 1));\n}\n\n/**\n * vcn_v2_0_enc_ring_get_rptr - get enc read pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Returns the current hardware enc read pointer\n */\nstatic uint64_t vcn_v2_0_enc_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring == &adev->vcn.inst->ring_enc[0])\n\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RB_RPTR);\n\telse\n\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RB_RPTR2);\n}\n\n /**\n * vcn_v2_0_enc_ring_get_wptr - get enc write pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Returns the current hardware enc write pointer\n */\nstatic uint64_t vcn_v2_0_enc_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring == &adev->vcn.inst->ring_enc[0]) {\n\t\tif (ring->use_doorbell)\n\t\t\treturn *ring->wptr_cpu_addr;\n\t\telse\n\t\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RB_WPTR);\n\t} else {\n\t\tif (ring->use_doorbell)\n\t\t\treturn *ring->wptr_cpu_addr;\n\t\telse\n\t\t\treturn RREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2);\n\t}\n}\n\n /**\n * vcn_v2_0_enc_ring_set_wptr - set enc write pointer\n *\n * @ring: amdgpu_ring pointer\n *\n * Commits the enc write pointer to the hardware\n */\nstatic void vcn_v2_0_enc_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring == &adev->vcn.inst->ring_enc[0]) {\n\t\tif (ring->use_doorbell) {\n\t\t\t*ring->wptr_cpu_addr = lower_32_bits(ring->wptr);\n\t\t\tWDOORBELL32(ring->doorbell_index, lower_32_bits(ring->wptr));\n\t\t} else {\n\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR, lower_32_bits(ring->wptr));\n\t\t}\n\t} else {\n\t\tif (ring->use_doorbell) {\n\t\t\t*ring->wptr_cpu_addr = lower_32_bits(ring->wptr);\n\t\t\tWDOORBELL32(ring->doorbell_index, lower_32_bits(ring->wptr));\n\t\t} else {\n\t\t\tWREG32_SOC15(UVD, 0, mmUVD_RB_WPTR2, lower_32_bits(ring->wptr));\n\t\t}\n\t}\n}\n\n/**\n * vcn_v2_0_enc_ring_emit_fence - emit an enc fence & trap command\n *\n * @ring: amdgpu_ring pointer\n * @addr: address\n * @seq: sequence number\n * @flags: fence related flags\n *\n * Write enc a fence and a trap command to the ring.\n */\nvoid vcn_v2_0_enc_ring_emit_fence(struct amdgpu_ring *ring, u64 addr,\n\t\t\t\tu64 seq, unsigned flags)\n{\n\tWARN_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\n\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_FENCE);\n\tamdgpu_ring_write(ring, addr);\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, seq);\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_TRAP);\n}\n\nvoid vcn_v2_0_enc_ring_insert_end(struct amdgpu_ring *ring)\n{\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_END);\n}\n\n/**\n * vcn_v2_0_enc_ring_emit_ib - enc execute indirect buffer\n *\n * @ring: amdgpu_ring pointer\n * @job: job to retrive vmid from\n * @ib: indirect buffer to execute\n * @flags: unused\n *\n * Write enc ring commands to execute the indirect buffer\n */\nvoid vcn_v2_0_enc_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t       struct amdgpu_job *job,\n\t\t\t       struct amdgpu_ib *ib,\n\t\t\t       uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_IB);\n\tamdgpu_ring_write(ring, vmid);\n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n}\n\nvoid vcn_v2_0_enc_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\tuint32_t val, uint32_t mask)\n{\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_REG_WAIT);\n\tamdgpu_ring_write(ring, reg << 2);\n\tamdgpu_ring_write(ring, mask);\n\tamdgpu_ring_write(ring, val);\n}\n\nvoid vcn_v2_0_enc_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\tunsigned int vmid, uint64_t pd_addr)\n{\n\tstruct amdgpu_vmhub *hub = &ring->adev->vmhub[ring->vm_hub];\n\n\tpd_addr = amdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\t/* wait for reg writes */\n\tvcn_v2_0_enc_ring_emit_reg_wait(ring, hub->ctx0_ptb_addr_lo32 +\n\t\t\t\t\tvmid * hub->ctx_addr_distance,\n\t\t\t\t\tlower_32_bits(pd_addr), 0xffffffff);\n}\n\nvoid vcn_v2_0_enc_ring_emit_wreg(struct amdgpu_ring *ring, uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_REG_WRITE);\n\tamdgpu_ring_write(ring,\treg << 2);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic int vcn_v2_0_set_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\treturn 0;\n}\n\nstatic int vcn_v2_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tDRM_DEBUG(\"IH: VCN TRAP\\n\");\n\n\tswitch (entry->src_id) {\n\tcase VCN_2_0__SRCID__UVD_SYSTEM_MESSAGE_INTERRUPT:\n\t\tamdgpu_fence_process(&adev->vcn.inst->ring_dec);\n\t\tbreak;\n\tcase VCN_2_0__SRCID__UVD_ENC_GENERAL_PURPOSE:\n\t\tamdgpu_fence_process(&adev->vcn.inst->ring_enc[0]);\n\t\tbreak;\n\tcase VCN_2_0__SRCID__UVD_ENC_LOW_LATENCY:\n\t\tamdgpu_fence_process(&adev->vcn.inst->ring_enc[1]);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unhandled interrupt: %d %d\\n\",\n\t\t\t  entry->src_id, entry->src_data[0]);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint vcn_v2_0_dec_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t tmp = 0;\n\tunsigned i;\n\tint r;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tWREG32(adev->vcn.inst[ring->me].external.scratch9, 0xCAFEDEAD);\n\tr = amdgpu_ring_alloc(ring, 4);\n\tif (r)\n\t\treturn r;\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.cmd, 0));\n\tamdgpu_ring_write(ring, VCN_DEC_KMD_CMD | (VCN_DEC_CMD_PACKET_START << 1));\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.scratch9, 0));\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = RREG32(adev->vcn.inst[ring->me].external.scratch9);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\n\treturn r;\n}\n\n\nstatic int vcn_v2_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\t/* This doesn't actually powergate the VCN block.\n\t * That's done in the dpm code via the SMC.  This\n\t * just re-inits the block as necessary.  The actual\n\t * gating still happens in the dpm code.  We should\n\t * revisit this when there is a cleaner line between\n\t * the smc and the hw blocks\n\t */\n\tint ret;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tadev->vcn.cur_state = AMD_PG_STATE_UNGATE;\n\t\treturn 0;\n\t}\n\n\tif (state == adev->vcn.cur_state)\n\t\treturn 0;\n\n\tif (state == AMD_PG_STATE_GATE)\n\t\tret = vcn_v2_0_stop(adev);\n\telse\n\t\tret = vcn_v2_0_start(adev);\n\n\tif (!ret)\n\t\tadev->vcn.cur_state = state;\n\treturn ret;\n}\n\nstatic int vcn_v2_0_start_mmsch(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_mm_table *table)\n{\n\tuint32_t data = 0, loop;\n\tuint64_t addr = table->gpu_addr;\n\tstruct mmsch_v2_0_init_header *header;\n\tuint32_t size;\n\tint i;\n\n\theader = (struct mmsch_v2_0_init_header *)table->cpu_addr;\n\tsize = header->header_size + header->vcn_table_size;\n\n\t/* 1, write to vce_mmsch_vf_ctx_addr_lo/hi register with GPU mc addr\n\t * of memory descriptor location\n\t */\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_CTX_ADDR_LO, lower_32_bits(addr));\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_CTX_ADDR_HI, upper_32_bits(addr));\n\n\t/* 2, update vmid of descriptor */\n\tdata = RREG32_SOC15(UVD, 0, mmMMSCH_VF_VMID);\n\tdata &= ~MMSCH_VF_VMID__VF_CTX_VMID_MASK;\n\t/* use domain0 for MM scheduler */\n\tdata |= (0 << MMSCH_VF_VMID__VF_CTX_VMID__SHIFT);\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_VMID, data);\n\n\t/* 3, notify mmsch about the size of this descriptor */\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_CTX_SIZE, size);\n\n\t/* 4, set resp to zero */\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_MAILBOX_RESP, 0);\n\n\tadev->vcn.inst->ring_dec.wptr = 0;\n\tadev->vcn.inst->ring_dec.wptr_old = 0;\n\tvcn_v2_0_dec_ring_set_wptr(&adev->vcn.inst->ring_dec);\n\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i) {\n\t\tadev->vcn.inst->ring_enc[i].wptr = 0;\n\t\tadev->vcn.inst->ring_enc[i].wptr_old = 0;\n\t\tvcn_v2_0_enc_ring_set_wptr(&adev->vcn.inst->ring_enc[i]);\n\t}\n\n\t/* 5, kick off the initialization and wait until\n\t * VCE_MMSCH_VF_MAILBOX_RESP becomes non-zero\n\t */\n\tWREG32_SOC15(UVD, 0, mmMMSCH_VF_MAILBOX_HOST, 0x10000001);\n\n\tdata = RREG32_SOC15(UVD, 0, mmMMSCH_VF_MAILBOX_RESP);\n\tloop = 1000;\n\twhile ((data & 0x10000002) != 0x10000002) {\n\t\tudelay(10);\n\t\tdata = RREG32_SOC15(UVD, 0, mmMMSCH_VF_MAILBOX_RESP);\n\t\tloop--;\n\t\tif (!loop)\n\t\t\tbreak;\n\t}\n\n\tif (!loop) {\n\t\tDRM_ERROR(\"failed to init MMSCH, \" \\\n\t\t\t\"mmMMSCH_VF_MAILBOX_RESP = 0x%08x\\n\", data);\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic int vcn_v2_0_start_sriov(struct amdgpu_device *adev)\n{\n\tint r;\n\tuint32_t tmp;\n\tstruct amdgpu_ring *ring;\n\tuint32_t offset, size;\n\tuint32_t table_size = 0;\n\tstruct mmsch_v2_0_cmd_direct_write direct_wt = { {0} };\n\tstruct mmsch_v2_0_cmd_direct_read_modify_write direct_rd_mod_wt = { {0} };\n\tstruct mmsch_v2_0_cmd_end end = { {0} };\n\tstruct mmsch_v2_0_init_header *header;\n\tuint32_t *init_table = adev->virt.mm_table.cpu_addr;\n\tuint8_t i = 0;\n\n\theader = (struct mmsch_v2_0_init_header *)init_table;\n\tdirect_wt.cmd_header.command_type = MMSCH_COMMAND__DIRECT_REG_WRITE;\n\tdirect_rd_mod_wt.cmd_header.command_type =\n\t\tMMSCH_COMMAND__DIRECT_REG_READ_MODIFY_WRITE;\n\tend.cmd_header.command_type = MMSCH_COMMAND__END;\n\n\tif (header->vcn_table_offset == 0 && header->vcn_table_size == 0) {\n\t\theader->version = MMSCH_VERSION;\n\t\theader->header_size = sizeof(struct mmsch_v2_0_init_header) >> 2;\n\n\t\theader->vcn_table_offset = header->header_size;\n\n\t\tinit_table += header->vcn_table_offset;\n\n\t\tsize = AMDGPU_GPU_PAGE_ALIGN(adev->vcn.fw[0]->size + 4);\n\n\t\tMMSCH_V2_0_INSERT_DIRECT_RD_MOD_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_STATUS),\n\t\t\t0xFFFFFFFF, 0x00000004);\n\n\t\t/* mc resume*/\n\t\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\t\tmmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW),\n\t\t\t\tadev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_lo);\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\t\tmmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH),\n\t\t\t\tadev->firmware.ucode[AMDGPU_UCODE_ID_VCN].tmr_mc_addr_hi);\n\t\t\toffset = 0;\n\t\t} else {\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\t\tmmUVD_LMI_VCPU_CACHE_64BIT_BAR_LOW),\n\t\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr));\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\t\tmmUVD_LMI_VCPU_CACHE_64BIT_BAR_HIGH),\n\t\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr));\n\t\t\toffset = size;\n\t\t}\n\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_OFFSET0),\n\t\t\t0);\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_SIZE0),\n\t\t\tsize);\n\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_VCPU_CACHE1_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_VCPU_CACHE1_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_OFFSET1),\n\t\t\t0);\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_SIZE1),\n\t\t\tAMDGPU_VCN_STACK_SIZE);\n\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_VCPU_CACHE2_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(adev->vcn.inst->gpu_addr + offset +\n\t\t\t\tAMDGPU_VCN_STACK_SIZE));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_VCPU_CACHE2_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(adev->vcn.inst->gpu_addr + offset +\n\t\t\t\tAMDGPU_VCN_STACK_SIZE));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_OFFSET2),\n\t\t\t0);\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_VCPU_CACHE_SIZE2),\n\t\t\tAMDGPU_VCN_CONTEXT_SIZE);\n\n\t\tfor (r = 0; r < adev->vcn.num_enc_rings; ++r) {\n\t\t\tring = &adev->vcn.inst->ring_enc[r];\n\t\t\tring->wptr = 0;\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_RB_BASE_LO),\n\t\t\t\tlower_32_bits(ring->gpu_addr));\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_RB_BASE_HI),\n\t\t\t\tupper_32_bits(ring->gpu_addr));\n\t\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_RB_SIZE),\n\t\t\t\tring->ring_size / 4);\n\t\t}\n\n\t\tring = &adev->vcn.inst->ring_dec;\n\t\tring->wptr = 0;\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_RBC_RB_64BIT_BAR_LOW),\n\t\t\tlower_32_bits(ring->gpu_addr));\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i,\n\t\t\t\tmmUVD_LMI_RBC_RB_64BIT_BAR_HIGH),\n\t\t\tupper_32_bits(ring->gpu_addr));\n\t\t/* force RBC into idle state */\n\t\ttmp = order_base_2(ring->ring_size);\n\t\ttmp = REG_SET_FIELD(0, UVD_RBC_RB_CNTL, RB_BUFSZ, tmp);\n\t\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_BLKSZ, 1);\n\t\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_FETCH, 1);\n\t\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_NO_UPDATE, 1);\n\t\ttmp = REG_SET_FIELD(tmp, UVD_RBC_RB_CNTL, RB_RPTR_WR_EN, 1);\n\t\tMMSCH_V2_0_INSERT_DIRECT_WT(\n\t\t\tSOC15_REG_OFFSET(UVD, i, mmUVD_RBC_RB_CNTL), tmp);\n\n\t\t/* add end packet */\n\t\ttmp = sizeof(struct mmsch_v2_0_cmd_end);\n\t\tmemcpy((void *)init_table, &end, tmp);\n\t\ttable_size += (tmp / 4);\n\t\theader->vcn_table_size = table_size;\n\n\t}\n\treturn vcn_v2_0_start_mmsch(adev, &adev->virt.mm_table);\n}\n\nstatic void vcn_v2_0_print_ip_state(void *handle, struct drm_printer *p)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i, j;\n\tuint32_t reg_count = ARRAY_SIZE(vcn_reg_list_2_0);\n\tuint32_t inst_off, is_powered;\n\n\tif (!adev->vcn.ip_dump)\n\t\treturn;\n\n\tdrm_printf(p, \"num_instances:%d\\n\", adev->vcn.num_vcn_inst);\n\tfor (i = 0; i < adev->vcn.num_vcn_inst; i++) {\n\t\tif (adev->vcn.harvest_config & (1 << i)) {\n\t\t\tdrm_printf(p, \"\\nHarvested Instance:VCN%d Skipping dump\\n\", i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tinst_off = i * reg_count;\n\t\tis_powered = (adev->vcn.ip_dump[inst_off] &\n\t\t\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK) != 1;\n\n\t\tif (is_powered) {\n\t\t\tdrm_printf(p, \"\\nActive Instance:VCN%d\\n\", i);\n\t\t\tfor (j = 0; j < reg_count; j++)\n\t\t\t\tdrm_printf(p, \"%-50s \\t 0x%08x\\n\", vcn_reg_list_2_0[j].reg_name,\n\t\t\t\t\t   adev->vcn.ip_dump[inst_off + j]);\n\t\t} else {\n\t\t\tdrm_printf(p, \"\\nInactive Instance:VCN%d\\n\", i);\n\t\t}\n\t}\n}\n\nstatic void vcn_v2_0_dump_ip_state(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i, j;\n\tbool is_powered;\n\tuint32_t inst_off;\n\tuint32_t reg_count = ARRAY_SIZE(vcn_reg_list_2_0);\n\n\tif (!adev->vcn.ip_dump)\n\t\treturn;\n\n\tfor (i = 0; i < adev->vcn.num_vcn_inst; i++) {\n\t\tif (adev->vcn.harvest_config & (1 << i))\n\t\t\tcontinue;\n\n\t\tinst_off = i * reg_count;\n\t\t/* mmUVD_POWER_STATUS is always readable and is first element of the array */\n\t\tadev->vcn.ip_dump[inst_off] = RREG32_SOC15(VCN, i, mmUVD_POWER_STATUS);\n\t\tis_powered = (adev->vcn.ip_dump[inst_off] &\n\t\t\t\tUVD_POWER_STATUS__UVD_POWER_STATUS_MASK) != 1;\n\n\t\tif (is_powered)\n\t\t\tfor (j = 1; j < reg_count; j++)\n\t\t\t\tadev->vcn.ip_dump[inst_off + j] =\n\t\t\t\t\tRREG32(SOC15_REG_ENTRY_OFFSET_INST(vcn_reg_list_2_0[j], i));\n\t}\n}\n\nstatic const struct amd_ip_funcs vcn_v2_0_ip_funcs = {\n\t.name = \"vcn_v2_0\",\n\t.early_init = vcn_v2_0_early_init,\n\t.late_init = NULL,\n\t.sw_init = vcn_v2_0_sw_init,\n\t.sw_fini = vcn_v2_0_sw_fini,\n\t.hw_init = vcn_v2_0_hw_init,\n\t.hw_fini = vcn_v2_0_hw_fini,\n\t.suspend = vcn_v2_0_suspend,\n\t.resume = vcn_v2_0_resume,\n\t.is_idle = vcn_v2_0_is_idle,\n\t.wait_for_idle = vcn_v2_0_wait_for_idle,\n\t.check_soft_reset = NULL,\n\t.pre_soft_reset = NULL,\n\t.soft_reset = NULL,\n\t.post_soft_reset = NULL,\n\t.set_clockgating_state = vcn_v2_0_set_clockgating_state,\n\t.set_powergating_state = vcn_v2_0_set_powergating_state,\n\t.dump_ip_state = vcn_v2_0_dump_ip_state,\n\t.print_ip_state = vcn_v2_0_print_ip_state,\n};\n\nstatic const struct amdgpu_ring_funcs vcn_v2_0_dec_ring_vm_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCN_DEC,\n\t.align_mask = 0xf,\n\t.secure_submission_supported = true,\n\t.get_rptr = vcn_v2_0_dec_ring_get_rptr,\n\t.get_wptr = vcn_v2_0_dec_ring_get_wptr,\n\t.set_wptr = vcn_v2_0_dec_ring_set_wptr,\n\t.emit_frame_size =\n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 6 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 8 +\n\t\t8 + /* vcn_v2_0_dec_ring_emit_vm_flush */\n\t\t14 + 14 + /* vcn_v2_0_dec_ring_emit_fence x2 vm fence */\n\t\t6,\n\t.emit_ib_size = 8, /* vcn_v2_0_dec_ring_emit_ib */\n\t.emit_ib = vcn_v2_0_dec_ring_emit_ib,\n\t.emit_fence = vcn_v2_0_dec_ring_emit_fence,\n\t.emit_vm_flush = vcn_v2_0_dec_ring_emit_vm_flush,\n\t.test_ring = vcn_v2_0_dec_ring_test_ring,\n\t.test_ib = amdgpu_vcn_dec_ring_test_ib,\n\t.insert_nop = vcn_v2_0_dec_ring_insert_nop,\n\t.insert_start = vcn_v2_0_dec_ring_insert_start,\n\t.insert_end = vcn_v2_0_dec_ring_insert_end,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_vcn_ring_begin_use,\n\t.end_use = amdgpu_vcn_ring_end_use,\n\t.emit_wreg = vcn_v2_0_dec_ring_emit_wreg,\n\t.emit_reg_wait = vcn_v2_0_dec_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic const struct amdgpu_ring_funcs vcn_v2_0_enc_ring_vm_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCN_ENC,\n\t.align_mask = 0x3f,\n\t.nop = VCN_ENC_CMD_NO_OP,\n\t.get_rptr = vcn_v2_0_enc_ring_get_rptr,\n\t.get_wptr = vcn_v2_0_enc_ring_get_wptr,\n\t.set_wptr = vcn_v2_0_enc_ring_set_wptr,\n\t.emit_frame_size =\n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 3 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 4 +\n\t\t4 + /* vcn_v2_0_enc_ring_emit_vm_flush */\n\t\t5 + 5 + /* vcn_v2_0_enc_ring_emit_fence x2 vm fence */\n\t\t1, /* vcn_v2_0_enc_ring_insert_end */\n\t.emit_ib_size = 5, /* vcn_v2_0_enc_ring_emit_ib */\n\t.emit_ib = vcn_v2_0_enc_ring_emit_ib,\n\t.emit_fence = vcn_v2_0_enc_ring_emit_fence,\n\t.emit_vm_flush = vcn_v2_0_enc_ring_emit_vm_flush,\n\t.test_ring = amdgpu_vcn_enc_ring_test_ring,\n\t.test_ib = amdgpu_vcn_enc_ring_test_ib,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.insert_end = vcn_v2_0_enc_ring_insert_end,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_vcn_ring_begin_use,\n\t.end_use = amdgpu_vcn_ring_end_use,\n\t.emit_wreg = vcn_v2_0_enc_ring_emit_wreg,\n\t.emit_reg_wait = vcn_v2_0_enc_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic void vcn_v2_0_set_dec_ring_funcs(struct amdgpu_device *adev)\n{\n\tadev->vcn.inst->ring_dec.funcs = &vcn_v2_0_dec_ring_vm_funcs;\n}\n\nstatic void vcn_v2_0_set_enc_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i)\n\t\tadev->vcn.inst->ring_enc[i].funcs = &vcn_v2_0_enc_ring_vm_funcs;\n}\n\nstatic const struct amdgpu_irq_src_funcs vcn_v2_0_irq_funcs = {\n\t.set = vcn_v2_0_set_interrupt_state,\n\t.process = vcn_v2_0_process_interrupt,\n};\n\nstatic void vcn_v2_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->vcn.inst->irq.num_types = adev->vcn.num_enc_rings + 1;\n\tadev->vcn.inst->irq.funcs = &vcn_v2_0_irq_funcs;\n}\n\nconst struct amdgpu_ip_block_version vcn_v2_0_ip_block =\n{\n\t\t.type = AMD_IP_BLOCK_TYPE_VCN,\n\t\t.major = 2,\n\t\t.minor = 0,\n\t\t.rev = 0,\n\t\t.funcs = &vcn_v2_0_ip_funcs,\n};\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/sprintf4params-bug.txt", "bug_report_text": "CID: 487621\nType: Buffer overflow\nSeverity: High\nChecker: BUFFER_SIZE\nCategory: RISKY_PROGRAMMING_PRACTICE\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nFunction: amdgpu_acpi_enumerate_xcc\nLine: 1092\n\nProblem:\nUnbounded string operation using sprintf can cause buffer overflow\n\nAbstract:\nThe code uses sprintf to write to buffer 'hid' without size checking. The sprintf \noperation could write beyond the buffer boundaries if the formatted output string \nexceeds the destination buffer size. This could corrupt adjacent memory and lead \nto system instability or security vulnerabilities.\n\nDetails:\nThe sprintf function is used to format a string containing \"AMD\" concatenated with \na number. Since sprintf performs no bounds checking, if AMD_XCC_HID_START + id \nproduces a large number, the resulting string could overflow the fixed-size 'hid' \nbuffer. This occurs in the GPU driver's ACPI enumeration code which runs with \nkernel privileges.\n\nFix:\nReplace sprintf with snprintf and specify the buffer size:\n  snprintf(hid, sizeof(hid), \"%s%d\", \"AMD\", AMD_XCC_HID_START + id)\n", "diff_path": "dataset/raw_data/bugs/dev-set/sprintf4params-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\nindex f85ace0384d2..10dfa3a37333 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c\n@@ -1089,7 +1089,7 @@ static int amdgpu_acpi_enumerate_xcc(void)\n        xa_init(&numa_info_xa);\n\n        for (id = 0; id < AMD_XCC_MAX_HID; id++) {\n-               sprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n+               snprintf(hid, sizeof(hid), \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n                acpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n                /* These ACPI objects are expected to be in sequential order. If\n                 * one is not found, no need to check the rest.", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c", "line_number": 1092, "code": "// SPDX-License-Identifier: MIT\n/*\n * Copyright 2012 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n */\n\n#include <linux/pci.h>\n#include <linux/acpi.h>\n#include <linux/backlight.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n#include <linux/power_supply.h>\n#include <linux/pm_runtime.h>\n#include <linux/suspend.h>\n#include <acpi/video.h>\n#include <acpi/actbl.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_display.h\"\n#include \"amd_acpi.h\"\n#include \"atom.h\"\n\n/* Declare GUID for AMD _DSM method for XCCs */\nstatic const guid_t amd_xcc_dsm_guid = GUID_INIT(0x8267f5d5, 0xa556, 0x44f2,\n\t\t\t\t\t\t 0xb8, 0xb4, 0x45, 0x56, 0x2e,\n\t\t\t\t\t\t 0x8c, 0x5b, 0xec);\n\n#define AMD_XCC_HID_START 3000\n#define AMD_XCC_DSM_GET_NUM_FUNCS 0\n#define AMD_XCC_DSM_GET_SUPP_MODE 1\n#define AMD_XCC_DSM_GET_XCP_MODE 2\n#define AMD_XCC_DSM_GET_VF_XCC_MAPPING 4\n#define AMD_XCC_DSM_GET_TMR_INFO 5\n#define AMD_XCC_DSM_NUM_FUNCS 5\n\n#define AMD_XCC_MAX_HID 24\n\nstruct xarray numa_info_xa;\n\n/* Encapsulates the XCD acpi object information */\nstruct amdgpu_acpi_xcc_info {\n\tstruct list_head list;\n\tstruct amdgpu_numa_info *numa_info;\n\tuint8_t xcp_node;\n\tuint8_t phy_id;\n\tacpi_handle handle;\n};\n\nstruct amdgpu_acpi_dev_info {\n\tstruct list_head list;\n\tstruct list_head xcc_list;\n\tuint32_t sbdf;\n\tuint16_t supp_xcp_mode;\n\tuint16_t xcp_mode;\n\tuint16_t mem_mode;\n\tuint64_t tmr_base;\n\tuint64_t tmr_size;\n};\n\nstruct list_head amdgpu_acpi_dev_list;\n\nstruct amdgpu_atif_notification_cfg {\n\tbool enabled;\n\tint command_code;\n};\n\nstruct amdgpu_atif_notifications {\n\tbool thermal_state;\n\tbool forced_power_state;\n\tbool system_power_state;\n\tbool brightness_change;\n\tbool dgpu_display_event;\n\tbool gpu_package_power_limit;\n};\n\nstruct amdgpu_atif_functions {\n\tbool system_params;\n\tbool sbios_requests;\n\tbool temperature_change;\n\tbool query_backlight_transfer_characteristics;\n\tbool ready_to_undock;\n\tbool external_gpu_information;\n};\n\nstruct amdgpu_atif {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atif_notifications notifications;\n\tstruct amdgpu_atif_functions functions;\n\tstruct amdgpu_atif_notification_cfg notification_cfg;\n\tstruct backlight_device *bd;\n\tstruct amdgpu_dm_backlight_caps backlight_caps;\n};\n\nstruct amdgpu_atcs_functions {\n\tbool get_ext_state;\n\tbool pcie_perf_req;\n\tbool pcie_dev_rdy;\n\tbool pcie_bus_width;\n\tbool power_shift_control;\n};\n\nstruct amdgpu_atcs {\n\tacpi_handle handle;\n\n\tstruct amdgpu_atcs_functions functions;\n};\n\nstatic struct amdgpu_acpi_priv {\n\tstruct amdgpu_atif atif;\n\tstruct amdgpu_atcs atcs;\n} amdgpu_acpi_priv;\n\n/* Call the ATIF method\n */\n/**\n * amdgpu_atif_call - call an ATIF method\n *\n * @atif: atif structure\n * @function: the ATIF function to execute\n * @params: ATIF function params\n *\n * Executes the requested ATIF function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atif_call(struct amdgpu_atif *atif,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atif_arg_elements[2];\n\tstruct acpi_object_list atif_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatif_arg.count = 2;\n\tatif_arg.pointer = &atif_arg_elements[0];\n\n\tatif_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatif_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatif_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatif_arg_elements[1].buffer.length = params->length;\n\t\tatif_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatif_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatif_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atif->handle, NULL, &atif_arg,\n\t\t\t\t      &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATIF got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atif_parse_notification - parse supported notifications\n *\n * @n: supported notifications struct\n * @mask: supported notifications mask from ATIF\n *\n * Use the supported notifications mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what notifications\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n, u32 mask)\n{\n\tn->thermal_state = mask & ATIF_THERMAL_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->forced_power_state = mask & ATIF_FORCED_POWER_STATE_CHANGE_REQUEST_SUPPORTED;\n\tn->system_power_state = mask & ATIF_SYSTEM_POWER_SOURCE_CHANGE_REQUEST_SUPPORTED;\n\tn->brightness_change = mask & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST_SUPPORTED;\n\tn->dgpu_display_event = mask & ATIF_DGPU_DISPLAY_EVENT_SUPPORTED;\n\tn->gpu_package_power_limit = mask & ATIF_GPU_PACKAGE_POWER_LIMIT_REQUEST_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATIF\n *\n * Use the supported functions mask from ATIF function\n * ATIF_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f, u32 mask)\n{\n\tf->system_params = mask & ATIF_GET_SYSTEM_PARAMETERS_SUPPORTED;\n\tf->sbios_requests = mask & ATIF_GET_SYSTEM_BIOS_REQUESTS_SUPPORTED;\n\tf->temperature_change = mask & ATIF_TEMPERATURE_CHANGE_NOTIFICATION_SUPPORTED;\n\tf->query_backlight_transfer_characteristics =\n\t\tmask & ATIF_QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS_SUPPORTED;\n\tf->ready_to_undock = mask & ATIF_READY_TO_UNDOCK_NOTIFICATION_SUPPORTED;\n\tf->external_gpu_information = mask & ATIF_GET_EXTERNAL_GPU_INFORMATION_SUPPORTED;\n}\n\n/**\n * amdgpu_atif_verify_interface - verify ATIF\n *\n * @atif: amdgpu atif struct\n *\n * Execute the ATIF_FUNCTION_VERIFY_INTERFACE ATIF function\n * to initialize ATIF and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_verify_interface(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 12) {\n\t\tDRM_INFO(\"ATIF buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATIF version %u\\n\", output.version);\n\n\tamdgpu_atif_parse_notification(&atif->notifications, output.notification_mask);\n\tamdgpu_atif_parse_functions(&atif->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_notification_params - determine notify configuration\n *\n * @atif: acpi handle\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_PARAMETERS ATIF function\n * to determine if a notifier is used and if so which one\n * (all asics).  This is either Notify(VGA, 0x81) or Notify(VGA, n)\n * where n is specified in the result if a notifier is used.\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_notification_params(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atif_notification_cfg *n = &atif->notification_cfg;\n\tstruct atif_system_params params;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_PARAMETERS,\n\t\t\t\tNULL);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\tsize = min(sizeof(params), size);\n\tmemcpy(&params, info->buffer.pointer, size);\n\n\tDRM_DEBUG_DRIVER(\"SYSTEM_PARAMS: mask = %#x, flags = %#x\\n\",\n\t\t\tparams.flags, params.valid_mask);\n\tparams.flags = params.flags & params.valid_mask;\n\n\tif ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_NONE) {\n\t\tn->enabled = false;\n\t\tn->command_code = 0;\n\t} else if ((params.flags & ATIF_NOTIFY_MASK) == ATIF_NOTIFY_81) {\n\t\tn->enabled = true;\n\t\tn->command_code = 0x81;\n\t} else {\n\t\tif (size < 11) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tn->enabled = true;\n\t\tn->command_code = params.command_code;\n\t}\n\nout:\n\tDRM_DEBUG_DRIVER(\"Notification %s, command code = %#x\\n\",\n\t\t\t(n->enabled ? \"enabled\" : \"disabled\"),\n\t\t\tn->command_code);\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_query_backlight_caps - get min and max backlight input signal\n *\n * @atif: acpi handle\n *\n * Execute the QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS ATIF function\n * to determine the acceptable range of backlight values\n *\n * Backlight_caps.caps_valid will be set to true if the query is successful\n *\n * The input signals are in range 0-255\n *\n * This function assumes the display with backlight is the first LCD\n *\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_query_backlight_caps(struct amdgpu_atif *atif)\n{\n\tunion acpi_object *info;\n\tstruct atif_qbtc_output characteristics;\n\tstruct atif_qbtc_arguments arguments;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tint err = 0;\n\n\targuments.size = sizeof(arguments);\n\targuments.requested_display = ATIF_QBTC_REQUEST_LCD1;\n\n\tparams.length = sizeof(arguments);\n\tparams.pointer = (void *)&arguments;\n\n\tinfo = amdgpu_atif_call(atif,\n\t\tATIF_FUNCTION_QUERY_BRIGHTNESS_TRANSFER_CHARACTERISTICS,\n\t\t&params);\n\tif (!info) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 10) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemset(&characteristics, 0, sizeof(characteristics));\n\tsize = min(sizeof(characteristics), size);\n\tmemcpy(&characteristics, info->buffer.pointer, size);\n\n\tatif->backlight_caps.caps_valid = true;\n\tatif->backlight_caps.min_input_signal =\n\t\t\tcharacteristics.min_input_signal;\n\tatif->backlight_caps.max_input_signal =\n\t\t\tcharacteristics.max_input_signal;\n\tatif->backlight_caps.ac_level = characteristics.ac_level;\n\tatif->backlight_caps.dc_level = characteristics.dc_level;\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_atif_get_sbios_requests - get requested sbios event\n *\n * @atif: acpi handle\n * @req: atif sbios request struct\n *\n * Execute the ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS ATIF function\n * to determine what requests the sbios is making to the driver\n * (all asics).\n * Returns 0 on success, error on failure.\n */\nstatic int amdgpu_atif_get_sbios_requests(struct amdgpu_atif *atif,\n\t\t\t\t\t  struct atif_sbios_requests *req)\n{\n\tunion acpi_object *info;\n\tsize_t size;\n\tint count = 0;\n\n\tinfo = amdgpu_atif_call(atif, ATIF_FUNCTION_GET_SYSTEM_BIOS_REQUESTS,\n\t\t\t\tNULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tsize = *(u16 *)info->buffer.pointer;\n\tif (size < 0xd) {\n\t\tcount = -EINVAL;\n\t\tgoto out;\n\t}\n\tmemset(req, 0, sizeof(*req));\n\n\tsize = min(sizeof(*req), size);\n\tmemcpy(req, info->buffer.pointer, size);\n\tDRM_DEBUG_DRIVER(\"SBIOS pending requests: %#x\\n\", req->pending);\n\n\tcount = hweight32(req->pending);\n\nout:\n\tkfree(info);\n\treturn count;\n}\n\n/**\n * amdgpu_atif_handler - handle ATIF notify requests\n *\n * @adev: amdgpu_device pointer\n * @event: atif sbios request struct\n *\n * Checks the acpi event and if it matches an atif event,\n * handles it.\n *\n * Returns:\n * NOTIFY_BAD or NOTIFY_DONE, depending on the event.\n */\nstatic int amdgpu_atif_handler(struct amdgpu_device *adev,\n\t\t\t       struct acpi_bus_event *event)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tint count;\n\n\tDRM_DEBUG_DRIVER(\"event, device_class = %s, type = %#x\\n\",\n\t\t\tevent->device_class, event->type);\n\n\tif (strcmp(event->device_class, ACPI_VIDEO_CLASS) != 0)\n\t\treturn NOTIFY_DONE;\n\n\t/* Is this actually our event? */\n\tif (!atif->notification_cfg.enabled ||\n\t    event->type != atif->notification_cfg.command_code) {\n\t\t/* These events will generate keypresses otherwise */\n\t\tif (event->type == ACPI_VIDEO_NOTIFY_PROBE)\n\t\t\treturn NOTIFY_BAD;\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\t}\n\n\tif (atif->functions.sbios_requests) {\n\t\tstruct atif_sbios_requests req;\n\n\t\t/* Check pending SBIOS requests */\n\t\tcount = amdgpu_atif_get_sbios_requests(atif, &req);\n\n\t\tif (count <= 0)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tDRM_DEBUG_DRIVER(\"ATIF: %d pending SBIOS requests\\n\", count);\n\n\t\tif (req.pending & ATIF_PANEL_BRIGHTNESS_CHANGE_REQUEST) {\n\t\t\tif (atif->bd) {\n\t\t\t\tDRM_DEBUG_DRIVER(\"Changing brightness to %d\\n\",\n\t\t\t\t\t\t req.backlight_level);\n\t\t\t\t/*\n\t\t\t\t * XXX backlight_device_set_brightness() is\n\t\t\t\t * hardwired to post BACKLIGHT_UPDATE_SYSFS.\n\t\t\t\t * It probably should accept 'reason' parameter.\n\t\t\t\t */\n\t\t\t\tbacklight_device_set_brightness(atif->bd, req.backlight_level);\n\t\t\t}\n\t\t}\n\n\t\tif (req.pending & ATIF_DGPU_DISPLAY_EVENT) {\n\t\t\tif (adev->flags & AMD_IS_PX) {\n\t\t\t\tpm_runtime_get_sync(adev_to_drm(adev)->dev);\n\t\t\t\t/* Just fire off a uevent and let userspace tell us what to do */\n\t\t\t\tdrm_helper_hpd_irq_event(adev_to_drm(adev));\n\t\t\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\t\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t\t\t}\n\t\t}\n\t\t/* TODO: check other events */\n\t}\n\n\t/* We've handled the event, stop the notifier chain. The ACPI interface\n\t * overloads ACPI_VIDEO_NOTIFY_PROBE, we don't want to send that to\n\t * userspace if the event was generated only to signal a SBIOS\n\t * request.\n\t */\n\treturn NOTIFY_BAD;\n}\n\n/* Call the ATCS method\n */\n/**\n * amdgpu_atcs_call - call an ATCS method\n *\n * @atcs: atcs structure\n * @function: the ATCS function to execute\n * @params: ATCS function params\n *\n * Executes the requested ATCS function (all asics).\n * Returns a pointer to the acpi output buffer.\n */\nstatic union acpi_object *amdgpu_atcs_call(struct amdgpu_atcs *atcs,\n\t\t\t\t\t   int function,\n\t\t\t\t\t   struct acpi_buffer *params)\n{\n\tacpi_status status;\n\tunion acpi_object atcs_arg_elements[2];\n\tstruct acpi_object_list atcs_arg;\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\n\tatcs_arg.count = 2;\n\tatcs_arg.pointer = &atcs_arg_elements[0];\n\n\tatcs_arg_elements[0].type = ACPI_TYPE_INTEGER;\n\tatcs_arg_elements[0].integer.value = function;\n\n\tif (params) {\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_BUFFER;\n\t\tatcs_arg_elements[1].buffer.length = params->length;\n\t\tatcs_arg_elements[1].buffer.pointer = params->pointer;\n\t} else {\n\t\t/* We need a second fake parameter */\n\t\tatcs_arg_elements[1].type = ACPI_TYPE_INTEGER;\n\t\tatcs_arg_elements[1].integer.value = 0;\n\t}\n\n\tstatus = acpi_evaluate_object(atcs->handle, NULL, &atcs_arg, &buffer);\n\n\t/* Fail only if calling the method fails and ATIF is supported */\n\tif (ACPI_FAILURE(status) && status != AE_NOT_FOUND) {\n\t\tDRM_DEBUG_DRIVER(\"failed to evaluate ATCS got %s\\n\",\n\t\t\t\t acpi_format_exception(status));\n\t\tkfree(buffer.pointer);\n\t\treturn NULL;\n\t}\n\n\treturn buffer.pointer;\n}\n\n/**\n * amdgpu_atcs_parse_functions - parse supported functions\n *\n * @f: supported functions struct\n * @mask: supported functions mask from ATCS\n *\n * Use the supported functions mask from ATCS function\n * ATCS_FUNCTION_VERIFY_INTERFACE to determine what functions\n * are supported (all asics).\n */\nstatic void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f, u32 mask)\n{\n\tf->get_ext_state = mask & ATCS_GET_EXTERNAL_STATE_SUPPORTED;\n\tf->pcie_perf_req = mask & ATCS_PCIE_PERFORMANCE_REQUEST_SUPPORTED;\n\tf->pcie_dev_rdy = mask & ATCS_PCIE_DEVICE_READY_NOTIFICATION_SUPPORTED;\n\tf->pcie_bus_width = mask & ATCS_SET_PCIE_BUS_WIDTH_SUPPORTED;\n\tf->power_shift_control = mask & ATCS_SET_POWER_SHIFT_CONTROL_SUPPORTED;\n}\n\n/**\n * amdgpu_atcs_verify_interface - verify ATCS\n *\n * @atcs: amdgpu atcs struct\n *\n * Execute the ATCS_FUNCTION_VERIFY_INTERFACE ATCS function\n * to initialize ATCS and determine what features are supported\n * (all asics).\n * returns 0 on success, error on failure.\n */\nstatic int amdgpu_atcs_verify_interface(struct amdgpu_atcs *atcs)\n{\n\tunion acpi_object *info;\n\tstruct atcs_verify_interface output;\n\tsize_t size;\n\tint err = 0;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_VERIFY_INTERFACE, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tmemset(&output, 0, sizeof(output));\n\n\tsize = *(u16 *) info->buffer.pointer;\n\tif (size < 8) {\n\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tsize = min(sizeof(output), size);\n\n\tmemcpy(&output, info->buffer.pointer, size);\n\n\t/* TODO: check version? */\n\tDRM_DEBUG_DRIVER(\"ATCS version %u\\n\", output.version);\n\n\tamdgpu_atcs_parse_functions(&atcs->functions, output.function_bits);\n\nout:\n\tkfree(info);\n\treturn err;\n}\n\n/**\n * amdgpu_acpi_is_pcie_performance_request_supported\n *\n * @adev: amdgpu_device pointer\n *\n * Check if the ATCS pcie_perf_req and pcie_dev_rdy methods\n * are supported (all asics).\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (atcs->functions.pcie_perf_req && atcs->functions.pcie_dev_rdy)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * amdgpu_acpi_is_power_shift_control_supported\n *\n * Check if the ATCS power shift control method\n * is supported.\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_power_shift_control_supported(void)\n{\n\treturn amdgpu_acpi_priv.atcs.functions.power_shift_control;\n}\n\n/**\n * amdgpu_acpi_pcie_notify_device_ready\n *\n * @adev: amdgpu_device pointer\n *\n * Executes the PCIE_DEVICE_READY_NOTIFICATION method\n * (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\n\tif (!atcs->functions.pcie_dev_rdy)\n\t\treturn -EINVAL;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_DEVICE_READY_NOTIFICATION, NULL);\n\tif (!info)\n\t\treturn -EIO;\n\n\tkfree(info);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_pcie_performance_request\n *\n * @adev: amdgpu_device pointer\n * @perf_req: requested perf level (pcie gen speed)\n * @advertise: set advertise caps flag if set\n *\n * Executes the PCIE_PERFORMANCE_REQUEST method to\n * change the pcie gen speed (all asics).\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t u8 perf_req, bool advertise)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pref_req_input atcs_input;\n\tstruct atcs_pref_req_output atcs_output;\n\tstruct acpi_buffer params;\n\tsize_t size;\n\tu32 retry = 3;\n\n\tif (amdgpu_acpi_pcie_notify_device_ready(adev))\n\t\treturn -EINVAL;\n\n\tif (!atcs->functions.pcie_perf_req)\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pref_req_input);\n\t/* client id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.client_id = pci_dev_id(adev->pdev);\n\tatcs_input.valid_flags_mask = ATCS_VALID_FLAGS_MASK;\n\tatcs_input.flags = ATCS_WAIT_FOR_COMPLETION;\n\tif (advertise)\n\t\tatcs_input.flags |= ATCS_ADVERTISE_CAPS;\n\tatcs_input.req_type = ATCS_PCIE_LINK_SPEED;\n\tatcs_input.perf_req = perf_req;\n\n\tparams.length = sizeof(struct atcs_pref_req_input);\n\tparams.pointer = &atcs_input;\n\n\twhile (retry--) {\n\t\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_PCIE_PERFORMANCE_REQUEST, &params);\n\t\tif (!info)\n\t\t\treturn -EIO;\n\n\t\tmemset(&atcs_output, 0, sizeof(atcs_output));\n\n\t\tsize = *(u16 *) info->buffer.pointer;\n\t\tif (size < 3) {\n\t\t\tDRM_INFO(\"ATCS buffer is too small: %zu\\n\", size);\n\t\t\tkfree(info);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsize = min(sizeof(atcs_output), size);\n\n\t\tmemcpy(&atcs_output, info->buffer.pointer, size);\n\n\t\tkfree(info);\n\n\t\tswitch (atcs_output.ret_val) {\n\t\tcase ATCS_REQUEST_REFUSED:\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase ATCS_REQUEST_COMPLETE:\n\t\t\treturn 0;\n\t\tcase ATCS_REQUEST_IN_PROGRESS:\n\t\t\tudelay(10);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_power_shift_control\n *\n * @adev: amdgpu_device pointer\n * @dev_state: device acpi state\n * @drv_state: driver state\n *\n * Executes the POWER_SHIFT_CONTROL method to\n * communicate current dGPU device state and\n * driver state to APU/SBIOS.\n * returns 0 on success, error on failure.\n */\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state)\n{\n\tunion acpi_object *info;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct atcs_pwr_shift_input atcs_input;\n\tstruct acpi_buffer params;\n\n\tif (!amdgpu_acpi_is_power_shift_control_supported())\n\t\treturn -EINVAL;\n\n\tatcs_input.size = sizeof(struct atcs_pwr_shift_input);\n\t/* dGPU id (bit 2-0: func num, 7-3: dev num, 15-8: bus num) */\n\tatcs_input.dgpu_id = pci_dev_id(adev->pdev);\n\tatcs_input.dev_acpi_state = dev_state;\n\tatcs_input.drv_state = drv_state;\n\n\tparams.length = sizeof(struct atcs_pwr_shift_input);\n\tparams.pointer = &atcs_input;\n\n\tinfo = amdgpu_atcs_call(atcs, ATCS_FUNCTION_POWER_SHIFT_CONTROL, &params);\n\tif (!info) {\n\t\tDRM_ERROR(\"ATCS PSC update failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n/**\n * amdgpu_acpi_smart_shift_update - update dGPU device state to SBIOS\n *\n * @dev: drm_device pointer\n * @ss_state: current smart shift event\n *\n * returns 0 on success,\n * otherwise return error number.\n */\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!amdgpu_device_supports_smart_shift(dev))\n\t\treturn 0;\n\n\tswitch (ss_state) {\n\t/* SBIOS trigger “stop”, “enable” and “start” at D0, Driver Operational.\n\t * SBIOS trigger “stop” at D3, Driver Not Operational.\n\t * SBIOS trigger “stop” and “disable” at D0, Driver NOT operational.\n\t */\n\tcase AMDGPU_SS_DRV_LOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D0:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DEV_D3:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tcase AMDGPU_SS_DRV_UNLOAD:\n\t\tr = amdgpu_acpi_power_shift_control(adev,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DEV_STATE_D0,\n\t\t\t\t\t\t    AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\n#ifdef CONFIG_ACPI_NUMA\nstatic inline uint64_t amdgpu_acpi_get_numa_size(int nid)\n{\n\t/* This is directly using si_meminfo_node implementation as the\n\t * function is not exported.\n\t */\n\tint zone_type;\n\tuint64_t managed_pages = 0;\n\n\tpg_data_t *pgdat = NODE_DATA(nid);\n\n\tfor (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)\n\t\tmanaged_pages +=\n\t\t\tzone_managed_pages(&pgdat->node_zones[zone_type]);\n\treturn managed_pages * PAGE_SIZE;\n}\n\nstatic struct amdgpu_numa_info *amdgpu_acpi_get_numa_info(uint32_t pxm)\n{\n\tstruct amdgpu_numa_info *numa_info;\n\tint nid;\n\n\tnuma_info = xa_load(&numa_info_xa, pxm);\n\n\tif (!numa_info) {\n\t\tstruct sysinfo info;\n\n\t\tnuma_info = kzalloc(sizeof(*numa_info), GFP_KERNEL);\n\t\tif (!numa_info)\n\t\t\treturn NULL;\n\n\t\tnid = pxm_to_node(pxm);\n\t\tnuma_info->pxm = pxm;\n\t\tnuma_info->nid = nid;\n\n\t\tif (numa_info->nid == NUMA_NO_NODE) {\n\t\t\tsi_meminfo(&info);\n\t\t\tnuma_info->size = info.totalram * info.mem_unit;\n\t\t} else {\n\t\t\tnuma_info->size = amdgpu_acpi_get_numa_size(nid);\n\t\t}\n\t\txa_store(&numa_info_xa, numa_info->pxm, numa_info, GFP_KERNEL);\n\t}\n\n\treturn numa_info;\n}\n#endif\n\n/**\n * amdgpu_acpi_get_node_id - obtain the NUMA node id for corresponding amdgpu\n * acpi device handle\n *\n * @handle: acpi handle\n * @numa_info: amdgpu_numa_info structure holding numa information\n *\n * Queries the ACPI interface to fetch the corresponding NUMA Node ID for a\n * given amdgpu acpi device.\n *\n * Returns ACPI STATUS OK with Node ID on success or the corresponding failure reason\n */\nstatic acpi_status amdgpu_acpi_get_node_id(acpi_handle handle,\n\t\t\t\t    struct amdgpu_numa_info **numa_info)\n{\n#ifdef CONFIG_ACPI_NUMA\n\tu64 pxm;\n\tacpi_status status;\n\n\tif (!numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\tstatus = acpi_evaluate_integer(handle, \"_PXM\", NULL, &pxm);\n\n\tif (ACPI_FAILURE(status))\n\t\treturn status;\n\n\t*numa_info = amdgpu_acpi_get_numa_info(pxm);\n\n\tif (!*numa_info)\n\t\treturn_ACPI_STATUS(AE_ERROR);\n\n\treturn_ACPI_STATUS(AE_OK);\n#else\n\treturn_ACPI_STATUS(AE_NOT_EXIST);\n#endif\n}\n\nstatic struct amdgpu_acpi_dev_info *amdgpu_acpi_get_dev(u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *acpi_dev;\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn NULL;\n\n\tlist_for_each_entry(acpi_dev, &amdgpu_acpi_dev_list, list)\n\t\tif (acpi_dev->sbdf == sbdf)\n\t\t\treturn acpi_dev;\n\n\treturn NULL;\n}\n\nstatic int amdgpu_acpi_dev_init(struct amdgpu_acpi_dev_info **dev_info,\n\t\t\t\tstruct amdgpu_acpi_xcc_info *xcc_info, u32 sbdf)\n{\n\tstruct amdgpu_acpi_dev_info *tmp;\n\tunion acpi_object *obj;\n\tint ret = -ENOENT;\n\n\t*dev_info = NULL;\n\ttmp = kzalloc(sizeof(struct amdgpu_acpi_dev_info), GFP_KERNEL);\n\tif (!tmp)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&tmp->xcc_list);\n\tINIT_LIST_HEAD(&tmp->list);\n\ttmp->sbdf = sbdf;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_SUPP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_SUPP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->supp_xcp_mode = obj->integer.value & 0xFFFF;\n\tACPI_FREE(obj);\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_XCP_MODE, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_XCP_MODE);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->xcp_mode = obj->integer.value & 0xFFFF;\n\ttmp->mem_mode = (obj->integer.value >> 32) & 0xFFFF;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_TMR_INFO, NULL,\n\t\t\t\t      ACPI_TYPE_PACKAGE);\n\n\tif (!obj || obj->package.count < 2) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_TMR_INFO);\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\ttmp->tmr_base = obj->package.elements[0].integer.value;\n\ttmp->tmr_size = obj->package.elements[1].integer.value;\n\tACPI_FREE(obj);\n\n\tDRM_DEBUG_DRIVER(\n\t\t\"New dev(%x): Supported xcp mode: %x curr xcp_mode : %x mem mode : %x, tmr base: %llx tmr size: %llx  \",\n\t\ttmp->sbdf, tmp->supp_xcp_mode, tmp->xcp_mode, tmp->mem_mode,\n\t\ttmp->tmr_base, tmp->tmr_size);\n\tlist_add_tail(&tmp->list, &amdgpu_acpi_dev_list);\n\t*dev_info = tmp;\n\n\treturn 0;\n\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\tkfree(tmp);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_get_xcc_info(struct amdgpu_acpi_xcc_info *xcc_info,\n\t\t\t\t    u32 *sbdf)\n{\n\tunion acpi_object *obj;\n\tacpi_status status;\n\tint ret = -ENOENT;\n\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_NUM_FUNCS, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj || obj->integer.value != AMD_XCC_DSM_NUM_FUNCS)\n\t\tgoto out;\n\tACPI_FREE(obj);\n\n\t/* Evaluate DSMs and fill XCC information */\n\tobj = acpi_evaluate_dsm_typed(xcc_info->handle, &amd_xcc_dsm_guid, 0,\n\t\t\t\t      AMD_XCC_DSM_GET_VF_XCC_MAPPING, NULL,\n\t\t\t\t      ACPI_TYPE_INTEGER);\n\n\tif (!obj) {\n\t\tacpi_handle_debug(xcc_info->handle,\n\t\t\t\t  \"_DSM function %d evaluation failed\",\n\t\t\t\t  AMD_XCC_DSM_GET_VF_XCC_MAPPING);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* PF xcc id [39:32] */\n\txcc_info->phy_id = (obj->integer.value >> 32) & 0xFF;\n\t/* xcp node of this xcc [47:40] */\n\txcc_info->xcp_node = (obj->integer.value >> 40) & 0xFF;\n\t/* PF domain of this xcc [31:16] */\n\t*sbdf = (obj->integer.value) & 0xFFFF0000;\n\t/* PF bus/dev/fn of this xcc [63:48] */\n\t*sbdf |= (obj->integer.value >> 48) & 0xFFFF;\n\tACPI_FREE(obj);\n\tobj = NULL;\n\n\tstatus =\n\t\tamdgpu_acpi_get_node_id(xcc_info->handle, &xcc_info->numa_info);\n\n\t/* TODO: check if this check is required */\n\tif (ACPI_SUCCESS(status))\n\t\tret = 0;\nout:\n\tif (obj)\n\t\tACPI_FREE(obj);\n\n\treturn ret;\n}\n\nstatic int amdgpu_acpi_enumerate_xcc(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info = NULL;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tstruct acpi_device *acpi_dev;\n\tchar hid[ACPI_ID_LEN];\n\tint ret, id;\n\tu32 sbdf;\n\n\tINIT_LIST_HEAD(&amdgpu_acpi_dev_list);\n\txa_init(&numa_info_xa);\n\n\tfor (id = 0; id < AMD_XCC_MAX_HID; id++) {\n\t\tsprintf(hid, \"%s%d\", \"AMD\", AMD_XCC_HID_START + id);\n\t\tacpi_dev = acpi_dev_get_first_match_dev(hid, NULL, -1);\n\t\t/* These ACPI objects are expected to be in sequential order. If\n\t\t * one is not found, no need to check the rest.\n\t\t */\n\t\tif (!acpi_dev) {\n\t\t\tDRM_DEBUG_DRIVER(\"No matching acpi device found for %s\",\n\t\t\t\t\t hid);\n\t\t\tbreak;\n\t\t}\n\n\t\txcc_info = kzalloc(sizeof(struct amdgpu_acpi_xcc_info),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!xcc_info) {\n\t\t\tDRM_ERROR(\"Failed to allocate memory for xcc info\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&xcc_info->list);\n\t\txcc_info->handle = acpi_device_handle(acpi_dev);\n\t\tacpi_dev_put(acpi_dev);\n\n\t\tret = amdgpu_acpi_get_xcc_info(xcc_info, &sbdf);\n\t\tif (ret) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\n\t\tif (!dev_info)\n\t\t\tret = amdgpu_acpi_dev_init(&dev_info, xcc_info, sbdf);\n\n\t\tif (ret == -ENOMEM)\n\t\t\treturn ret;\n\n\t\tif (!dev_info) {\n\t\t\tkfree(xcc_info);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_add_tail(&xcc_info->list, &dev_info->xcc_list);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tu32 sbdf;\n\n\tif (!tmr_offset || !tmr_size)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\t*tmr_offset = dev_info->tmr_base;\n\t*tmr_size = dev_info->tmr_size;\n\n\treturn 0;\n}\n\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info;\n\tstruct amdgpu_acpi_xcc_info *xcc_info;\n\tu32 sbdf;\n\n\tif (!numa_info)\n\t\treturn -EINVAL;\n\n\tsbdf = (pci_domain_nr(adev->pdev->bus) << 16);\n\tsbdf |= pci_dev_id(adev->pdev);\n\tdev_info = amdgpu_acpi_get_dev(sbdf);\n\tif (!dev_info)\n\t\treturn -ENOENT;\n\n\tlist_for_each_entry(xcc_info, &dev_info->xcc_list, list) {\n\t\tif (xcc_info->phy_id == xcc_id) {\n\t\t\tmemcpy(numa_info, xcc_info->numa_info,\n\t\t\t       sizeof(*numa_info));\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\n/**\n * amdgpu_acpi_event - handle notify events\n *\n * @nb: notifier block\n * @val: val\n * @data: acpi event\n *\n * Calls relevant amdgpu functions in response to various\n * acpi events.\n * Returns NOTIFY code\n */\nstatic int amdgpu_acpi_event(struct notifier_block *nb,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tstruct amdgpu_device *adev = container_of(nb, struct amdgpu_device, acpi_nb);\n\tstruct acpi_bus_event *entry = (struct acpi_bus_event *)data;\n\n\tif (strcmp(entry->device_class, ACPI_AC_CLASS) == 0) {\n\t\tif (power_supply_is_system_supplied() > 0)\n\t\t\tDRM_DEBUG_DRIVER(\"pm: AC\\n\");\n\t\telse\n\t\t\tDRM_DEBUG_DRIVER(\"pm: DC\\n\");\n\n\t\tamdgpu_pm_acpi_event_handler(adev);\n\t}\n\n\t/* Check for pending SBIOS requests */\n\treturn amdgpu_atif_handler(adev, entry);\n}\n\n/* Call all ACPI methods here */\n/**\n * amdgpu_acpi_init - init driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Verifies the AMD ACPI interfaces and registers with the acpi\n * notifier chain (all asics).\n * Returns 0 on success, error on failure.\n */\nint amdgpu_acpi_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tif (atif->notifications.brightness_change) {\n\t\tif (adev->dc_enabled) {\n#if defined(CONFIG_DRM_AMD_DC)\n\t\t\tstruct amdgpu_display_manager *dm = &adev->dm;\n\n\t\t\tif (dm->backlight_dev[0])\n\t\t\t\tatif->bd = dm->backlight_dev[0];\n#endif\n\t\t} else {\n\t\t\tstruct drm_encoder *tmp;\n\n\t\t\t/* Find the encoder controlling the brightness */\n\t\t\tlist_for_each_entry(tmp, &adev_to_drm(adev)->mode_config.encoder_list,\n\t\t\t\t\t    head) {\n\t\t\t\tstruct amdgpu_encoder *enc = to_amdgpu_encoder(tmp);\n\n\t\t\t\tif ((enc->devices & (ATOM_DEVICE_LCD_SUPPORT)) &&\n\t\t\t\t    enc->enc_priv) {\n\t\t\t\t\tstruct amdgpu_encoder_atom_dig *dig = enc->enc_priv;\n\n\t\t\t\t\tif (dig->bl_dev) {\n\t\t\t\t\t\tatif->bd = dig->bl_dev;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tadev->acpi_nb.notifier_call = amdgpu_acpi_event;\n\tregister_acpi_notifier(&adev->acpi_nb);\n\n\treturn 0;\n}\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\n\tcaps->caps_valid = atif->backlight_caps.caps_valid;\n\tcaps->min_input_signal = atif->backlight_caps.min_input_signal;\n\tcaps->max_input_signal = atif->backlight_caps.max_input_signal;\n\tcaps->ac_level = atif->backlight_caps.ac_level;\n\tcaps->dc_level = atif->backlight_caps.dc_level;\n}\n\n/**\n * amdgpu_acpi_fini - tear down driver acpi support\n *\n * @adev: amdgpu_device pointer\n *\n * Unregisters with the acpi notifier chain (all asics).\n */\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev)\n{\n\tunregister_acpi_notifier(&adev->acpi_nb);\n}\n\n/**\n * amdgpu_atif_pci_probe_handle - look up the ATIF handle\n *\n * @pdev: pci device\n *\n * Look up the ATIF handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atif_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = {sizeof(acpi_method_name), acpi_method_name};\n\tacpi_handle dhandle, atif_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATIF\", &atif_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atif.handle = atif_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atif.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATIF handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atif_verify_interface(&amdgpu_acpi_priv.atif);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atif.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * amdgpu_atcs_pci_probe_handle - look up the ATCS handle\n *\n * @pdev: pci device\n *\n * Look up the ATCS handles (all asics).\n * Returns true if the handle is found, false if not.\n */\nstatic bool amdgpu_atcs_pci_probe_handle(struct pci_dev *pdev)\n{\n\tchar acpi_method_name[255] = { 0 };\n\tstruct acpi_buffer buffer = { sizeof(acpi_method_name), acpi_method_name };\n\tacpi_handle dhandle, atcs_handle;\n\tacpi_status status;\n\tint ret;\n\n\tdhandle = ACPI_HANDLE(&pdev->dev);\n\tif (!dhandle)\n\t\treturn false;\n\n\tstatus = acpi_get_handle(dhandle, \"ATCS\", &atcs_handle);\n\tif (ACPI_FAILURE(status))\n\t\treturn false;\n\n\tamdgpu_acpi_priv.atcs.handle = atcs_handle;\n\tacpi_get_name(amdgpu_acpi_priv.atcs.handle, ACPI_FULL_PATHNAME, &buffer);\n\tDRM_DEBUG_DRIVER(\"Found ATCS handle %s\\n\", acpi_method_name);\n\tret = amdgpu_atcs_verify_interface(&amdgpu_acpi_priv.atcs);\n\tif (ret) {\n\t\tamdgpu_acpi_priv.atcs.handle = 0;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n\n/**\n * amdgpu_acpi_should_gpu_reset\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if should reset GPU, false if not\n */\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev)\n{\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gfx.imu.funcs) /* Not need to do mode2 reset for IMU enabled APUs */\n\t\treturn false;\n\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    amdgpu_acpi_is_s3_active(adev))\n\t\treturn false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n\treturn pm_suspend_target_state != PM_SUSPEND_TO_IDLE;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * amdgpu_acpi_detect - detect ACPI ATIF/ATCS methods\n *\n * Check if we have the ATIF/ATCS methods and populate\n * the structures in the driver.\n */\nvoid amdgpu_acpi_detect(void)\n{\n\tstruct amdgpu_atif *atif = &amdgpu_acpi_priv.atif;\n\tstruct amdgpu_atcs *atcs = &amdgpu_acpi_priv.atcs;\n\tstruct pci_dev *pdev = NULL;\n\tint ret;\n\n\twhile ((pdev = pci_get_base_class(PCI_BASE_CLASS_DISPLAY, pdev))) {\n\t\tif ((pdev->class != PCI_CLASS_DISPLAY_VGA << 8) &&\n\t\t    (pdev->class != PCI_CLASS_DISPLAY_OTHER << 8))\n\t\t\tcontinue;\n\n\t\tif (!atif->handle)\n\t\t\tamdgpu_atif_pci_probe_handle(pdev);\n\t\tif (!atcs->handle)\n\t\t\tamdgpu_atcs_pci_probe_handle(pdev);\n\t}\n\n\tif (atif->functions.sbios_requests && !atif->functions.system_params) {\n\t\t/* XXX check this workraround, if sbios request function is\n\t\t * present we have to see how it's configured in the system\n\t\t * params\n\t\t */\n\t\tatif->functions.system_params = true;\n\t}\n\n\tif (atif->functions.system_params) {\n\t\tret = amdgpu_atif_get_notification_params(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to GET_SYSTEM_PARAMS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\t/* Disable notification */\n\t\t\tatif->notification_cfg.enabled = false;\n\t\t}\n\t}\n\n\tif (atif->functions.query_backlight_transfer_characteristics) {\n\t\tret = amdgpu_atif_query_backlight_caps(atif);\n\t\tif (ret) {\n\t\t\tDRM_DEBUG_DRIVER(\"Call to QUERY_BACKLIGHT_TRANSFER_CHARACTERISTICS failed: %d\\n\",\n\t\t\t\t\tret);\n\t\t\tatif->backlight_caps.caps_valid = false;\n\t\t}\n\t} else {\n\t\tatif->backlight_caps.caps_valid = false;\n\t}\n\n\tamdgpu_acpi_enumerate_xcc();\n}\n\nvoid amdgpu_acpi_release(void)\n{\n\tstruct amdgpu_acpi_dev_info *dev_info, *dev_tmp;\n\tstruct amdgpu_acpi_xcc_info *xcc_info, *xcc_tmp;\n\tstruct amdgpu_numa_info *numa_info;\n\tunsigned long index;\n\n\txa_for_each(&numa_info_xa, index, numa_info) {\n\t\tkfree(numa_info);\n\t\txa_erase(&numa_info_xa, index);\n\t}\n\n\tif (list_empty(&amdgpu_acpi_dev_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev_info, dev_tmp, &amdgpu_acpi_dev_list,\n\t\t\t\t list) {\n\t\tlist_for_each_entry_safe(xcc_info, xcc_tmp, &dev_info->xcc_list,\n\t\t\t\t\t list) {\n\t\t\tlist_del(&xcc_info->list);\n\t\t\tkfree(xcc_info);\n\t\t}\n\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_SUSPEND)\n/**\n * amdgpu_acpi_is_s3_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev)\n{\n\treturn !(adev->flags & AMD_IS_APU) ||\n\t\t(pm_suspend_target_state == PM_SUSPEND_MEM);\n}\n\n/**\n * amdgpu_acpi_is_s0ix_active\n *\n * @adev: amdgpu_device_pointer\n *\n * returns true if supported, false if not.\n */\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    (pm_suspend_target_state != PM_SUSPEND_TO_IDLE))\n\t\treturn false;\n\n\tif (adev->asic_type < CHIP_RAVEN)\n\t\treturn false;\n\n\tif (!(adev->pm.pp_feature & PP_GFXOFF_MASK))\n\t\treturn false;\n\n\t/*\n\t * If ACPI_FADT_LOW_POWER_S0 is not set in the FADT, it is generally\n\t * risky to do any special firmware-related preparations for entering\n\t * S0ix even though the system is suspending to idle, so return false\n\t * in that case.\n\t */\n\tif (!(acpi_gbl_FADT.flags & ACPI_FADT_LOW_POWER_S0)) {\n\t\tdev_err_once(adev->dev,\n\t\t\t      \"Power consumption will be higher as BIOS has not been configured for suspend-to-idle.\\n\"\n\t\t\t      \"To use suspend-to-idle change the sleep mode in BIOS setup.\\n\");\n\t\treturn false;\n\t}\n\n#if !IS_ENABLED(CONFIG_AMD_PMC)\n\tdev_err_once(adev->dev,\n\t\t      \"Power consumption will be higher as the kernel has not been compiled with CONFIG_AMD_PMC.\\n\");\n\treturn false;\n#else\n\treturn true;\n#endif /* CONFIG_AMD_PMC */\n}\n\n/**\n * amdgpu_choose_low_power_state\n *\n * @adev: amdgpu_device_pointer\n *\n * Choose the target low power state for the GPU\n */\nvoid amdgpu_choose_low_power_state(struct amdgpu_device *adev)\n{\n\tif (adev->in_runpm)\n\t\treturn;\n\n\tif (amdgpu_acpi_is_s0ix_active(adev))\n\t\tadev->in_s0ix = true;\n\telse if (amdgpu_acpi_is_s3_active(adev))\n\t\tadev->in_s3 = true;\n}\n\n#endif /* CONFIG_SUSPEND */\n"}
{"bug_report_path": "dataset/raw_data/bugs/dev-set/sprintf0-bug.txt", "bug_report_text": "CID: 1234567\nType: Buffer overflow\nCategory: BUFFER_OVERFLOW\nClassification: Bad use of string function\nSeverity: High\nCertainty: Absolute\nStatus: New\nFunction: amdgpu_atombios_i2c_init\nFile: drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nLine: 150\n\nIssue:\nUnbounded sprintf can cause buffer overflow. The code uses sprintf() without size limits to write into buffer 'stmp', which could lead to buffer overflow if the formatted string exceeds the buffer size.\n\nDescription:\nThe function amdgpu_atombios_i2c_init() uses sprintf() to format a hexadecimal value into a string buffer 'stmp' without checking if the resulting string will fit in the destination buffer. This could lead to a buffer overflow if i2c.i2c_id generates a string longer than the size of stmp.\n\nUse snprintf instead", "diff_path": "dataset/raw_data/bugs/dev-set/sprintf0-diff.txt", "diff_text": "diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\nindex 0c8975ac5af9..a6245ec89453 100644\n--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c\n@@ -147,7 +147,7 @@ void amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n                        i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n                        if (i2c.valid) {\n-                               sprintf(stmp, \"0x%x\", i2c.i2c_id);\n+                               snprintf(stmp, sizeof(stmp) \"0x%x\", i2c.i2c_id);\n                                adev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n                        }\n                        gpio = (ATOM_GPIO_I2C_ASSIGMENT *)", "source_code_path": "drivers/gpu/drm/amd/amdgpu/amdgpu_atombios.c", "line_number": 150, "code": "/*\n * Copyright 2007-8 Advanced Micro Devices, Inc.\n * Copyright 2008 Red Hat Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie\n *          Alex Deucher\n */\n\n#include <drm/amdgpu_drm.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_i2c.h\"\n#include \"amdgpu_display.h\"\n\n#include \"atom.h\"\n#include \"atom-bits.h\"\n#include \"atombios_encoders.h\"\n#include \"bif/bif_4_1_d.h\"\n\nstatic void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev,\n\t\t\t\t\t  ATOM_GPIO_I2C_ASSIGMENT *gpio,\n\t\t\t\t\t  u8 index)\n{\n\n}\n\nstatic struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio)\n{\n\tstruct amdgpu_i2c_bus_rec i2c;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\n\ti2c.mask_clk_reg = le16_to_cpu(gpio->usClkMaskRegisterIndex);\n\ti2c.mask_data_reg = le16_to_cpu(gpio->usDataMaskRegisterIndex);\n\ti2c.en_clk_reg = le16_to_cpu(gpio->usClkEnRegisterIndex);\n\ti2c.en_data_reg = le16_to_cpu(gpio->usDataEnRegisterIndex);\n\ti2c.y_clk_reg = le16_to_cpu(gpio->usClkY_RegisterIndex);\n\ti2c.y_data_reg = le16_to_cpu(gpio->usDataY_RegisterIndex);\n\ti2c.a_clk_reg = le16_to_cpu(gpio->usClkA_RegisterIndex);\n\ti2c.a_data_reg = le16_to_cpu(gpio->usDataA_RegisterIndex);\n\ti2c.mask_clk_mask = (1 << gpio->ucClkMaskShift);\n\ti2c.mask_data_mask = (1 << gpio->ucDataMaskShift);\n\ti2c.en_clk_mask = (1 << gpio->ucClkEnShift);\n\ti2c.en_data_mask = (1 << gpio->ucDataEnShift);\n\ti2c.y_clk_mask = (1 << gpio->ucClkY_Shift);\n\ti2c.y_data_mask = (1 << gpio->ucDataY_Shift);\n\ti2c.a_clk_mask = (1 << gpio->ucClkA_Shift);\n\ti2c.a_data_mask = (1 << gpio->ucDataA_Shift);\n\n\tif (gpio->sucI2cId.sbfAccess.bfHW_Capable)\n\t\ti2c.hw_capable = true;\n\telse\n\t\ti2c.hw_capable = false;\n\n\tif (gpio->sucI2cId.ucAccess == 0xa0)\n\t\ti2c.mm_i2c = true;\n\telse\n\t\ti2c.mm_i2c = false;\n\n\ti2c.i2c_id = gpio->sucI2cId.ucAccess;\n\n\tif (i2c.mask_clk_reg)\n\t\ti2c.valid = true;\n\telse\n\t\ti2c.valid = false;\n\n\treturn i2c;\n}\n\nstruct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint8_t id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&i2c, 0, sizeof(struct amdgpu_i2c_bus_rec));\n\ti2c.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\tif (gpio->sucI2cId.ucAccess == id) {\n\t\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n\n\treturn i2c;\n}\n\nvoid amdgpu_atombios_i2c_init(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tATOM_GPIO_I2C_ASSIGMENT *gpio;\n\tstruct amdgpu_i2c_bus_rec i2c;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_I2C_Info);\n\tstruct _ATOM_GPIO_I2C_INFO *i2c_info;\n\tuint16_t data_offset, size;\n\tint i, num_indices;\n\tchar stmp[32];\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\ti2c_info = (struct _ATOM_GPIO_I2C_INFO *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_I2C_ASSIGMENT);\n\n\t\tgpio = &i2c_info->asGPIO_Info[0];\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tamdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, i);\n\n\t\t\ti2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);\n\n\t\t\tif (i2c.valid) {\n\t\t\t\tsprintf(stmp, \"0x%x\", i2c.i2c_id);\n\t\t\t\tadev->i2c_bus[i] = amdgpu_i2c_create(adev_to_drm(adev), &i2c, stmp);\n\t\t\t}\n\t\t\tgpio = (ATOM_GPIO_I2C_ASSIGMENT *)\n\t\t\t\t((u8 *)gpio + sizeof(ATOM_GPIO_I2C_ASSIGMENT));\n\t\t}\n\t}\n}\n\nstruct amdgpu_gpio_rec\namdgpu_atombios_lookup_gpio(struct amdgpu_device *adev,\n\t\t\t    u8 id)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tstruct amdgpu_gpio_rec gpio;\n\tint index = GetIndexIntoMasterTable(DATA, GPIO_Pin_LUT);\n\tstruct _ATOM_GPIO_PIN_LUT *gpio_info;\n\tATOM_GPIO_PIN_ASSIGNMENT *pin;\n\tu16 data_offset, size;\n\tint i, num_indices;\n\n\tmemset(&gpio, 0, sizeof(struct amdgpu_gpio_rec));\n\tgpio.valid = false;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, &size, NULL, NULL, &data_offset)) {\n\t\tgpio_info = (struct _ATOM_GPIO_PIN_LUT *)(ctx->bios + data_offset);\n\n\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\tsizeof(ATOM_GPIO_PIN_ASSIGNMENT);\n\n\t\tpin = gpio_info->asGPIO_Pin;\n\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\tif (id == pin->ucGPIO_ID) {\n\t\t\t\tgpio.id = pin->ucGPIO_ID;\n\t\t\t\tgpio.reg = le16_to_cpu(pin->usGpioPin_AIndex);\n\t\t\t\tgpio.shift = pin->ucGpioPinBitShift;\n\t\t\t\tgpio.mask = (1 << pin->ucGpioPinBitShift);\n\t\t\t\tgpio.valid = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpin = (ATOM_GPIO_PIN_ASSIGNMENT *)\n\t\t\t\t((u8 *)pin + sizeof(ATOM_GPIO_PIN_ASSIGNMENT));\n\t\t}\n\t}\n\n\treturn gpio;\n}\n\nstatic struct amdgpu_hpd\namdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gpio_rec *gpio)\n{\n\tstruct amdgpu_hpd hpd;\n\tu32 reg;\n\n\tmemset(&hpd, 0, sizeof(struct amdgpu_hpd));\n\n\treg = amdgpu_display_hpd_get_gpio_reg(adev);\n\n\thpd.gpio = *gpio;\n\tif (gpio->reg == reg) {\n\t\tswitch(gpio->mask) {\n\t\tcase (1 << 0):\n\t\t\thpd.hpd = AMDGPU_HPD_1;\n\t\t\tbreak;\n\t\tcase (1 << 8):\n\t\t\thpd.hpd = AMDGPU_HPD_2;\n\t\t\tbreak;\n\t\tcase (1 << 16):\n\t\t\thpd.hpd = AMDGPU_HPD_3;\n\t\t\tbreak;\n\t\tcase (1 << 24):\n\t\t\thpd.hpd = AMDGPU_HPD_4;\n\t\t\tbreak;\n\t\tcase (1 << 26):\n\t\t\thpd.hpd = AMDGPU_HPD_5;\n\t\t\tbreak;\n\t\tcase (1 << 28):\n\t\t\thpd.hpd = AMDGPU_HPD_6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\thpd.hpd = AMDGPU_HPD_NONE;\n\treturn hpd;\n}\n\nstatic const int object_connector_convert[] = {\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVII,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_DVID,\n\tDRM_MODE_CONNECTOR_VGA,\n\tDRM_MODE_CONNECTOR_Composite,\n\tDRM_MODE_CONNECTOR_SVIDEO,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_HDMIA,\n\tDRM_MODE_CONNECTOR_HDMIB,\n\tDRM_MODE_CONNECTOR_LVDS,\n\tDRM_MODE_CONNECTOR_9PinDIN,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_Unknown,\n\tDRM_MODE_CONNECTOR_DisplayPort,\n\tDRM_MODE_CONNECTOR_eDP,\n\tDRM_MODE_CONNECTOR_Unknown\n};\n\nbool amdgpu_atombios_has_dce_engine_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\n\tif (path_obj->ucNumOfDispPath)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nbool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tstruct atom_context *ctx = mode_info->atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, Object_Header);\n\tu16 size, data_offset;\n\tu8 frev, crev;\n\tATOM_CONNECTOR_OBJECT_TABLE *con_obj;\n\tATOM_ENCODER_OBJECT_TABLE *enc_obj;\n\tATOM_OBJECT_TABLE *router_obj;\n\tATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj;\n\tATOM_OBJECT_HEADER *obj_header;\n\tint i, j, k, path_size, device_support;\n\tint connector_type;\n\tu16 conn_id, connector_object_id;\n\tstruct amdgpu_i2c_bus_rec ddc_bus;\n\tstruct amdgpu_router router;\n\tstruct amdgpu_gpio_rec gpio;\n\tstruct amdgpu_hpd hpd;\n\n\tif (!amdgpu_atom_parse_data_header(ctx, index, &size, &frev, &crev, &data_offset))\n\t\treturn false;\n\n\tif (crev < 2)\n\t\treturn false;\n\n\tobj_header = (ATOM_OBJECT_HEADER *) (ctx->bios + data_offset);\n\tpath_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usDisplayPathTableOffset));\n\tcon_obj = (ATOM_CONNECTOR_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usConnectorObjectTableOffset));\n\tenc_obj = (ATOM_ENCODER_OBJECT_TABLE *)\n\t    (ctx->bios + data_offset +\n\t     le16_to_cpu(obj_header->usEncoderObjectTableOffset));\n\trouter_obj = (ATOM_OBJECT_TABLE *)\n\t\t(ctx->bios + data_offset +\n\t\t le16_to_cpu(obj_header->usRouterObjectTableOffset));\n\tdevice_support = le16_to_cpu(obj_header->usDeviceSupport);\n\n\tpath_size = 0;\n\tfor (i = 0; i < path_obj->ucNumOfDispPath; i++) {\n\t\tuint8_t *addr = (uint8_t *) path_obj->asDispPath;\n\t\tATOM_DISPLAY_OBJECT_PATH *path;\n\t\taddr += path_size;\n\t\tpath = (ATOM_DISPLAY_OBJECT_PATH *) addr;\n\t\tpath_size += le16_to_cpu(path->usSize);\n\n\t\tif (device_support & le16_to_cpu(path->usDeviceTag)) {\n\t\t\tuint8_t con_obj_id =\n\t\t\t    (le16_to_cpu(path->usConnObjectId) & OBJECT_ID_MASK)\n\t\t\t    >> OBJECT_ID_SHIFT;\n\n\t\t\t/* Skip TV/CV support */\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_TV1_SUPPORT) ||\n\t\t\t    (le16_to_cpu(path->usDeviceTag) ==\n\t\t\t     ATOM_DEVICE_CV_SUPPORT))\n\t\t\t\tcontinue;\n\n\t\t\tif (con_obj_id >= ARRAY_SIZE(object_connector_convert)) {\n\t\t\t\tDRM_ERROR(\"invalid con_obj_id %d for device tag 0x%04x\\n\",\n\t\t\t\t\t  con_obj_id, le16_to_cpu(path->usDeviceTag));\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconnector_type =\n\t\t\t\tobject_connector_convert[con_obj_id];\n\t\t\tconnector_object_id = con_obj_id;\n\n\t\t\tif (connector_type == DRM_MODE_CONNECTOR_Unknown)\n\t\t\t\tcontinue;\n\n\t\t\trouter.ddc_valid = false;\n\t\t\trouter.cd_valid = false;\n\t\t\tfor (j = 0; j < ((le16_to_cpu(path->usSize) - 8) / 2); j++) {\n\t\t\t\tuint8_t grph_obj_type =\n\t\t\t\t    (le16_to_cpu(path->usGraphicObjIds[j]) &\n\t\t\t\t     OBJECT_TYPE_MASK) >> OBJECT_TYPE_SHIFT;\n\n\t\t\t\tif (grph_obj_type == GRAPH_OBJECT_TYPE_ENCODER) {\n\t\t\t\t\tfor (k = 0; k < enc_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 encoder_obj = le16_to_cpu(enc_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == encoder_obj) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(enc_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_ENCODER_CAP_RECORD *cap_record;\n\t\t\t\t\t\t\tu16 caps = 0;\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_ENCODER_CAP_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcap_record =(ATOM_ENCODER_CAP_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\tcaps = le16_to_cpu(cap_record->usEncoderCap);\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tamdgpu_display_add_encoder(adev, encoder_obj,\n\t\t\t\t\t\t\t\t\t\t    le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t\t\t\t\t    caps);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if (grph_obj_type == GRAPH_OBJECT_TYPE_ROUTER) {\n\t\t\t\t\tfor (k = 0; k < router_obj->ucNumberOfObjects; k++) {\n\t\t\t\t\t\tu16 router_obj_id = le16_to_cpu(router_obj->asObjects[k].usObjectID);\n\t\t\t\t\t\tif (le16_to_cpu(path->usGraphicObjIds[j]) == router_obj_id) {\n\t\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER *record = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usRecordOffset));\n\t\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\t\t\t\t\t\t\tATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path;\n\t\t\t\t\t\t\tATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path;\n\t\t\t\t\t\t\tATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table =\n\t\t\t\t\t\t\t\t(ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)\n\t\t\t\t\t\t\t\t(ctx->bios + data_offset +\n\t\t\t\t\t\t\t\t le16_to_cpu(router_obj->asObjects[k].usSrcDstTableOffset));\n\t\t\t\t\t\t\tu8 *num_dst_objs = (u8 *)\n\t\t\t\t\t\t\t\t((u8 *)router_src_dst_table + 1 +\n\t\t\t\t\t\t\t\t (router_src_dst_table->ucNumberOfSrc * 2));\n\t\t\t\t\t\t\tu16 *dst_objs = (u16 *)(num_dst_objs + 1);\n\t\t\t\t\t\t\tint enum_id;\n\n\t\t\t\t\t\t\trouter.router_id = router_obj_id;\n\t\t\t\t\t\t\tfor (enum_id = 0; enum_id < (*num_dst_objs); enum_id++) {\n\t\t\t\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t\t\t\t    le16_to_cpu(dst_objs[enum_id]))\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\t\trouter.i2c_info =\n\t\t\t\t\t\t\t\t\t\tamdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t       i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t       ucAccess);\n\t\t\t\t\t\t\t\t\trouter.i2c_addr = i2c_record->ucI2CAddr >> 1;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DDC_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.ddc_valid = true;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_type = ddc_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_control_pin = ddc_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.ddc_mux_state = ddc_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\tcase ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD_TYPE:\n\t\t\t\t\t\t\t\t\tcd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)\n\t\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\t\trouter.cd_valid = true;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_type = cd_path->ucMuxType;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_control_pin = cd_path->ucMuxControlPin;\n\t\t\t\t\t\t\t\t\trouter.cd_mux_state = cd_path->ucMuxState[enum_id];\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\trecord = (ATOM_COMMON_RECORD_HEADER *)\n\t\t\t\t\t\t\t\t\t((char *)record + record->ucRecordSize);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* look up gpio for ddc, hpd */\n\t\t\tddc_bus.valid = false;\n\t\t\thpd.hpd = AMDGPU_HPD_NONE;\n\t\t\tif ((le16_to_cpu(path->usDeviceTag) &\n\t\t\t     (ATOM_DEVICE_TV_SUPPORT | ATOM_DEVICE_CV_SUPPORT)) == 0) {\n\t\t\t\tfor (j = 0; j < con_obj->ucNumberOfObjects; j++) {\n\t\t\t\t\tif (le16_to_cpu(path->usConnObjectId) ==\n\t\t\t\t\t    le16_to_cpu(con_obj->asObjects[j].\n\t\t\t\t\t\t\tusObjectID)) {\n\t\t\t\t\t\tATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t    *record =\n\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t     *)\n\t\t\t\t\t\t    (ctx->bios + data_offset +\n\t\t\t\t\t\t     le16_to_cpu(con_obj->\n\t\t\t\t\t\t\t\t asObjects[j].\n\t\t\t\t\t\t\t\t usRecordOffset));\n\t\t\t\t\t\tATOM_I2C_RECORD *i2c_record;\n\t\t\t\t\t\tATOM_HPD_INT_RECORD *hpd_record;\n\t\t\t\t\t\tATOM_I2C_ID_CONFIG_ACCESS *i2c_config;\n\n\t\t\t\t\t\twhile (record->ucRecordSize > 0 &&\n\t\t\t\t\t\t       record->ucRecordType > 0 &&\n\t\t\t\t\t\t       record->ucRecordType <= ATOM_MAX_OBJECT_RECORD_NUMBER) {\n\t\t\t\t\t\t\tswitch (record->ucRecordType) {\n\t\t\t\t\t\t\tcase ATOM_I2C_RECORD_TYPE:\n\t\t\t\t\t\t\t\ti2c_record =\n\t\t\t\t\t\t\t\t    (ATOM_I2C_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\ti2c_config =\n\t\t\t\t\t\t\t\t\t(ATOM_I2C_ID_CONFIG_ACCESS *)\n\t\t\t\t\t\t\t\t\t&i2c_record->sucI2cId;\n\t\t\t\t\t\t\t\tddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t\t i2c_config->\n\t\t\t\t\t\t\t\t\t\t\t\t ucAccess);\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\tcase ATOM_HPD_INT_RECORD_TYPE:\n\t\t\t\t\t\t\t\thpd_record =\n\t\t\t\t\t\t\t\t\t(ATOM_HPD_INT_RECORD *)\n\t\t\t\t\t\t\t\t\trecord;\n\t\t\t\t\t\t\t\tgpio = amdgpu_atombios_lookup_gpio(adev,\n\t\t\t\t\t\t\t\t\t\t\t  hpd_record->ucHPDIntGPIOID);\n\t\t\t\t\t\t\t\thpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, &gpio);\n\t\t\t\t\t\t\t\thpd.plugged_state = hpd_record->ucPlugged_PinState;\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\trecord =\n\t\t\t\t\t\t\t    (ATOM_COMMON_RECORD_HEADER\n\t\t\t\t\t\t\t     *) ((char *)record\n\t\t\t\t\t\t\t\t +\n\t\t\t\t\t\t\t\t record->\n\t\t\t\t\t\t\t\t ucRecordSize);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* needed for aux chan transactions */\n\t\t\tddc_bus.hpd = hpd.hpd;\n\n\t\t\tconn_id = le16_to_cpu(path->usConnObjectId);\n\n\t\t\tamdgpu_display_add_connector(adev,\n\t\t\t\t\t\t      conn_id,\n\t\t\t\t\t\t      le16_to_cpu(path->usDeviceTag),\n\t\t\t\t\t\t      connector_type, &ddc_bus,\n\t\t\t\t\t\t      connector_object_id,\n\t\t\t\t\t\t      &hpd,\n\t\t\t\t\t\t      &router);\n\n\t\t}\n\t}\n\n\tamdgpu_link_encoder_connector(adev_to_drm(adev));\n\n\treturn true;\n}\n\nunion firmware_info {\n\tATOM_FIRMWARE_INFO info;\n\tATOM_FIRMWARE_INFO_V1_2 info_12;\n\tATOM_FIRMWARE_INFO_V1_3 info_13;\n\tATOM_FIRMWARE_INFO_V1_4 info_14;\n\tATOM_FIRMWARE_INFO_V2_1 info_21;\n\tATOM_FIRMWARE_INFO_V2_2 info_22;\n};\n\nint amdgpu_atombios_get_clock_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tint i;\n\t\tstruct amdgpu_pll *ppll = &adev->clock.ppll[0];\n\t\tstruct amdgpu_pll *spll = &adev->clock.spll;\n\t\tstruct amdgpu_pll *mpll = &adev->clock.mpll;\n\t\tunion firmware_info *firmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t/* pixel clocks */\n\t\tppll->reference_freq =\n\t\t    le16_to_cpu(firmware_info->info.usReferenceClock);\n\t\tppll->reference_div = 0;\n\n\t\tppll->pll_out_min =\n\t\t\tle32_to_cpu(firmware_info->info_12.ulMinPixelClockPLL_Output);\n\t\tppll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxPixelClockPLL_Output);\n\n\t\tppll->lcd_pll_out_min =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMinPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_min == 0)\n\t\t\tppll->lcd_pll_out_min = ppll->pll_out_min;\n\t\tppll->lcd_pll_out_max =\n\t\t\tle16_to_cpu(firmware_info->info_14.usLcdMaxPixelClockPLL_Output) * 100;\n\t\tif (ppll->lcd_pll_out_max == 0)\n\t\t\tppll->lcd_pll_out_max = ppll->pll_out_max;\n\n\t\tif (ppll->pll_out_min == 0)\n\t\t\tppll->pll_out_min = 64800;\n\n\t\tppll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinPixelClockPLL_Input);\n\t\tppll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxPixelClockPLL_Input);\n\n\t\tppll->min_post_div = 2;\n\t\tppll->max_post_div = 0x7f;\n\t\tppll->min_frac_feedback_div = 0;\n\t\tppll->max_frac_feedback_div = 9;\n\t\tppll->min_ref_div = 2;\n\t\tppll->max_ref_div = 0x3ff;\n\t\tppll->min_feedback_div = 4;\n\t\tppll->max_feedback_div = 0xfff;\n\t\tppll->best_vco = 0;\n\n\t\tfor (i = 1; i < AMDGPU_MAX_PPLL; i++)\n\t\t\tadev->clock.ppll[i] = *ppll;\n\n\t\t/* system clock */\n\t\tspll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usCoreReferenceClock);\n\t\tspll->reference_div = 0;\n\n\t\tspll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Output);\n\t\tspll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxEngineClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (spll->pll_out_min == 0)\n\t\t\tspll->pll_out_min = 64800;\n\n\t\tspll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinEngineClockPLL_Input);\n\t\tspll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxEngineClockPLL_Input);\n\n\t\tspll->min_post_div = 1;\n\t\tspll->max_post_div = 1;\n\t\tspll->min_ref_div = 2;\n\t\tspll->max_ref_div = 0xff;\n\t\tspll->min_feedback_div = 4;\n\t\tspll->max_feedback_div = 0xff;\n\t\tspll->best_vco = 0;\n\n\t\t/* memory clock */\n\t\tmpll->reference_freq =\n\t\t\tle16_to_cpu(firmware_info->info_21.usMemoryReferenceClock);\n\t\tmpll->reference_div = 0;\n\n\t\tmpll->pll_out_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Output);\n\t\tmpll->pll_out_max =\n\t\t    le32_to_cpu(firmware_info->info.ulMaxMemoryClockPLL_Output);\n\n\t\t/* ??? */\n\t\tif (mpll->pll_out_min == 0)\n\t\t\tmpll->pll_out_min = 64800;\n\n\t\tmpll->pll_in_min =\n\t\t    le16_to_cpu(firmware_info->info.usMinMemoryClockPLL_Input);\n\t\tmpll->pll_in_max =\n\t\t    le16_to_cpu(firmware_info->info.usMaxMemoryClockPLL_Input);\n\n\t\tadev->clock.default_sclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultEngineClock);\n\t\tadev->clock.default_mclk =\n\t\t    le32_to_cpu(firmware_info->info.ulDefaultMemoryClock);\n\n\t\tmpll->min_post_div = 1;\n\t\tmpll->max_post_div = 1;\n\t\tmpll->min_ref_div = 2;\n\t\tmpll->max_ref_div = 0xff;\n\t\tmpll->min_feedback_div = 4;\n\t\tmpll->max_feedback_div = 0xff;\n\t\tmpll->best_vco = 0;\n\n\t\t/* disp clock */\n\t\tadev->clock.default_dispclk =\n\t\t\tle32_to_cpu(firmware_info->info_21.ulDefaultDispEngineClkFreq);\n\t\t/* set a reasonable default for DP */\n\t\tif (adev->clock.default_dispclk < 53900) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 600Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 60000;\n\t\t} else if (adev->clock.default_dispclk <= 60000) {\n\t\t\tDRM_DEBUG(\"Changing default dispclk from %dMhz to 625Mhz\\n\",\n\t\t\t\t  adev->clock.default_dispclk / 100);\n\t\t\tadev->clock.default_dispclk = 62500;\n\t\t}\n\t\tadev->clock.dp_extclk =\n\t\t\tle16_to_cpu(firmware_info->info_21.usUniphyDPModeExtClkFreq);\n\t\tadev->clock.current_dispclk = adev->clock.default_dispclk;\n\n\t\tadev->clock.max_pixel_clock = le16_to_cpu(firmware_info->info.usMaxPixelClock);\n\t\tif (adev->clock.max_pixel_clock == 0)\n\t\t\tadev->clock.max_pixel_clock = 40000;\n\n\t\t/* not technically a clock, but... */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tle16_to_cpu(firmware_info->info.usFirmwareCapability.susAccess);\n\n\t\tret = 0;\n\t}\n\n\tadev->pm.current_sclk = adev->clock.default_sclk;\n\tadev->pm.current_mclk = adev->clock.default_mclk;\n\n\treturn ret;\n}\n\nunion gfx_info {\n\tATOM_GFX_INFO_V2_1 info;\n};\n\nint amdgpu_atombios_get_gfx_info(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, GFX_Info);\n\tuint8_t frev, crev;\n\tuint16_t data_offset;\n\tint ret = -EINVAL;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tunion gfx_info *gfx_info = (union gfx_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\n\t\tadev->gfx.config.max_shader_engines = gfx_info->info.max_shader_engines;\n\t\tadev->gfx.config.max_tile_pipes = gfx_info->info.max_tile_pipes;\n\t\tadev->gfx.config.max_cu_per_sh = gfx_info->info.max_cu_per_sh;\n\t\tadev->gfx.config.max_sh_per_se = gfx_info->info.max_sh_per_se;\n\t\tadev->gfx.config.max_backends_per_se = gfx_info->info.max_backends_per_se;\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tgfx_info->info.max_texture_channel_caches;\n\n\t\tret = 0;\n\t}\n\treturn ret;\n}\n\nunion igp_info {\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO info;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8;\n\tstruct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9;\n};\n\n/*\n * Return vram width from integrated system info table, if available,\n * or 0 if not.\n */\nint amdgpu_atombios_get_vram_width(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 8:\n\t\tcase 9:\n\t\t\treturn igp_info->info_8.ucUMAChannelNumber * 64;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_atom_ss *ss,\n\t\t\t\t\t\t int id)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, IntegratedSystemInfo);\n\tu16 data_offset, size;\n\tunion igp_info *igp_info;\n\tu8 frev, crev;\n\tu16 percentage = 0, rate = 0;\n\n\t/* get any igp specific overrides */\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tigp_info = (union igp_info *)\n\t\t\t(mode_info->atom_context->bios + data_offset);\n\t\tswitch (crev) {\n\t\tcase 6:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_6.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_6.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_7.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_7.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_8.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_8.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 9:\n\t\t\tswitch (id) {\n\t\t\tcase ASIC_INTERNAL_SS_ON_TMDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usDVISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usDVISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_HDMI:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usHDMISSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usHDMISSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\tcase ASIC_INTERNAL_SS_ON_LVDS:\n\t\t\t\tpercentage = le16_to_cpu(igp_info->info_9.usLvdsSSPercentage);\n\t\t\t\trate = le16_to_cpu(igp_info->info_9.usLvdsSSpreadRateIn10Hz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported IGP table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\t\tif (percentage)\n\t\t\tss->percentage = percentage;\n\t\tif (rate)\n\t\t\tss->rate = rate;\n\t}\n}\n\nunion asic_ss_info {\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO info;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2;\n\tstruct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3;\n};\n\nunion asic_ss_assignment {\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT v1;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2;\n\tstruct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3;\n};\n\nbool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_atom_ss *ss,\n\t\t\t\t      int id, u32 clock)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, ASIC_InternalSS_Info);\n\tuint16_t data_offset, size;\n\tunion asic_ss_info *ss_info;\n\tunion asic_ss_assignment *ss_assign;\n\tuint8_t frev, crev;\n\tint i, num_indices;\n\n\tif (id == ASIC_INTERNAL_MEMORY_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_MEMORY_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\tif (id == ASIC_INTERNAL_ENGINE_SS) {\n\t\tif (!(adev->mode_info.firmware_flags & ATOM_BIOS_INFO_ENGINE_CLOCK_SS_SUPPORT))\n\t\t\treturn false;\n\t}\n\n\tmemset(ss, 0, sizeof(struct amdgpu_atom_ss));\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\n\t\tss_info =\n\t\t\t(union asic_ss_info *)(mode_info->atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT);\n\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v1.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v1.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v1.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v1.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v1.usSpreadRateInKhz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V2);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_2.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v2.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v2.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v2.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v2.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v2.usSpreadRateIn10Hz);\n\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((crev == 2) &&\n\t\t\t\t\t    ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t     (id == ASIC_INTERNAL_MEMORY_SS)))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V2));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnum_indices = (size - sizeof(ATOM_COMMON_TABLE_HEADER)) /\n\t\t\t\tsizeof(ATOM_ASIC_SS_ASSIGNMENT_V3);\n\t\t\tss_assign = (union asic_ss_assignment *)((u8 *)&ss_info->info_3.asSpreadSpectrum[0]);\n\t\t\tfor (i = 0; i < num_indices; i++) {\n\t\t\t\tif ((ss_assign->v3.ucClockIndication == id) &&\n\t\t\t\t    (clock <= le32_to_cpu(ss_assign->v3.ulTargetClockRange))) {\n\t\t\t\t\tss->percentage =\n\t\t\t\t\t\tle16_to_cpu(ss_assign->v3.usSpreadSpectrumPercentage);\n\t\t\t\t\tss->type = ss_assign->v3.ucSpreadSpectrumMode;\n\t\t\t\t\tss->rate = le16_to_cpu(ss_assign->v3.usSpreadRateIn10Hz);\n\t\t\t\t\tif (ss_assign->v3.ucSpreadSpectrumMode &\n\t\t\t\t\t    SS_MODE_V3_PERCENTAGE_DIV_BY_1000_MASK)\n\t\t\t\t\t\tss->percentage_divider = 1000;\n\t\t\t\t\telse\n\t\t\t\t\t\tss->percentage_divider = 100;\n\t\t\t\t\tif ((id == ASIC_INTERNAL_ENGINE_SS) ||\n\t\t\t\t\t    (id == ASIC_INTERNAL_MEMORY_SS))\n\t\t\t\t\t\tss->rate /= 100;\n\t\t\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\t\t\tamdgpu_atombios_get_igp_ss_overrides(adev, ss, id);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t\tss_assign = (union asic_ss_assignment *)\n\t\t\t\t\t((u8 *)ss_assign + sizeof(ATOM_ASIC_SS_ASSIGNMENT_V3));\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unsupported ASIC_InternalSS_Info table: %d %d\\n\", frev, crev);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nunion get_clock_dividers {\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4;\n\tstruct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5;\n\tstruct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in;\n\tstruct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out;\n};\n\nint amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev,\n\t\t\t\t       u8 clock_type,\n\t\t\t\t       u32 clock,\n\t\t\t\t       bool strobe_mode,\n\t\t\t\t       struct atom_clock_dividers *dividers)\n{\n\tunion get_clock_dividers args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryEnginePLL);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(dividers, 0, sizeof(struct atom_clock_dividers));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 2:\n\tcase 3:\n\tcase 5:\n\t\t/* r6xx, r7xx, evergreen, ni, si.\n\t\t * TODO: add support for asic_type <= CHIP_RV770*/\n\t\tif (clock_type == COMPUTE_ENGINE_PLL_PARAM) {\n\t\t\targs.v3.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\n\t\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tdividers->post_div = args.v3.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v3.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v3.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v3.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v3.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t} else {\n\t\t\t/* for SI we use ComputeMemoryClockParam for memory plls */\n\t\t\tif (adev->asic_type >= CHIP_TAHITI)\n\t\t\t\treturn -EINVAL;\n\t\t\targs.v5.ulClockParams = cpu_to_le32((clock_type << 24) | clock);\n\t\t\tif (strobe_mode)\n\t\t\t\targs.v5.ucInputFlag = ATOM_PLL_INPUT_FLAG_PLL_STROBE_MODE_EN;\n\n\t\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tdividers->post_div = args.v5.ucPostDiv;\n\t\t\tdividers->enable_post_div = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t     ATOM_PLL_CNTL_FLAG_PLL_POST_DIV_EN) ? true : false;\n\t\t\tdividers->enable_dithen = (args.v5.ucCntlFlag &\n\t\t\t\t\t\t   ATOM_PLL_CNTL_FLAG_FRACTION_DISABLE) ? false : true;\n\t\t\tdividers->whole_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDiv);\n\t\t\tdividers->frac_fb_div = le16_to_cpu(args.v5.ulFbDiv.usFbDivFrac);\n\t\t\tdividers->ref_div = args.v5.ucRefDiv;\n\t\t\tdividers->vco_mode = (args.v5.ucCntlFlag &\n\t\t\t\t\t      ATOM_PLL_CNTL_FLAG_MPLL_VCO_MODE) ? 1 : 0;\n\t\t}\n\t\tbreak;\n\tcase 4:\n\t\t/* fusion */\n\t\targs.v4.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\treturn -EINVAL;\n\n\t\tdividers->post_divider = dividers->post_div = args.v4.ucPostDiv;\n\t\tdividers->real_clock = le32_to_cpu(args.v4.ulClock);\n\t\tbreak;\n\tcase 6:\n\t\t/* CI */\n\t\t/* COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK, COMPUTE_GPUCLK_INPUT_FLAG_SCLK */\n\t\targs.v6_in.ulClock.ulComputeClockFlag = clock_type;\n\t\targs.v6_in.ulClock.ulClockFreq = cpu_to_le32(clock);\t/* 10 khz */\n\n\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\treturn -EINVAL;\n\n\t\tdividers->whole_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDiv);\n\t\tdividers->frac_fb_div = le16_to_cpu(args.v6_out.ulFbDiv.usFbDivFrac);\n\t\tdividers->ref_div = args.v6_out.ucPllRefDiv;\n\t\tdividers->post_div = args.v6_out.ucPllPostDiv;\n\t\tdividers->flags = args.v6_out.ucPllCntlFlag;\n\t\tdividers->real_clock = le32_to_cpu(args.v6_out.ulClock.ulClock);\n\t\tdividers->post_divider = args.v6_out.ulClock.ucPostDiv;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nint amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev,\n\t\t\t\t\t    u32 clock,\n\t\t\t\t\t    bool strobe_mode,\n\t\t\t\t\t    struct atom_mpll_param *mpll_param)\n{\n\tCOMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args;\n\tint index = GetIndexIntoMasterTable(COMMAND, ComputeMemoryClockParam);\n\tu8 frev, crev;\n\n\tmemset(&args, 0, sizeof(args));\n\tmemset(mpll_param, 0, sizeof(struct atom_mpll_param));\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (frev) {\n\tcase 2:\n\t\tswitch (crev) {\n\t\tcase 1:\n\t\t\t/* SI */\n\t\t\targs.ulClock = cpu_to_le32(clock);\t/* 10 khz */\n\t\t\targs.ucInputFlag = 0;\n\t\t\tif (strobe_mode)\n\t\t\t\targs.ucInputFlag |= MPLL_INPUT_FLAG_STROBE_MODE_EN;\n\n\t\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tmpll_param->clkfrac = le16_to_cpu(args.ulFbDiv.usFbDivFrac);\n\t\t\tmpll_param->clkf = le16_to_cpu(args.ulFbDiv.usFbDiv);\n\t\t\tmpll_param->post_div = args.ucPostDiv;\n\t\t\tmpll_param->dll_speed = args.ucDllSpeed;\n\t\t\tmpll_param->bwcntl = args.ucBWCntl;\n\t\t\tmpll_param->vco_mode =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_VCO_MODE_MASK);\n\t\t\tmpll_param->yclk_sel =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_BYPASS_DQ_PLL) ? 1 : 0;\n\t\t\tmpll_param->qdr =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_QDR_ENABLE) ? 1 : 0;\n\t\t\tmpll_param->half_rate =\n\t\t\t\t(args.ucPllCntlFlag & MPLL_CNTL_FLAG_AD_HALF_RATE) ? 1 : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev,\n\t\t\t\t\t     u32 eng_clock, u32 mem_clock)\n{\n\tSET_ENGINE_CLOCK_PS_ALLOCATION args;\n\tint index = GetIndexIntoMasterTable(COMMAND, DynamicMemorySettings);\n\tu32 tmp;\n\n\tmemset(&args, 0, sizeof(args));\n\n\ttmp = eng_clock & SET_CLOCK_FREQ_MASK;\n\ttmp |= (COMPUTE_ENGINE_PLL_PARAM << 24);\n\n\targs.ulTargetEngineClock = cpu_to_le32(tmp);\n\tif (mem_clock)\n\t\targs.sReserved.ulClock = cpu_to_le32(mem_clock & SET_CLOCK_FREQ_MASK);\n\n\tamdgpu_atom_execute_table(adev->mode_info.atom_context, index, (uint32_t *)&args,\n\t\tsizeof(args));\n}\n\nvoid amdgpu_atombios_get_default_voltages(struct amdgpu_device *adev,\n\t\t\t\t\t  u16 *vddc, u16 *vddci, u16 *mvdd)\n{\n\tstruct amdgpu_mode_info *mode_info = &adev->mode_info;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\tu8 frev, crev;\n\tu16 data_offset;\n\tunion firmware_info *firmware_info;\n\n\t*vddc = 0;\n\t*vddci = 0;\n\t*mvdd = 0;\n\n\tif (amdgpu_atom_parse_data_header(mode_info->atom_context, index, NULL,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tfirmware_info =\n\t\t\t(union firmware_info *)(mode_info->atom_context->bios +\n\t\t\t\t\t\tdata_offset);\n\t\t*vddc = le16_to_cpu(firmware_info->info_14.usBootUpVDDCVoltage);\n\t\tif ((frev == 2) && (crev >= 2)) {\n\t\t\t*vddci = le16_to_cpu(firmware_info->info_22.usBootUpVDDCIVoltage);\n\t\t\t*mvdd = le16_to_cpu(firmware_info->info_22.usBootUpMVDDCVoltage);\n\t\t}\n\t}\n}\n\nunion set_voltage {\n\tstruct _SET_VOLTAGE_PS_ALLOCATION alloc;\n\tstruct _SET_VOLTAGE_PARAMETERS v1;\n\tstruct _SET_VOLTAGE_PARAMETERS_V2 v2;\n\tstruct _SET_VOLTAGE_PARAMETERS_V1_3 v3;\n};\n\nint amdgpu_atombios_get_max_vddc(struct amdgpu_device *adev, u8 voltage_type,\n\t\t\t     u16 voltage_id, u16 *voltage)\n{\n\tunion set_voltage args;\n\tint index = GetIndexIntoMasterTable(COMMAND, SetVoltage);\n\tu8 frev, crev;\n\n\tif (!amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, &frev, &crev))\n\t\treturn -EINVAL;\n\n\tswitch (crev) {\n\tcase 1:\n\t\treturn -EINVAL;\n\tcase 2:\n\t\targs.v2.ucVoltageType = SET_VOLTAGE_GET_MAX_VOLTAGE;\n\t\targs.v2.ucVoltageMode = 0;\n\t\targs.v2.usVoltageLevel = 0;\n\n\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\treturn -EINVAL;\n\n\t\t*voltage = le16_to_cpu(args.v2.usVoltageLevel);\n\t\tbreak;\n\tcase 3:\n\t\targs.v3.ucVoltageType = voltage_type;\n\t\targs.v3.ucVoltageMode = ATOM_GET_VOLTAGE_LEVEL;\n\t\targs.v3.usVoltageLevel = cpu_to_le16(voltage_id);\n\n\t\tif (amdgpu_atom_execute_table(adev->mode_info.atom_context,\n\t\t    index, (uint32_t *)&args, sizeof(args)))\n\t\t\treturn -EINVAL;\n\n\t\t*voltage = le16_to_cpu(args.v3.usVoltageLevel);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_leakage_vddc_based_on_leakage_idx(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u16 *voltage,\n\t\t\t\t\t\t      u16 leakage_idx)\n{\n\treturn amdgpu_atombios_get_max_vddc(adev, VOLTAGE_TYPE_VDDC, leakage_idx, voltage);\n}\n\nunion voltage_object_info {\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2;\n\tstruct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3;\n};\n\nunion voltage_object {\n\tstruct _ATOM_VOLTAGE_OBJECT v1;\n\tstruct _ATOM_VOLTAGE_OBJECT_V2 v2;\n\tunion _ATOM_VOLTAGE_OBJECT_V3 v3;\n};\n\n\nstatic ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3,\n\t\t\t\t\t\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tu32 size = le16_to_cpu(v3->sHeader.usStructureSize);\n\tu32 offset = offsetof(ATOM_VOLTAGE_OBJECT_INFO_V3_1, asVoltageObj[0]);\n\tu8 *start = (u8 *)v3;\n\n\twhile (offset < size) {\n\t\tATOM_VOLTAGE_OBJECT_V3 *vo = (ATOM_VOLTAGE_OBJECT_V3 *)(start + offset);\n\t\tif ((vo->asGpioVoltageObj.sHeader.ucVoltageType == voltage_type) &&\n\t\t    (vo->asGpioVoltageObj.sHeader.ucVoltageMode == voltage_mode))\n\t\t\treturn vo;\n\t\toffset += le16_to_cpu(vo->asGpioVoltageObj.sHeader.usSize);\n\t}\n\treturn NULL;\n}\n\nint amdgpu_atombios_get_svi2_info(struct amdgpu_device *adev,\n\t\t\t      u8 voltage_type,\n\t\t\t      u8 *svd_gpio_id, u8 *svc_gpio_id)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type,\n\t\t\t\t\t\t\t\t      VOLTAGE_OBJ_SVID2);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\t*svd_gpio_id = voltage_object->v3.asSVID2Obj.ucSVDGpioId;\n\t\t\t\t\t*svc_gpio_id = voltage_object->v3.asSVID2Obj.ucSVCGpioId;\n\t\t\t\t} else {\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nbool\namdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev,\n\t\t\t\tu8 voltage_type, u8 voltage_mode)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tunion voltage_object_info *voltage_info;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (amdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t  voltage_type, voltage_mode))\n\t\t\t\t\treturn true;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t}\n\treturn false;\n}\n\nint amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 voltage_type, u8 voltage_mode,\n\t\t\t\t      struct atom_voltage_table *voltage_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VoltageObjectInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\tint i;\n\tunion voltage_object_info *voltage_info;\n\tunion voltage_object *voltage_object = NULL;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvoltage_info = (union voltage_object_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\n\t\tswitch (frev) {\n\t\tcase 3:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tvoltage_object = (union voltage_object *)\n\t\t\t\t\tamdgpu_atombios_lookup_voltage_object_v3(&voltage_info->v3,\n\t\t\t\t\t\t\t\t      voltage_type, voltage_mode);\n\t\t\t\tif (voltage_object) {\n\t\t\t\t\tATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio =\n\t\t\t\t\t\t&voltage_object->v3.asGpioVoltageObj;\n\t\t\t\t\tVOLTAGE_LUT_ENTRY_V2 *lut;\n\t\t\t\t\tif (gpio->ucGpioEntryNum > MAX_VOLTAGE_ENTRIES)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tlut = &gpio->asVolGpioLut[0];\n\t\t\t\t\tfor (i = 0; i < gpio->ucGpioEntryNum; i++) {\n\t\t\t\t\t\tvoltage_table->entries[i].value =\n\t\t\t\t\t\t\tle16_to_cpu(lut->usVoltageValue);\n\t\t\t\t\t\tvoltage_table->entries[i].smio_low =\n\t\t\t\t\t\t\tle32_to_cpu(lut->ulVoltageId);\n\t\t\t\t\t\tlut = (VOLTAGE_LUT_ENTRY_V2 *)\n\t\t\t\t\t\t\t((u8 *)lut + sizeof(VOLTAGE_LUT_ENTRY_V2));\n\t\t\t\t\t}\n\t\t\t\t\tvoltage_table->mask_low = le32_to_cpu(gpio->ulGpioMaskVal);\n\t\t\t\t\tvoltage_table->count = gpio->ucGpioEntryNum;\n\t\t\t\t\tvoltage_table->phase_delay = gpio->ucPhaseDelay;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"unknown voltage object table\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nunion vram_info {\n\tstruct _ATOM_VRAM_INFO_V3 v1_3;\n\tstruct _ATOM_VRAM_INFO_V4 v1_4;\n\tstruct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1;\n};\n\n#define MEM_ID_MASK           0xff000000\n#define MEM_ID_SHIFT          24\n#define CLOCK_RANGE_MASK      0x00ffffff\n#define CLOCK_RANGE_SHIFT     0\n#define LOW_NIBBLE_MASK       0xf\n#define DATA_EQU_PREV         0\n#define DATA_FROM_TABLE       4\n\nint amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev,\n\t\t\t\t      u8 module_index,\n\t\t\t\t      struct atom_mc_reg_table *reg_table)\n{\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_Info);\n\tu8 frev, crev, num_entries, t_mem_id, num_ranges = 0;\n\tu32 i = 0, j;\n\tu16 data_offset, size;\n\tunion vram_info *vram_info;\n\n\tmemset(reg_table, 0, sizeof(struct atom_mc_reg_table));\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t   &frev, &crev, &data_offset)) {\n\t\tvram_info = (union vram_info *)\n\t\t\t(adev->mode_info.atom_context->bios + data_offset);\n\t\tswitch (frev) {\n\t\tcase 1:\n\t\t\tDRM_ERROR(\"old table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\tcase 2:\n\t\t\tswitch (crev) {\n\t\t\tcase 1:\n\t\t\t\tif (module_index < vram_info->v2_1.ucNumOfVRAMModule) {\n\t\t\t\t\tATOM_INIT_REG_BLOCK *reg_block =\n\t\t\t\t\t\t(ATOM_INIT_REG_BLOCK *)\n\t\t\t\t\t\t((u8 *)vram_info + le16_to_cpu(vram_info->v2_1.usMemClkPatchTblOffset));\n\t\t\t\t\tATOM_MEMORY_SETTING_DATA_BLOCK *reg_data =\n\t\t\t\t\t\t(ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t((u8 *)reg_block + (2 * sizeof(u16)) +\n\t\t\t\t\t\t le16_to_cpu(reg_block->usRegIndexTblSize));\n\t\t\t\t\tATOM_INIT_REG_INDEX_FORMAT *format = &reg_block->asRegIndexBuf[0];\n\t\t\t\t\tnum_entries = (u8)((le16_to_cpu(reg_block->usRegIndexTblSize)) /\n\t\t\t\t\t\t\t   sizeof(ATOM_INIT_REG_INDEX_FORMAT)) - 1;\n\t\t\t\t\tif (num_entries > VBIOS_MC_REGISTER_ARRAY_SIZE)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\twhile (i < num_entries) {\n\t\t\t\t\t\tif (format->ucPreRegDataLength & ACCESS_PLACEHOLDER)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\treg_table->mc_reg_address[i].s1 =\n\t\t\t\t\t\t\t(u16)(le16_to_cpu(format->usRegIndex));\n\t\t\t\t\t\treg_table->mc_reg_address[i].pre_reg_data =\n\t\t\t\t\t\t\t(u8)(format->ucPreRegDataLength);\n\t\t\t\t\t\ti++;\n\t\t\t\t\t\tformat = (ATOM_INIT_REG_INDEX_FORMAT *)\n\t\t\t\t\t\t\t((u8 *)format + sizeof(ATOM_INIT_REG_INDEX_FORMAT));\n\t\t\t\t\t}\n\t\t\t\t\treg_table->last = i;\n\t\t\t\t\twhile ((le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK) &&\n\t\t\t\t\t       (num_ranges < VBIOS_MAX_AC_TIMING_ENTRIES)) {\n\t\t\t\t\t\tt_mem_id = (u8)((le32_to_cpu(*(u32 *)reg_data) & MEM_ID_MASK)\n\t\t\t\t\t\t\t\t>> MEM_ID_SHIFT);\n\t\t\t\t\t\tif (module_index == t_mem_id) {\n\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mclk_max =\n\t\t\t\t\t\t\t\t(u32)((le32_to_cpu(*(u32 *)reg_data) & CLOCK_RANGE_MASK)\n\t\t\t\t\t\t\t\t      >> CLOCK_RANGE_SHIFT);\n\t\t\t\t\t\t\tfor (i = 0, j = 1; i < reg_table->last; i++) {\n\t\t\t\t\t\t\t\tif ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_FROM_TABLE) {\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\t(u32)le32_to_cpu(*((u32 *)reg_data + j));\n\t\t\t\t\t\t\t\t\tj++;\n\t\t\t\t\t\t\t\t} else if ((reg_table->mc_reg_address[i].pre_reg_data & LOW_NIBBLE_MASK) == DATA_EQU_PREV) {\n\t\t\t\t\t\t\t\t\tif (i == 0)\n\t\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i] =\n\t\t\t\t\t\t\t\t\t\treg_table->mc_reg_table_entry[num_ranges].mc_data[i - 1];\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnum_ranges++;\n\t\t\t\t\t\t}\n\t\t\t\t\t\treg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)\n\t\t\t\t\t\t\t((u8 *)reg_data + le16_to_cpu(reg_block->usRegDataBlkSize));\n\t\t\t\t\t}\n\t\t\t\t\tif (le32_to_cpu(*(u32 *)reg_data) != END_OF_REG_DATA_BLOCK)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\treg_table->num_entries = num_ranges;\n\t\t\t\t} else\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown table version %d, %d\\n\", frev, crev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n#endif\n\nbool amdgpu_atombios_has_gpu_virtualization_table(struct amdgpu_device *adev)\n{\n\tint index = GetIndexIntoMasterTable(DATA, GPUVirtualizationInfo);\n\tu8 frev, crev;\n\tu16 data_offset, size;\n\n\tif (amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, &size,\n\t\t\t\t\t  &frev, &crev, &data_offset))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev, bool lock)\n{\n\tuint32_t bios_6_scratch;\n\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\tif (lock) {\n\t\tbios_6_scratch |= ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch &= ~ATOM_S6_ACC_MODE;\n\t} else {\n\t\tbios_6_scratch &= ~ATOM_S6_CRITICAL_STATE;\n\t\tbios_6_scratch |= ATOM_S6_ACC_MODE;\n\t}\n\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nstatic void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev)\n{\n\tuint32_t bios_2_scratch, bios_6_scratch;\n\n\tadev->bios_scratch_reg_offset = mmBIOS_SCRATCH_0;\n\n\tbios_2_scratch = RREG32(adev->bios_scratch_reg_offset + 2);\n\tbios_6_scratch = RREG32(adev->bios_scratch_reg_offset + 6);\n\n\t/* let the bios control the backlight */\n\tbios_2_scratch &= ~ATOM_S2_VRI_BRIGHT_ENABLE;\n\n\t/* tell the bios not to handle mode switching */\n\tbios_6_scratch |= ATOM_S6_ACC_BLOCK_DISPLAY_SWITCH;\n\n\t/* clear the vbios dpms state */\n\tbios_2_scratch &= ~ATOM_S2_DEVICE_DPMS_STATE;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, bios_2_scratch);\n\tWREG32(adev->bios_scratch_reg_offset + 6, bios_6_scratch);\n}\n\nvoid amdgpu_atombios_scratch_regs_engine_hung(struct amdgpu_device *adev,\n\t\t\t\t\t      bool hung)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 3);\n\n\tif (hung)\n\t\ttmp |= ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\telse\n\t\ttmp &= ~ATOM_S3_ASIC_GUI_ENGINE_HUNG;\n\n\tWREG32(adev->bios_scratch_reg_offset + 3, tmp);\n}\n\nvoid amdgpu_atombios_scratch_regs_set_backlight_level(struct amdgpu_device *adev,\n\t\t\t\t\t\t      u32 backlight_level)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 2);\n\n\ttmp &= ~ATOM_S2_CURRENT_BL_LEVEL_MASK;\n\ttmp |= (backlight_level << ATOM_S2_CURRENT_BL_LEVEL_SHIFT) &\n\t\tATOM_S2_CURRENT_BL_LEVEL_MASK;\n\n\tWREG32(adev->bios_scratch_reg_offset + 2, tmp);\n}\n\nbool amdgpu_atombios_scratch_need_asic_init(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(adev->bios_scratch_reg_offset + 7);\n\n\tif (tmp & ATOM_S7_ASIC_INIT_COMPLETE_MASK)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\n/* Atom needs data in little endian format so swap as appropriate when copying\n * data to or from atom. Note that atom operates on dw units.\n *\n * Use to_le=true when sending data to atom and provide at least\n * ALIGN(num_bytes,4) bytes in the dst buffer.\n *\n * Use to_le=false when receiving data from atom and provide ALIGN(num_bytes,4)\n * byes in the src buffer.\n */\nvoid amdgpu_atombios_copy_swap(u8 *dst, u8 *src, u8 num_bytes, bool to_le)\n{\n#ifdef __BIG_ENDIAN\n\tu32 src_tmp[5], dst_tmp[5];\n\tint i;\n\tu8 align_num_bytes = ALIGN(num_bytes, 4);\n\n\tif (to_le) {\n\t\tmemcpy(src_tmp, src, num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = cpu_to_le32(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, align_num_bytes);\n\t} else {\n\t\tmemcpy(src_tmp, src, align_num_bytes);\n\t\tfor (i = 0; i < align_num_bytes / 4; i++)\n\t\t\tdst_tmp[i] = le32_to_cpu(src_tmp[i]);\n\t\tmemcpy(dst, dst_tmp, num_bytes);\n\t}\n#else\n\tmemcpy(dst, src, num_bytes);\n#endif\n}\n\nstatic int amdgpu_atombios_allocate_fb_scratch(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\tint index = GetIndexIntoMasterTable(DATA, VRAM_UsageByFirmware);\n\tuint16_t data_offset;\n\tint usage_bytes = 0;\n\tstruct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage;\n\tu64 start_addr;\n\tu64 size;\n\n\tif (amdgpu_atom_parse_data_header(ctx, index, NULL, NULL, NULL, &data_offset)) {\n\t\tfirmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)(ctx->bios + data_offset);\n\n\t\tDRM_DEBUG(\"atom firmware requested %08x %dkb\\n\",\n\t\t\t  le32_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware),\n\t\t\t  le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb));\n\n\t\tstart_addr = firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware;\n\t\tsize = firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb;\n\n\t\tif ((uint32_t)(start_addr & ATOM_VRAM_OPERATION_FLAGS_MASK) ==\n\t\t\t(uint32_t)(ATOM_VRAM_BLOCK_SRIOV_MSG_SHARE_RESERVATION <<\n\t\t\tATOM_VRAM_OPERATION_FLAGS_SHIFT)) {\n\t\t\t/* Firmware request VRAM reservation for SR-IOV */\n\t\t\tadev->mman.fw_vram_usage_start_offset = (start_addr &\n\t\t\t\t(~ATOM_VRAM_OPERATION_FLAGS_MASK)) << 10;\n\t\t\tadev->mman.fw_vram_usage_size = size << 10;\n\t\t\t/* Use the default scratch size */\n\t\t\tusage_bytes = 0;\n\t\t} else {\n\t\t\tusage_bytes = le16_to_cpu(firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb) * 1024;\n\t\t}\n\t}\n\tctx->scratch_size_bytes = 0;\n\tif (usage_bytes == 0)\n\t\tusage_bytes = 20 * 1024;\n\t/* allocate some scratch memory */\n\tctx->scratch = kzalloc(usage_bytes, GFP_KERNEL);\n\tif (!ctx->scratch)\n\t\treturn -ENOMEM;\n\tctx->scratch_size_bytes = usage_bytes;\n\treturn 0;\n}\n\n/* ATOM accessor methods */\n/*\n * ATOM is an interpreted byte code stored in tables in the vbios.  The\n * driver registers callbacks to access registers and the interpreter\n * in the driver parses the tables and executes then to program specific\n * actions (set display modes, asic init, etc.).  See amdgpu_atombios.c,\n * atombios.h, and atom.c\n */\n\n/**\n * cail_pll_read - read PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n * Returns the value of the PLL register.\n */\nstatic uint32_t cail_pll_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_pll_write - write PLL register\n *\n * @info: atom card_info pointer\n * @reg: PLL register offset\n * @val: value to write to the pll register\n *\n * Provides a PLL register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_pll_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_mc_read - read MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n *\n * Provides an MC register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MC register.\n */\nstatic uint32_t cail_mc_read(struct card_info *info, uint32_t reg)\n{\n\treturn 0;\n}\n\n/**\n * cail_mc_write - write MC (Memory Controller) register\n *\n * @info: atom card_info pointer\n * @reg: MC register offset\n * @val: value to write to the pll register\n *\n * Provides a MC register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_mc_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\n}\n\n/**\n * cail_reg_write - write MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n * @val: value to write to the pll register\n *\n * Provides a MMIO register accessor for the atom interpreter (r4xx+).\n */\nstatic void cail_reg_write(struct card_info *info, uint32_t reg, uint32_t val)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\n\tWREG32(reg, val);\n}\n\n/**\n * cail_reg_read - read MMIO register\n *\n * @info: atom card_info pointer\n * @reg: MMIO register offset\n *\n * Provides an MMIO register accessor for the atom interpreter (r4xx+).\n * Returns the value of the MMIO register.\n */\nstatic uint32_t cail_reg_read(struct card_info *info, uint32_t reg)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(info->dev);\n\tuint32_t r;\n\n\tr = RREG32(reg);\n\treturn r;\n}\n\nstatic ssize_t amdgpu_atombios_get_vbios_version(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\treturn sysfs_emit(buf, \"%s\\n\", ctx->vbios_pn);\n}\n\nstatic DEVICE_ATTR(vbios_version, 0444, amdgpu_atombios_get_vbios_version,\n\t\t   NULL);\n\nstatic struct attribute *amdgpu_vbios_version_attrs[] = {\n\t&dev_attr_vbios_version.attr,\n\tNULL\n};\n\nconst struct attribute_group amdgpu_vbios_version_attr_group = {\n\t.attrs = amdgpu_vbios_version_attrs\n};\n\nint amdgpu_atombios_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context)\n\t\treturn devm_device_add_group(adev->dev,\n\t\t\t\t\t     &amdgpu_vbios_version_attr_group);\n\n\treturn 0;\n}\n\n/**\n * amdgpu_atombios_fini - free the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Frees the driver info and register access callbacks for the ATOM\n * interpreter (r4xx+).\n * Called at driver shutdown.\n */\nvoid amdgpu_atombios_fini(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.atom_context) {\n\t\tkfree(adev->mode_info.atom_context->scratch);\n\t\tkfree(adev->mode_info.atom_context->iio);\n\t}\n\tkfree(adev->mode_info.atom_context);\n\tadev->mode_info.atom_context = NULL;\n\tkfree(adev->mode_info.atom_card_info);\n\tadev->mode_info.atom_card_info = NULL;\n}\n\n/**\n * amdgpu_atombios_init - init the driver info and callbacks for atombios\n *\n * @adev: amdgpu_device pointer\n *\n * Initializes the driver info and register access callbacks for the\n * ATOM interpreter (r4xx+).\n * Returns 0 on sucess, -ENOMEM on failure.\n * Called at driver startup.\n */\nint amdgpu_atombios_init(struct amdgpu_device *adev)\n{\n\tstruct card_info *atom_card_info =\n\t    kzalloc(sizeof(struct card_info), GFP_KERNEL);\n\n\tif (!atom_card_info)\n\t\treturn -ENOMEM;\n\n\tadev->mode_info.atom_card_info = atom_card_info;\n\tatom_card_info->dev = adev_to_drm(adev);\n\tatom_card_info->reg_read = cail_reg_read;\n\tatom_card_info->reg_write = cail_reg_write;\n\tatom_card_info->mc_read = cail_mc_read;\n\tatom_card_info->mc_write = cail_mc_write;\n\tatom_card_info->pll_read = cail_pll_read;\n\tatom_card_info->pll_write = cail_pll_write;\n\n\tadev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, adev->bios);\n\tif (!adev->mode_info.atom_context) {\n\t\tamdgpu_atombios_fini(adev);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&adev->mode_info.atom_context->mutex);\n\tif (adev->is_atom_fw) {\n\t\tamdgpu_atomfirmware_scratch_regs_init(adev);\n\t\tamdgpu_atomfirmware_allocate_fb_scratch(adev);\n\t\t/* cached firmware_flags for further usage */\n\t\tadev->mode_info.firmware_flags =\n\t\t\tamdgpu_atomfirmware_query_firmware_capability(adev);\n\t} else {\n\t\tamdgpu_atombios_scratch_regs_init(adev);\n\t\tamdgpu_atombios_allocate_fb_scratch(adev);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_atombios_get_data_table(struct amdgpu_device *adev,\n\t\t\t\t   uint32_t table,\n\t\t\t\t   uint16_t *size,\n\t\t\t\t   uint8_t *frev,\n\t\t\t\t   uint8_t *crev,\n\t\t\t\t   uint8_t **addr)\n{\n\tuint16_t data_start;\n\n\tif (!amdgpu_atom_parse_data_header(adev->mode_info.atom_context, table,\n\t\t\t\t\t   size, frev, crev, &data_start))\n\t\treturn -EINVAL;\n\n\t*addr = (uint8_t *)adev->mode_info.atom_context->bios + data_start;\n\n\treturn 0;\n}\n"}
